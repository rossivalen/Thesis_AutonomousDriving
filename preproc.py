# -*- coding: utf-8 -*-
"""preproc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bsiEbXFROsS_hke1sO2yY15JgtbAR5FS

Will have to change the filenames, i.e from kitti_utils to a more general name
"""

import os

import numpy as np

import avod
from lyft_dataset_sdk.lyftdataset import LyftDataset
from avod.builders.dataset_builder import DatasetBuilder
from avod.core.mini_batch_utils import MiniBatchUtils

def do_preprocessing(dataset, indices):
    
    mini_batch_utils = MiniBatchUtils(dataset)
    print("Generating mini batches in {}".format(mini_batch_utils.avod_iou_type))

    # Generate all mini-batches, this can take a long time
    mini_batch_utils.preprocess_rpn_mini_batches(dataset, indices)

    print("Mini batches generated")


def split_indices(dataset, num_children):
    """Splits indices between children

    Args:
        dataset: Dataset object
        num_children: Number of children to split samples between

    Returns:
        indices_split: A list of evenly split indices
    """

    all_indices = np.arange(dataset.num_samples)

    # Pad indices to divide evenly
    length_padding = (-len(all_indices)) % num_children
    padded_indices = np.concatenate((all_indices,
                                     np.zeros(length_padding,
                                              dtype=np.int32)))

    # Split and trim last set of indices to original length
    indices_split = np.split(padded_indices, num_children)
    indices_split[-1] = np.trim_zeros(indices_split[-1])

    return indices_split


def split_work(all_child_pids, dataset, indices_split, num_children):
    """Spawns children to do work

    Args:
        all_child_pids: List of child pids are appended here, the parent
            process should use this list to wait for all children to finish
        dataset: Dataset object
        indices_split: List of indices to split between children
        num_children: Number of children
    """

    for child_idx in range(num_children):
        new_pid = os.fork()
        if new_pid:
            all_child_pids.append(new_pid)
        else:
            indices = indices_split[child_idx]
            print('child', dataset.classes,
                  indices_split[child_idx][0],
                  indices_split[child_idx][-1])
            do_preprocessing(dataset, indices)
            os._exit(0)

DATASET_VERSION = 'v1.02-train'
DATASET_ROOT = '../../nuscenes-devkit/data/'

level5data = LyftDataset(json_path=DATASET_ROOT + "/v1.02-train", data_path=DATASET_ROOT, verbose=True)

classes=[]
for i in level5data.category:
    classes.append(i.get("name"))
classes

print("start processing and clustering..")
do_preprocessing(level5data, None)

print('All Done :)')
