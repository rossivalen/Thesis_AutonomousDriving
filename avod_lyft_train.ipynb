{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING\n",
    "### Get the dataset\n",
    "Load the dataset, split it in two for trainin and validation. As in the Reference model provided by [Lyft](https://level5.lyft.com/), a dataframe with one scene per row is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = 'v1.02-train'\n",
    "DATASET_ROOT = '../../nuscenes-devkit/data/'\n",
    "\n",
    "#The code will generate data, visualization and model checkpoints\n",
    "ARTIFACTS_FOLDER = \"./artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import glob\n",
    "#Disabled for numpy and opencv: avod has opencv and numpy versions for several methods\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import os\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 850M, pci bus id: 0000:0a:00.0, compute capability: 5.0\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.compat.v1.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.4) \n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True, gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "15991 instance,\n",
      "8 sensor,\n",
      "128 calibrated_sensor,\n",
      "149072 ego_pose,\n",
      "148 log,\n",
      "148 scene,\n",
      "18634 sample,\n",
      "149072 sample_data,\n",
      "539765 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 11.7 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 3.8 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "level5data = LyftDataset(json_path=DATASET_ROOT + \"/v1.02-train\", data_path=DATASET_ROOT, verbose=True)\n",
    "os.makedirs(ARTIFACTS_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">TRAINING</font>\n",
    "Based on the [AVOD algorithm](https://github.com/kujason/avod), we train the dataset. \n",
    "The goal is to study how the accuracy changes based on the type of sensors in input, and their number, thus changes to the AVOD algorithm have been made. Here we keep the two stage model.\n",
    "Will be divided in steps, to mimick the divisions made by AVOD's authors in the code.\n",
    "\n",
    "With respect to the original AVOD code, the following changes have been made:\n",
    "<li> Upgrades for compatibity issues with tensorflow 2.0: migrated from slim libs to keras Sequential</li>\n",
    "<li> Changes to support single type input </li>\n",
    "<li> VGGs take as input Lyft-style dataset </li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import avod\n",
    "from avod.core import trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>RPN MODEL</b>: It is the fist subnetwork that makes up the double stage AVOD algorithm. It uses two VGGs, one for images, one for LiDar, to find the bottleneck.\n",
    "Img VGG and Bev VGG have the same strucure, just have input from different sources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>VGG:</b> VGG is a convolutional neural network model. Here simplified model wrt K. Simonyan and A. Zisserman's model proposed in the paper \"Very Deep Convolutional Networks for Large-Scale Image Recognition\".\n",
    "Basically, it lacks dense layers at the end, and the last group of conv layers is smaller that theirs.\n",
    "Two VGGs, one for BEV, one for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anchor_helper\n",
    "from frame_helper import FrameCalibrationData\n",
    "import frame_helper\n",
    "import preproc_helper\n",
    "import bev_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Utente1\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from avod.core import anchor_filter\n",
    "from avod.core import anchor_projector\n",
    "from avod.core import box_3d_encoder\n",
    "from avod.core import constants\n",
    "from avod.core import losses\n",
    "from avod.core import model\n",
    "from avod.core import summary_utils\n",
    "from avod.core.anchor_generators import grid_anchor_3d_generator\n",
    "from avod.datasets.kitti import kitti_aug\n",
    "import avod.datasets.kitti.kitti_utils as kitti_utils\n",
    "from avod.core.label_cluster_utils import LabelClusterUtils\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "class RpnModel(model.DetectionModel):\n",
    "    ##############################\n",
    "    # Keys for Placeholders\n",
    "    ##############################\n",
    "    PL_BEV_INPUT = 'bev_input_pl'\n",
    "    PL_IMG_INPUT = 'img_input_pl'\n",
    "    PL_ANCHORS = 'anchors_pl'\n",
    "\n",
    "    PL_BEV_ANCHORS = 'bev_anchors_pl'\n",
    "    PL_BEV_ANCHORS_NORM = 'bev_anchors_norm_pl'\n",
    "    PL_IMG_ANCHORS = 'img_anchors_pl'\n",
    "    PL_IMG_ANCHORS_NORM = 'img_anchors_norm_pl'\n",
    "    PL_LABEL_ANCHORS = 'label_anchors_pl'\n",
    "    PL_LABEL_BOXES_3D = 'label_boxes_3d_pl'\n",
    "    PL_LABEL_CLASSES = 'label_classes_pl'\n",
    "\n",
    "    PL_ANCHOR_IOUS = 'anchor_ious_pl'\n",
    "    PL_ANCHOR_OFFSETS = 'anchor_offsets_pl'\n",
    "    PL_ANCHOR_CLASSES = 'anchor_classes_pl'\n",
    "\n",
    "    # Sample info, including keys for projection to image space\n",
    "    # (e.g. camera matrix, image index, etc.)\n",
    "    PL_CALIB_P2 = 'frame_calib_p2'\n",
    "    PL_IMG_IDX = 'current_img_idx'\n",
    "    PL_GROUND_PLANE = 'ground_plane'\n",
    "\n",
    "    ##############################\n",
    "    # Keys for Predictions\n",
    "    ##############################\n",
    "    PRED_ANCHORS = 'rpn_anchors'\n",
    "\n",
    "    PRED_MB_OBJECTNESS_GT = 'rpn_mb_objectness_gt'\n",
    "    PRED_MB_OFFSETS_GT = 'rpn_mb_offsets_gt'\n",
    "\n",
    "    PRED_MB_MASK = 'rpn_mb_mask'\n",
    "    PRED_MB_OBJECTNESS = 'rpn_mb_objectness'\n",
    "    PRED_MB_OFFSETS = 'rpn_mb_offsets'\n",
    "\n",
    "    PRED_TOP_INDICES = 'rpn_top_indices'\n",
    "    PRED_TOP_ANCHORS = 'rpn_top_anchors'\n",
    "    PRED_TOP_OBJECTNESS_SOFTMAX = 'rpn_top_objectness_softmax'\n",
    "\n",
    "    ##############################\n",
    "    # Keys for Loss\n",
    "    ##############################\n",
    "    LOSS_RPN_OBJECTNESS = 'rpn_objectness_loss'\n",
    "    LOSS_RPN_REGRESSION = 'rpn_regression_loss'\n",
    "\n",
    "    def __init__(self, model_config, pipeline_config, train_val_test, dataset):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_config: configuration for the model\n",
    "            train_val_test: \"train\", \"val\", or \"test\"\n",
    "            dataset: the dataset that will provide samples and ground truth\n",
    "        \"\"\"\n",
    "\n",
    "        # Sets model configs (_config)\n",
    "        super(RpnModel, self).__init__(model_config)\n",
    "        self.dataset=dataset\n",
    "        self.pipeline_config = pipeline_config\n",
    "\n",
    "        if train_val_test not in [\"train\", \"val\", \"test\"]:\n",
    "            raise ValueError('Invalid train_val_test value,'\n",
    "                             'should be one of [\"train\", \"val\", \"test\"]')\n",
    "        self._train_val_test = train_val_test\n",
    "\n",
    "        self._is_training = (self._train_val_test == 'train')\n",
    "\n",
    "        # Input config\n",
    "        input_config = self._config.input_config\n",
    "        self._bev_pixel_size = np.asarray([input_config.bev_dims_h,\n",
    "                                           input_config.bev_dims_w])\n",
    "        self._bev_depth = input_config.bev_depth\n",
    "\n",
    "        self._img_pixel_size = np.asarray([input_config.img_dims_h,\n",
    "                                           input_config.img_dims_w])\n",
    "        self._img_depth = input_config.img_depth\n",
    "\n",
    "        # Rpn config\n",
    "        rpn_config = self._config.rpn_config\n",
    "        self.proposal_roi_crop_size = 3*2  #3*2\n",
    "        self._fusion_method = rpn_config.rpn_fusion_method\n",
    "\n",
    "        if self._train_val_test in [\"train\", \"val\"]:\n",
    "            self._nms_size = rpn_config.rpn_train_nms_size\n",
    "        else:\n",
    "            self._nms_size = rpn_config.rpn_test_nms_size\n",
    "\n",
    "        self._nms_iou_thresh = rpn_config.rpn_nms_iou_thresh\n",
    "\n",
    "        # Network input placeholders\n",
    "        self.placeholders = dict()\n",
    "\n",
    "        # Inputs to network placeholders\n",
    "        self._placeholder_inputs = dict()\n",
    "\n",
    "        # Information about the current sample\n",
    "        self.sample_info = dict()\n",
    "\n",
    "        # Dataset\n",
    "        classes=[]\n",
    "        for i in self.dataset.category:\n",
    "            classes.append(i.get(\"name\"))\n",
    "        self.classes = classes\n",
    "        self.dataset = dataset\n",
    "        self.dataset.train_val_test = self._train_val_test\n",
    "        area_extents = self.pipeline_config.kitti_utils_config.area_extents\n",
    "        self._area_extents = np.reshape(area_extents, (3, 2))\n",
    "        self._bev_extents = self._area_extents[[0, 2]]\n",
    "        \n",
    "        label_cluster_utils = LabelClusterUtils(self.dataset)\n",
    "        self._cluster_sizes, self._all_std = label_cluster_utils.get_clusters(5, self.dataset)\n",
    "        \n",
    "        anchor_strides = self.pipeline_config.kitti_utils_config.anchor_strides\n",
    "        self._anchor_strides= np.reshape(anchor_strides, (-1, 2))\n",
    "        self._anchor_generator = grid_anchor_3d_generator.GridAnchor3dGenerator()\n",
    "\n",
    "        self._path_drop_probabilities = self._config.path_drop_probabilities\n",
    "        self._train_on_all_samples = self._config.train_on_all_samples\n",
    "        self._eval_all_samples = self._config.eval_all_samples\n",
    "\n",
    "        if self._train_val_test in [\"val\", \"test\"]:\n",
    "            # Disable path-drop, this should already be disabled inside the\n",
    "            # evaluator, but just in case.\n",
    "            self._path_drop_probabilities[0] = 1.0\n",
    "            self._path_drop_probabilities[1] = 1.0\n",
    "\n",
    "    def _add_placeholder(self, dtype, shape, name):\n",
    "        placeholder = tf.compat.v1.placeholder(dtype, shape, name)\n",
    "        self.placeholders[name] = placeholder\n",
    "        return placeholder\n",
    "\n",
    "    def _set_up_input_pls(self):\n",
    "        \"\"\"Sets up input placeholders by adding them to self._placeholders.\n",
    "        Keys are defined as self.PL_*.\n",
    "        \"\"\"\n",
    "        # Combine config data\n",
    "        bev_dims = np.append(self._bev_pixel_size, self._bev_depth)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('bev_input'):\n",
    "            # Placeholder for BEV image input, to be filled in with feed_dict\n",
    "            bev_input_placeholder = self._add_placeholder(tf.float32, bev_dims,\n",
    "                                                          self.PL_BEV_INPUT)\n",
    "\n",
    "            self._bev_input_batches = tf.expand_dims(\n",
    "                bev_input_placeholder, axis=0)\n",
    "\n",
    "            self._bev_preprocessed = tf.image.resize(self._bev_input_batches, self._bev_pixel_size)\n",
    "\n",
    "            # Summary Images\n",
    "            bev_summary_images = tf.split(bev_input_placeholder, self._bev_depth, axis=2)\n",
    "            tf.summary.image(\"bev_maps\", bev_summary_images, max_outputs=self._bev_depth)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('img_input'):\n",
    "            # Take variable size input images\n",
    "            img_input_placeholder = self._add_placeholder(tf.float32, [None, None, self._img_depth],self.PL_IMG_INPUT)\n",
    "\n",
    "            self._img_input_batches = tf.expand_dims(img_input_placeholder, axis=0)\n",
    "\n",
    "            self._img_preprocessed = tf.image.resize(self._img_input_batches, self._img_pixel_size)\n",
    "\n",
    "            # Summary Image\n",
    "            tf.summary.image(\"rgb_image\", self._img_preprocessed, max_outputs=2)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('pl_labels'):\n",
    "            #self._add_placeholder(tf.float32, [None, 6], self.PL_LABEL_ANCHORS)\n",
    "            self._add_placeholder(tf.float32, [None, 7], self.PL_LABEL_BOXES_3D)\n",
    "            #self._add_placeholder(tf.float32, [None], self.PL_LABEL_CLASSES)\n",
    "\n",
    "        # Placeholders for anchors\n",
    "        with tf.compat.v1.variable_scope('pl_anchors'):\n",
    "            self._add_placeholder(tf.float32, [None, 6], self.PL_ANCHORS)\n",
    "            self._add_placeholder(tf.float32, [None], self.PL_ANCHOR_IOUS)\n",
    "            self._add_placeholder(tf.float32, [None, 6], self.PL_ANCHOR_OFFSETS)\n",
    "            self._add_placeholder(tf.float32, [None], self.PL_ANCHOR_CLASSES)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('bev_anchor_projections'):\n",
    "                self._add_placeholder(tf.float32, [None, 4], self.PL_BEV_ANCHORS)\n",
    "                self._bev_anchors_norm_pl = self._add_placeholder( tf.float32, [None, 4], self.PL_BEV_ANCHORS_NORM)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('img_anchor_projections'):\n",
    "                self._add_placeholder(tf.float32, [None, 4], self.PL_IMG_ANCHORS)\n",
    "                self._img_anchors_norm_pl = self._add_placeholder( tf.float32, [None, 4], self.PL_IMG_ANCHORS_NORM)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('sample_info'):\n",
    "                # the calib matrix shape is (3 x 4)\n",
    "                self._add_placeholder( tf.float32, [3, 4], self.PL_CALIB_P2)\n",
    "                self._add_placeholder(tf.int32, shape=[1], name=self.PL_IMG_IDX)\n",
    "                self._add_placeholder(tf.float32, [4], self.PL_GROUND_PLANE)\n",
    "\n",
    "    def _set_up_feature_extractors(self):\n",
    "        \"\"\"Sets up feature extractors and stores feature maps and\n",
    "        bottlenecks as member variables.\n",
    "        \"\"\"\n",
    "        weight_decay=0.0005\n",
    "        #shape due to shape provided by dataset. BEV could not be adapted: too sparse.\n",
    "        \n",
    "        inputs_img = tf.keras.layers.Input(batch_shape=(None,1024,1224,3))\n",
    "        net = tf.keras.layers.Conv2D(filters = 32, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), \n",
    "                                     activation=tf.nn.relu, kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv1\")(inputs_img)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch1\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 32, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), \n",
    "                                     use_bias=False, padding='same', name=\"conv2\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch2\")(net)   \n",
    "\n",
    "        net = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool1\")(net)\n",
    "\n",
    "        net = tf.keras.layers.Conv2D(filters = 64, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv3\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch3\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 64, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv4\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch4\")(net)\n",
    "\n",
    "        net = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool2\")(net)\n",
    "\n",
    "        net = tf.keras.layers.Conv2D(filters = 128,kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv5\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch5\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 128, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), \n",
    "                                     use_bias=False, padding='same', name=\"conv6\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch6\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 128, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv7\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch7\")(net)\n",
    "\n",
    "        net = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool3\")(net)\n",
    "\n",
    "        net = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv8\") (net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch8\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv9\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch9\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv10\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch10\")(net)\n",
    "        \n",
    "        img_vgg = tf.keras.models.Model(inputs = inputs_img, outputs = net, name=\"img_vgg\")\n",
    "        \n",
    "        self.img_bottleneck = tf.keras.layers.Conv2D(filters = 32, kernel_size = [1,1], strides =(1,1), padding='same', name=\"bottleneck\")(net)\n",
    "        self.img_bottleneck= tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros',\n",
    "                                            gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', \n",
    "                                            beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(self.img_bottleneck)\n",
    "        \n",
    "         #shape due to shape provided by dataset. BEV could not be adapted: too sparse.\n",
    "        \n",
    "        inputs_bev = tf.keras.layers.Input(batch_shape=(None,336,336,3))\n",
    "        out = tf.keras.layers.Conv2D(filters = 32, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), \n",
    "                                     activation=tf.nn.relu, kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv1\")(inputs_bev)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch1\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 32, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), \n",
    "                                     use_bias=False, padding='same', name=\"conv2\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch2\")(out)   \n",
    "\n",
    "        out = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool1\")(out)\n",
    "\n",
    "        out = tf.keras.layers.Conv2D(filters = 64, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv3\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch3\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 64, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv4\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch4\")(out)\n",
    "\n",
    "        out = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool2\")(out)\n",
    "\n",
    "        out = tf.keras.layers.Conv2D(filters = 128,kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv5\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch5\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 128, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), \n",
    "                                     use_bias=False, padding='same', name=\"conv6\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch6\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 128, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv7\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch7\")(out)\n",
    "\n",
    "        out = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool3\")(out)\n",
    "\n",
    "        out = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv8\") (out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch8\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv9\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch9\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv10\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch10\")(out)\n",
    "        \n",
    "        bev_vgg = tf.keras.models.Model(inputs = inputs_bev, outputs = out, name=\"bev_vgg\")\n",
    "        \n",
    "        with tf.compat.v1.variable_scope(\"bev_bottleneck\"):\n",
    "            self.bev_bottleneck = tf.keras.layers.Conv2D(filters = 32, kernel_size = [1,1], strides =(1,1), padding='same', name=\"bottleneck\")(out)\n",
    "        with tf.compat.v1.variable_scope(\"img_bottleneck\"):\n",
    "            self.bev_bottleneck= tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros',\n",
    "                                            gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', \n",
    "                                            beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(self.bev_bottleneck)\n",
    "\n",
    "        # # Visualize the end point feature maps being used\n",
    "        bev_vgg.summary()\n",
    "        bev_end_point=bev_vgg.get_config()\n",
    "        img_vgg.summary()\n",
    "        img_end_point=img_vgg.get_config()\n",
    "\n",
    "    def build(self):\n",
    "\n",
    "        # Setup input placeholders\n",
    "        self._set_up_input_pls()\n",
    "\n",
    "        # Setup feature extractors\n",
    "        self._set_up_feature_extractors()\n",
    "\n",
    "        bev_proposal_input = self.bev_bottleneck\n",
    "        img_proposal_input = self.img_bottleneck\n",
    "\n",
    "        fusion_mean_div_factor = 2.0\n",
    "\n",
    "        # If both img and bev probabilites are set to 1.0, don't do\n",
    "        # path drop.\n",
    "        if not (self._path_drop_probabilities[0] ==\n",
    "                self._path_drop_probabilities[1] == 1.0):\n",
    "            with tf.compat.v1.variable_scope('rpn_path_drop'):\n",
    "\n",
    "                random_values = tf.random_uniform(shape=[3],\n",
    "                                                  minval=0.0,\n",
    "                                                  maxval=1.0)\n",
    "\n",
    "                img_mask, bev_mask = self.create_path_drop_masks(\n",
    "                    self._path_drop_probabilities[0],\n",
    "                    self._path_drop_probabilities[1],\n",
    "                    random_values)\n",
    "\n",
    "                img_proposal_input = tf.multiply(img_proposal_input,\n",
    "                                                 img_mask)\n",
    "\n",
    "                bev_proposal_input = tf.multiply(bev_proposal_input,\n",
    "                                                 bev_mask)\n",
    "\n",
    "                self.img_path_drop_mask = img_mask\n",
    "                self.bev_path_drop_mask = bev_mask\n",
    "\n",
    "                # Overwrite the division factor\n",
    "                fusion_mean_div_factor = img_mask + bev_mask\n",
    "\n",
    "        with tf.compat.v1.variable_scope('proposal_roi_pooling'):\n",
    "\n",
    "            with tf.compat.v1.variable_scope('box_indices'):\n",
    "                def get_box_indices(boxes):\n",
    "                    proposals_shape = boxes.get_shape().as_list()\n",
    "                    if any(dim is None for dim in proposals_shape):\n",
    "                        proposals_shape = tf.shape(boxes)\n",
    "                    ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)\n",
    "                    multiplier = tf.expand_dims(\n",
    "                        tf.range(start=0, limit=proposals_shape[0]), 1)\n",
    "                    return tf.reshape(ones_mat * multiplier, [-1])\n",
    "\n",
    "                bev_boxes_norm_batches = tf.expand_dims(\n",
    "                    self._bev_anchors_norm_pl, axis=0)\n",
    "\n",
    "                # These should be all 0's since there is only 1 image\n",
    "                tf_box_indices = get_box_indices(bev_boxes_norm_batches)\n",
    "            \n",
    "            proposal_roi_size_tf = [3,3]\n",
    "            # Do ROI Pooling on BEV\n",
    "            bev_proposal_rois = tf.image.crop_and_resize(\n",
    "                bev_proposal_input,\n",
    "                self._bev_anchors_norm_pl,\n",
    "                tf_box_indices,\n",
    "                proposal_roi_size_tf)\n",
    "            # Do ROI Pooling on image\n",
    "            img_proposal_rois = tf.image.crop_and_resize(\n",
    "                img_proposal_input,\n",
    "                self._bev_anchors_norm_pl,\n",
    "                tf_box_indices,\n",
    "                proposal_roi_size_tf)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('proposal_roi_fusion'):\n",
    "            rpn_fusion_out = None\n",
    "            if self._fusion_method == 'mean':\n",
    "                tf_features_sum = tf.add(bev_proposal_rois, img_proposal_rois)\n",
    "                #rpn_fusion_out = tf.divide(tf_features_sum, fusion_mean_div_factor)\n",
    "                rpn_fusion_out = tf.divide(tf_features_sum, 2)\n",
    "            elif self._fusion_method == 'concat':\n",
    "                rpn_fusion_out = tf.concat(\n",
    "                    [bev_proposal_rois, img_proposal_rois], axis=3)\n",
    "            else:\n",
    "                raise ValueError('Invalid fusion method', self._fusion_method)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('anchor_predictor', 'ap', [rpn_fusion_out]):\n",
    "            #None because unknown\n",
    "            tensor_in = tf.keras.Input(shape=None, tensor=rpn_fusion_out)\n",
    "            print(\"here\", tf_features_sum)\n",
    "            # Rpn layers config\n",
    "            weight_decay = 0.005\n",
    "\n",
    "            # Use conv2d instead of fully_connected layers.\n",
    "            cls_fc6 = tf.keras.layers.Conv2D(filters=32, kernel_size = [3,3], kernel_initializer='ones', \n",
    "                                             kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='VALID', name=\"conv1\")(tensor_in)\n",
    "\n",
    "            cls_fc6_drop = tf.keras.layers.Dropout(rate = 0.5, name=\"drop1\")(cls_fc6)\n",
    "\n",
    "            cls_fc7 = tf.keras.layers.Conv2D(filters=32, kernel_size = [1,1], kernel_initializer='ones',\n",
    "                                             kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='VALID', name=\"conv2\")(cls_fc6_drop)\n",
    "\n",
    "            cls_fc7_drop = tf.keras.layers.Dropout(rate = 0.5, name=\"drop2\")(cls_fc7)\n",
    "\n",
    "            cls_fc8 = tf.keras.layers.Conv2D(filters=2, kernel_size = [1,1], kernel_initializer='ones',\n",
    "                                             kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='VALID', name=\"conv3\")(cls_fc7_drop)\n",
    "\n",
    "            objectness = tf.squeeze(cls_fc8, axis=[1,2], name='conv3/squeezed')\n",
    "\n",
    "            # Use conv2d instead of fully_connected layers.\n",
    "            reg_fc6 = tf.keras.layers.Conv2D(filters=32, kernel_size = [3,3], kernel_initializer=\"ones\", \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='VALID', name=\"conv4\")(tensor_in)\n",
    "\n",
    "            reg_fc6_drop = tf.keras.layers.Dropout(rate = 0.5, name=\"drop3\")(reg_fc6)\n",
    "\n",
    "            reg_fc7 = tf.keras.layers.Conv2D(filters = 16, kernel_size = [1, 1], kernel_initializer=\"ones\",\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding=\"same\", name=\"conv5\")(reg_fc6_drop)\n",
    "\n",
    "            reg_fc7_drop = tf.keras.layers.Dropout(rate = 0.5, name=\"drop4\")(reg_fc7)\n",
    "\n",
    "            reg_fc8 = tf.keras.layers.Conv2D(filters = 6,  kernel_size = [1, 1],  kernel_initializer=\"ones\",\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding=\"same\", name=\"conv6\")(reg_fc7_drop)\n",
    "\n",
    "            offsets = tf.squeeze(reg_fc8, axis=[1,2], name='conv6/squeezed')\n",
    "            \n",
    "            model = tf.keras.models.Model(inputs = rpn_fusion_out, outputs = offsets, name=\"rpn_fusion_prediction_anchors\")\n",
    "            model1 = tf.keras.models.Model(inputs = rpn_fusion_out, outputs = objectness, name=\"objectness predictions\")\n",
    "            model.summary()\n",
    "            model1.summary()\n",
    "        # Return the proposals\n",
    "        with tf.compat.v1.variable_scope('proposals'):\n",
    "            anchors = self.placeholders[self.PL_ANCHORS]\n",
    "\n",
    "            # Decode anchor regression offsets\n",
    "            with tf.compat.v1.variable_scope('decoding'):\n",
    "                regressed_anchors = anchor_helper.offset_to_anchor( anchors, offsets)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('bev_projection'):\n",
    "                _, bev_proposal_boxes_norm = anchor_projector.project_to_bev(regressed_anchors, self._bev_extents)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('softmax'):\n",
    "                objectness_softmax = tf.nn.softmax(objectness)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('nms'):\n",
    "                objectness_scores = objectness_softmax[:, 1]\n",
    "\n",
    "                # Do NMS on regressed anchors\n",
    "                top_indices = tf.image.non_max_suppression(\n",
    "                    bev_proposal_boxes_norm, objectness_scores,\n",
    "                    max_output_size=self._nms_size,\n",
    "                    iou_threshold=self._nms_iou_thresh)\n",
    "\n",
    "                top_anchors = tf.gather(regressed_anchors, top_indices)\n",
    "                top_objectness_softmax = tf.gather(objectness_scores,\n",
    "                                                   top_indices)\n",
    "                # top_offsets = tf.gather(offsets, top_indices)\n",
    "                # top_objectness = tf.gather(objectness, top_indices)\n",
    "\n",
    "        # Get mini batch\n",
    "        all_ious_gt = self.placeholders[self.PL_ANCHOR_IOUS]\n",
    "        all_offsets_gt = self.placeholders[self.PL_ANCHOR_OFFSETS]\n",
    "        all_classes_gt = self.placeholders[self.PL_ANCHOR_CLASSES]\n",
    "\n",
    "        with tf.compat.v1.variable_scope('mini_batch'):\n",
    "            mini_batch_mask, _ = anchor_helper.sample_mini_batch(all_ious_gt, 64,[0, 0.3], [0.5,1])\n",
    "\n",
    "        # ROI summary images\n",
    "        rpn_mini_batch_size =64\n",
    "        with tf.compat.v1.variable_scope('bev_rpn_rois'):\n",
    "            mb_bev_anchors_norm = tf.boolean_mask(self._bev_anchors_norm_pl,\n",
    "                                                  mini_batch_mask)\n",
    "            mb_bev_box_indices = tf.zeros_like(\n",
    "                tf.boolean_mask(all_classes_gt, mini_batch_mask),\n",
    "                dtype=tf.int32)\n",
    "\n",
    "            # Show the ROIs of the BEV input density map\n",
    "            # for the mini batch anchors\n",
    "            bev_input_rois = tf.image.crop_and_resize(self._bev_preprocessed,\n",
    "                                                      mb_bev_anchors_norm, mb_bev_box_indices, (32, 32))\n",
    "\n",
    "            bev_input_roi_summary_images = tf.split(bev_input_rois, self._bev_depth, axis=3)\n",
    "            tf.summary.image('bev_rpn_rois', bev_input_roi_summary_images[-1], max_outputs=rpn_mini_batch_size)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('img_rpn_rois'):\n",
    "            # ROIs on image input\n",
    "            mb_img_anchors_norm = tf.boolean_mask(self._img_anchors_norm_pl, mini_batch_mask)\n",
    "            mb_img_box_indices = tf.zeros_like( tf.boolean_mask(all_classes_gt, mini_batch_mask), dtype=tf.int32)\n",
    "\n",
    "            # Do test ROI pooling on mini batch\n",
    "            img_input_rois = tf.image.crop_and_resize( self._img_preprocessed,\n",
    "                                                      mb_img_anchors_norm, mb_img_box_indices, (32, 32))\n",
    "\n",
    "            tf.summary.image('img_rpn_rois', img_input_rois, max_outputs=rpn_mini_batch_size)\n",
    "\n",
    "        # Ground Truth Tensors\n",
    "        with tf.compat.v1.variable_scope('one_hot_classes'):\n",
    "\n",
    "            # Anchor classification ground truth\n",
    "            # Object / Not Object\n",
    "            min_pos_iou = 0.5\n",
    "\n",
    "            objectness_classes_gt = tf.cast(tf.greater_equal(all_ious_gt, min_pos_iou), dtype=tf.int32)\n",
    "            objectness_gt = tf.one_hot(objectness_classes_gt, depth=2, on_value=1.0 - self._config.label_smoothing_epsilon,\n",
    "                                       off_value=self._config.label_smoothing_epsilon)\n",
    "\n",
    "        # Mask predictions for mini batch\n",
    "        with tf.compat.v1.variable_scope('prediction_mini_batch'):\n",
    "            objectness_masked = tf.boolean_mask(objectness, mini_batch_mask)\n",
    "            offsets_masked = tf.boolean_mask(offsets, mini_batch_mask)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('ground_truth_mini_batch'):\n",
    "            objectness_gt_masked = tf.boolean_mask(objectness_gt, mini_batch_mask)\n",
    "            offsets_gt_masked = tf.boolean_mask(all_offsets_gt, mini_batch_mask)\n",
    "\n",
    "        # Specify the tensors to evaluate\n",
    "        predictions = dict()\n",
    "\n",
    "        # Temporary predictions for debugging\n",
    "#         predictions['anchor_ious'] = anchor_ious\n",
    "#         predictions['anchor_offsets'] = all_offsets_gt\n",
    "\n",
    "        if self._train_val_test in ['train', 'val']:\n",
    "            # All anchors\n",
    "            predictions[self.PRED_ANCHORS] = anchors\n",
    "\n",
    "            # Mini-batch masks\n",
    "            predictions[self.PRED_MB_MASK] = mini_batch_mask\n",
    "            # Mini-batch predictions\n",
    "            predictions[self.PRED_MB_OBJECTNESS] = objectness_masked\n",
    "            predictions[self.PRED_MB_OFFSETS] = offsets_masked\n",
    "\n",
    "            # Mini batch ground truth\n",
    "            predictions[self.PRED_MB_OFFSETS_GT] = offsets_gt_masked\n",
    "            predictions[self.PRED_MB_OBJECTNESS_GT] = objectness_gt_masked\n",
    "\n",
    "            # Proposals after nms\n",
    "            predictions[self.PRED_TOP_INDICES] = top_indices\n",
    "            predictions[self.PRED_TOP_ANCHORS] = top_anchors\n",
    "            predictions[\n",
    "                self.PRED_TOP_OBJECTNESS_SOFTMAX] = top_objectness_softmax\n",
    "\n",
    "        else:\n",
    "            # self._train_val_test == 'test'\n",
    "            predictions[self.PRED_TOP_ANCHORS] = top_anchors\n",
    "            predictions[\n",
    "                self.PRED_TOP_OBJECTNESS_SOFTMAX] = top_objectness_softmax\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def create_feed_dict(self, scene_index=None):\n",
    "        \"\"\" Fills in the placeholders with the actual input values.\n",
    "            Currently, only a batch size of 1 is supported\n",
    "\n",
    "        Args:\n",
    "            sample_index: optional, only used when train_val_test == 'test',\n",
    "                a particular sample index in the dataset\n",
    "                sample list to build the feed_dict for\n",
    "\n",
    "        Returns:\n",
    "            a feed_dict dictionary that can be used in a tensorflow session\n",
    "        \"\"\"\n",
    "#TODO fix to have multiple batches\n",
    "#         if self._train_val_test in [\"train\", \"val\"]:\n",
    "\n",
    "#             # sample_index should be None\n",
    "#             if sample_index is not None:\n",
    "#                 raise ValueError('sample_index should be None. Do not load '\n",
    "#                                  'particular samples during train or val')\n",
    "\n",
    "#             # During training/validation, we need a valid sample\n",
    "#             # with anchor info for loss calculation\n",
    "#             sample = None\n",
    "#             anchors_info = []\n",
    "\n",
    "#             valid_sample = False\n",
    "#             while not valid_sample:\n",
    "#                 if self._train_val_test == \"train\":\n",
    "#                     # Get the a random sample from the remaining epoch\n",
    "#                     samples = self.dataset.next_batch(batch_size=1)\n",
    "\n",
    "#                 else:  # self._train_val_test == \"val\"\n",
    "#                     # Load samples in order for validation\n",
    "#                     samples = self.dataset.next_batch(batch_size=1, shuffle=False)\n",
    "\n",
    "#                 # Only handle one sample at a time for now\n",
    "#                 sample = samples[0]\n",
    "#                 anchors_info = sample.get(constants.KEY_ANCHORS_INFO)\n",
    "\n",
    "#                 # When training, if the mini batch is empty, go to the next\n",
    "#                 # sample. Otherwise carry on with found the valid sample.\n",
    "#                 # For validation, even if 'anchors_info' is empty, keep the\n",
    "#                 # sample (this will help penalize false positives.)\n",
    "#                 # We will substitue the necessary info with zeros later on.\n",
    "#                 # Note: Training/validating all samples can be switched off.\n",
    "#                 train_cond = (self._train_val_test == \"train\" and self._train_on_all_samples)\n",
    "#                 eval_cond = (self._train_val_test == \"val\" and self._eval_all_samples)\n",
    "#                 if anchors_info or train_cond or eval_cond:\n",
    "#                     valid_sample = True\n",
    "#         else:\n",
    "        # For testing, any sample should work\n",
    "        if scene_index is not None:\n",
    "            my_scene = self.dataset.scene[scene_index]\n",
    "        else:\n",
    "            raise TypeError('for testing you need to put a number! will change it later on once it works fully :) ')\n",
    "    \n",
    "        \n",
    "        # Only handle one sample at a time for now\n",
    "        my_sample_token = my_scene[\"first_sample_token\"]\n",
    "        sample = self.dataset.get('sample', my_sample_token)\n",
    "        sample_name = sample.get(\"token\")\n",
    "        \n",
    "        last_sample_token = my_scene[\"last_sample_token\"]\n",
    "\n",
    "        # We only need orientation from box_3\n",
    "        anchors_info, obj_classes, label_classes, label_anchors, label_boxes_3d = preproc_helper.load_sample_info(sample_name, self.classes, self.dataset)\n",
    "        \n",
    "        # Network input data\n",
    "        img_input = self.dataset.get('sample_data', sample['data'][\"CAM_FRONT\"])\n",
    "        img_data=img_input\n",
    "        camera_token=img_input.get(\"token\")\n",
    "        file_name=self.dataset.get_sample_data_path(camera_token)\n",
    "        image = Image.open(file_name)\n",
    "        # convert image to numpy array\n",
    "        img_input = np.asarray(image)\n",
    "        bev_input = self.dataset.get('sample_data', sample['data'][\"LIDAR_TOP\"])\n",
    "        bev_data = bev_input\n",
    "        bev_token= bev_input.get(\"token\")\n",
    "        lidar_data = self.dataset.get(\"sample_data\", bev_token)\n",
    "        lidar_filepath = self.dataset.get_sample_data_path(bev_token)\n",
    "        ego_pose = self.dataset.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "        calibrated_sensor_lidar = self.dataset.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n",
    "        global_from_car = transform_matrix(ego_pose['translation'], Quaternion(ego_pose['rotation']), inverse=False)\n",
    "        car_from_sensor_lidar = transform_matrix(calibrated_sensor_lidar['translation'], Quaternion(calibrated_sensor_lidar['rotation']),\n",
    "                                                  inverse=False)\n",
    "        lidar_pointcloud = LidarPointCloud.from_file(lidar_filepath)\n",
    "        lidar_pointcloud.transform(car_from_sensor_lidar)\n",
    "        map_mask = level5data.map[0][\"mask\"]\n",
    "        voxel_size = (0.4,0.4,1.5)\n",
    "        z_offset = -2.0\n",
    "        #arbitrary shape, must be square though!\n",
    "        bev_shape = (336,336, 3)\n",
    "        bev = bev_helper.create_voxel_pointcloud(lidar_pointcloud.points, bev_shape, voxel_size=voxel_size, z_offset=z_offset)\n",
    "        ego_centric_map = bev_helper.get_semantic_map_around_ego(map_mask, ego_pose, voxel_size=0.4, output_shape=(336,336)) \n",
    "        bev_input = bev_helper.normalize_voxel_intensities(bev)\n",
    "\n",
    "        # Image shape (h, w)\n",
    "        image_shape = [img_data.get(\"height\"), img_data.get(\"width\")]\n",
    "        \n",
    "        #ground plane shape (a,b,c,d) in kitti:\n",
    "        #no info on ground plane in nuscenes data, just global coordinate system\n",
    "        #which is given as x, y, z. Computed from the cameras position:\n",
    "        #suppose as in kitti that ground plane is as the same level with the cameras\n",
    "                       \n",
    "        cam_front_token = self.dataset.get('sample_data', sample['data'][\"CAM_FRONT\"])\n",
    "        cam_front_data = cam_front_token.get(\"calibrated_sensor_token\")\n",
    "        cam_front_calib = self.dataset.get(\"calibrated_sensor\", cam_front_data )\n",
    "        cam_front_coords = cam_front_calib.get(\"translation\")\n",
    "\n",
    "        cam_front_left_token = self.dataset.get('sample_data', sample['data'][\"CAM_FRONT_LEFT\"])\n",
    "        cam_front_left_data = cam_front_left_token.get(\"calibrated_sensor_token\")\n",
    "        cam_front_left_calib = self.dataset.get(\"calibrated_sensor\", cam_front_left_data )\n",
    "        cam_front_left_coords = cam_front_left_calib.get(\"translation\")\n",
    "\n",
    "        cam_front_right_token = self.dataset.get('sample_data', sample['data'][\"CAM_FRONT_RIGHT\"])\n",
    "        cam_front_right_data = cam_front_right_token.get(\"calibrated_sensor_token\")\n",
    "        cam_front_right_calib = self.dataset.get(\"calibrated_sensor\", cam_front_right_data )\n",
    "        cam_front_right_coords = cam_front_right_calib.get(\"translation\")\n",
    "        \n",
    "        ground_plane = frame_helper.get_ground_plane_coeff(cam_front_coords, cam_front_left_coords, cam_front_right_coords)\n",
    "        \n",
    "        #only for cameras, of course lidars do not have instrinsic matrices\n",
    "        token=img_data.get(\"calibrated_sensor_token\") \n",
    "        stereo_calib_p2 = frame_helper.read_calibration(token, self.dataset)\n",
    "\n",
    "        # Fill the placeholders for anchor information\n",
    "        self._fill_anchor_pl_inputs(anchors_info=anchors_info,sample_token=bev_token, ground_plane=ground_plane,\n",
    "                                    image_shape=image_shape, stereo_calib_p2=stereo_calib_p2,\n",
    "                                    sample_name=sample_name)\n",
    "\n",
    "        # Fill in the rest\n",
    "        self._placeholder_inputs[self.PL_BEV_INPUT] = bev_input\n",
    "        self._placeholder_inputs[self.PL_IMG_INPUT] = img_input\n",
    "\n",
    "        self._placeholder_inputs[self.PL_LABEL_ANCHORS] = label_anchors\n",
    "        self._placeholder_inputs[self.PL_LABEL_BOXES_3D] = label_boxes_3d\n",
    "        self._placeholder_inputs[self.PL_LABEL_CLASSES] = label_classes\n",
    "\n",
    "        # Sample Info\n",
    "        # img_idx is a list to match the placeholder shape\n",
    "        self._placeholder_inputs[self.PL_IMG_IDX] = [str(sample_name)]\n",
    "        self._placeholder_inputs[self.PL_CALIB_P2] = stereo_calib_p2\n",
    "        self._placeholder_inputs[self.PL_GROUND_PLANE] = ground_plane\n",
    "\n",
    "        # Temporary sample info for debugging\n",
    "        self.sample_info.clear()\n",
    "        self.sample_info['sample_name'] = sample\n",
    "        self.sample_info['rpn_mini_batch'] = anchors_info\n",
    "\n",
    "        # Create a feed_dict and fill it with input values\n",
    "        feed_dict = dict()\n",
    "        for key, value in self.placeholders.items():\n",
    "            feed_dict[value] = self._placeholder_inputs[key]\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def _fill_anchor_pl_inputs(self,\n",
    "                               sample_token,\n",
    "                               anchors_info,\n",
    "                               ground_plane,\n",
    "                               image_shape,\n",
    "                               stereo_calib_p2,\n",
    "                               sample_name):\n",
    "        \"\"\"\n",
    "        Fills anchor placeholder inputs with corresponding data\n",
    "\n",
    "        Args:\n",
    "            anchors_info: anchor info from mini_batch_utils\n",
    "            ground_plane: ground plane coefficients\n",
    "            image_shape: image shape (h, w), used for projecting anchors\n",
    "            sample_name: name of the sample, e.g. \"000001\"\n",
    "        \"\"\"\n",
    "\n",
    "        # Lists for merging anchors info\n",
    "        all_anchor_boxes_3d = []\n",
    "        anchors_ious = []\n",
    "        anchor_offsets = []\n",
    "        anchor_classes = []\n",
    "        \n",
    "        # Create anchors for each class\n",
    "        if len(self.classes) > 1:\n",
    "            for class_idx in range(len(self.classes)):\n",
    "                cluster_sizes = []\n",
    "                for i in self._cluster_sizes[class_idx]:\n",
    "                    if i!=[]:\n",
    "                        cluster_sizes.append(i)\n",
    "                if len(cluster_sizes)!=0:\n",
    "                # Generate anchors for all classes\n",
    "                    grid_anchor_boxes_3d = self._anchor_generator.generate(\n",
    "                        area_3d=self._area_extents,\n",
    "                        anchor_3d_sizes=cluster_sizes,\n",
    "                        anchor_stride=self._anchor_strides[0],\n",
    "                        ground_plane=ground_plane)\n",
    "                else:\n",
    "                    #no labels per class, no anchor per class\n",
    "                    grid_anchor_boxes_3d=[]\n",
    "                all_anchor_boxes_3d.append(grid_anchor_boxes_3d)\n",
    "#             length=[]\n",
    "#             for i in grid_anchor_boxes_3d:\n",
    "#                 length.append(len(i))\n",
    "#             if not all(i==length[0] for i in length):\n",
    "#                 max_length=np.amax(length)\n",
    "#                 for all \n",
    "            \n",
    "            all_anchor_boxes_3d = np.concatenate(all_anchor_boxes_3d, axis=None)\n",
    "        else:\n",
    "            # Don't loop for a single class\n",
    "            class_idx = 0\n",
    "            cluster_sizes[class_idx] = [x for x in self._cluster_sizes[class_idx] if x != []]\n",
    "            if self._cluster_sizes[class_idx]!=[]:\n",
    "                grid_anchor_boxes_3d = self._anchor_generator.generate(\n",
    "                    area_3d=self._area_extents,\n",
    "                    anchor_3d_sizes=cluster_sizes[class_idx],\n",
    "                    anchor_stride=self._anchor_strides[0],\n",
    "                    ground_plane=ground_plane)\n",
    "                all_anchor_boxes_3d = grid_anchor_boxes_3d\n",
    "\n",
    "        # Filter empty anchors\n",
    "        # Skip if anchors_info is []\n",
    "        sample_has_labels = True\n",
    "\n",
    "        # Convert lists to ndarrays\n",
    "        #already filtered them before\n",
    "        anchor_boxes_3d_to_use = all_anchor_boxes_3d\n",
    "        anchors_ious = np.asarray(anchors_ious)\n",
    "        anchor_offsets = np.asarray(anchor_offsets)\n",
    "        anchor_classes = np.asarray(anchor_classes)\n",
    "\n",
    "        # Flip anchors and centroid x offsets for augmented samples\n",
    "#             if kitti_aug.AUG_FLIPPING in sample_augs:\n",
    "#                 anchor_boxes_3d_to_use = kitti_aug.flip_boxes_3d(anchor_boxes_3d_to_use, flip_ry=False)\n",
    "#                 if anchors_info:\n",
    "#                     anchor_offsets[:, 0] = -anchor_offsets[:, 0]\n",
    "\n",
    "        # Convert to anchors\n",
    "        anchors_to_use = box_3d_encoder.box_3d_to_anchor( anchor_boxes_3d_to_use)\n",
    "        num_anchors = len(anchors_to_use)\n",
    "\n",
    "        # Project anchors into bev\n",
    "        print(anchors_to_use.shape, self._bev_extents)\n",
    "        bev_anchors, bev_anchors_norm = anchor_projector.project_to_bev( anchors_to_use, self._bev_extents)\n",
    "\n",
    "        # Project box_3d anchors into image space\n",
    "        img_anchors, img_anchors_norm = anchor_projector.project_to_image_space(anchors_to_use, stereo_calib_p2, image_shape)\n",
    "\n",
    "        # Reorder into [y1, x1, y2, x2] for tf.crop_and_resize op\n",
    "        self._bev_anchors_norm = bev_anchors_norm[:, [1, 0, 3, 2]]\n",
    "        self._img_anchors_norm = img_anchors_norm[:, [1, 0, 3, 2]]\n",
    "\n",
    "        # Fill in placeholder inputs\n",
    "        self._placeholder_inputs[self.PL_ANCHORS] = anchors_to_use\n",
    "\n",
    "        # If we are in train/validation mode, and the anchor infos\n",
    "        # are not empty, store them. Checking for just anchors_ious\n",
    "        # to be non-empty should be enough.\n",
    "        if self._train_val_test in ['train', 'val'] and \\\n",
    "                len(anchors_ious) > 0:\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_IOUS] = anchors_ious\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_OFFSETS] = anchor_offsets\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_CLASSES] = anchor_classes\n",
    "\n",
    "        # During test, or val when there is no anchor info\n",
    "        elif self._train_val_test in ['test'] or \\\n",
    "                len(anchors_ious) == 0:\n",
    "            # During testing, or validation with no gt, fill these in with 0s\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_IOUS] = \\\n",
    "                np.zeros(num_anchors)\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_OFFSETS] = \\\n",
    "                np.zeros([num_anchors, 6])\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_CLASSES] = \\\n",
    "                np.zeros(num_anchors)\n",
    "        else:\n",
    "            raise ValueError('Got run mode {}, and non-empty anchor info'.\n",
    "                             format(self._train_val_test))\n",
    "\n",
    "        self._placeholder_inputs[self.PL_BEV_ANCHORS] = bev_anchors\n",
    "        self._placeholder_inputs[self.PL_BEV_ANCHORS_NORM] = self._bev_anchors_norm\n",
    "        self._placeholder_inputs[self.PL_IMG_ANCHORS] = img_anchors\n",
    "        self._placeholder_inputs[self.PL_IMG_ANCHORS_NORM] = self._img_anchors_norm\n",
    "\n",
    "    def loss(self, prediction_dict):\n",
    "\n",
    "        # these should include mini-batch values only\n",
    "        objectness_gt = prediction_dict[self.PRED_MB_OBJECTNESS_GT]\n",
    "        offsets_gt = prediction_dict[self.PRED_MB_OFFSETS_GT]\n",
    "\n",
    "        # Predictions\n",
    "        with tf.compat.v1.variable_scope('rpn_prediction_mini_batch'):\n",
    "            objectness = prediction_dict[self.PRED_MB_OBJECTNESS]\n",
    "            offsets = prediction_dict[self.PRED_MB_OFFSETS]\n",
    "\n",
    "        with tf.compat.v1.variable_scope('rpn_losses'):\n",
    "            with tf.compat.v1.variable_scope('objectness'):\n",
    "                cls_loss = losses.WeightedSoftmaxLoss()\n",
    "                cls_loss_weight = self._config.loss_config.cls_loss_weight\n",
    "                objectness_loss = cls_loss(objectness, objectness_gt, weight=cls_loss_weight)\n",
    "\n",
    "                with tf.compat.v1.variable_scope('obj_norm'):\n",
    "                    # normalize by the number of anchor mini-batches\n",
    "                    objectness_loss = objectness_loss / tf.cast( tf.shape(objectness_gt)[0], dtype=tf.float32)\n",
    "                    tf.summary.scalar('objectness', objectness_loss)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('regression'):\n",
    "                reg_loss = losses.WeightedSmoothL1Loss()\n",
    "                reg_loss_weight = self._config.loss_config.reg_loss_weight\n",
    "                anchorwise_localization_loss = reg_loss(offsets, offsets_gt, weight=reg_loss_weight)\n",
    "                masked_localization_loss = anchorwise_localization_loss * objectness_gt[:, 1]\n",
    "                localization_loss = tf.reduce_sum(masked_localization_loss)\n",
    "\n",
    "                with tf.compat.v1.variable_scope('reg_norm'):\n",
    "                    # normalize by the number of positive objects\n",
    "                    num_positives = tf.reduce_sum(objectness_gt[:, 1])\n",
    "                    # Assert the condition `num_positives > 0`\n",
    "                    with tf.control_dependencies([tf.debugging.assert_positive(num_positives)]):\n",
    "                        localization_loss = localization_loss / num_positives\n",
    "                        tf.summary.scalar('regression', localization_loss)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('total_loss'):\n",
    "                total_loss = objectness_loss + localization_loss\n",
    "\n",
    "        loss_dict = {\n",
    "            self.LOSS_RPN_OBJECTNESS: objectness_loss,\n",
    "            self.LOSS_RPN_REGRESSION: localization_loss,\n",
    "        }\n",
    "\n",
    "        return loss_dict, total_loss\n",
    "\n",
    "    def create_path_drop_masks(self,\n",
    "                               p_img,\n",
    "                               p_bev,\n",
    "                               random_values):\n",
    "        \"\"\"Determines global path drop decision based on given probabilities.\n",
    "\n",
    "        Args:\n",
    "            p_img: A tensor of float32, probability of keeping image branch\n",
    "            p_bev: A tensor of float32, probability of keeping bev branch\n",
    "            random_values: A tensor of float32 of shape [3], the results\n",
    "                of coin flips, values should range from 0.0 - 1.0.\n",
    "\n",
    "        Returns:\n",
    "            final_img_mask: A constant tensor mask containing either one or zero\n",
    "                depending on the final coin flip probability.\n",
    "            final_bev_mask: A constant tensor mask containing either one or zero\n",
    "                depending on the final coin flip probability.\n",
    "        \"\"\"\n",
    "\n",
    "        def keep_branch(): return tf.constant(1.0)\n",
    "\n",
    "        def kill_branch(): return tf.constant(0.0)\n",
    "\n",
    "        # The logic works as follows:\n",
    "        # We have flipped 3 coins, first determines the chance of keeping\n",
    "        # the image branch, second determines keeping bev branch, the third\n",
    "        # makes the final decision in the case where both branches were killed\n",
    "        # off, otherwise the initial img and bev chances are kept.\n",
    "\n",
    "        img_chances = tf.case([(tf.less(random_values[0], p_img), keep_branch)], default=kill_branch)\n",
    "\n",
    "        bev_chances = tf.case([(tf.less(random_values[1], p_bev), keep_branch)], default=kill_branch)\n",
    "\n",
    "        # Decision to determine whether both branches were killed off\n",
    "        third_flip = tf.logical_or(tf.cast(img_chances, dtype=tf.bool), tf.cast(bev_chances, dtype=tf.bool))\n",
    "        third_flip = tf.cast(third_flip, dtype=tf.float32)\n",
    "\n",
    "        # Make a second choice, for the third case\n",
    "        # Here we use a 50/50 chance to keep either image or bev\n",
    "        # If its greater than 0.5, keep the image\n",
    "        img_second_flip = tf.case([(tf.greater(random_values[2], 0.5), keep_branch)], default=kill_branch)\n",
    "        # If its less than or equal to 0.5, keep bev\n",
    "        bev_second_flip = tf.case([(tf.less_equal(random_values[2], 0.5), keep_branch)],\n",
    "                                  default=kill_branch)\n",
    "\n",
    "        # Use lambda since this returns another condition and it needs to\n",
    "        # be callable\n",
    "        final_img_mask = tf.case([(tf.equal(third_flip, 1), lambda: img_chances)], default=lambda: img_second_flip)\n",
    "\n",
    "        final_bev_mask = tf.case([(tf.equal(third_flip, 1), lambda: bev_chances)], default=lambda: bev_second_flip)\n",
    "\n",
    "        return final_img_mask, final_bev_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test this part!!\n",
    "\n",
    "Tests and results for the RPN model part. The following changes to the model were done after errors/issues with testing:\n",
    "<li>Maybe change the use of placeholders in the future, to fit with eager execution (shorter code)</li>\n",
    "<li>Some internal keras os function rises a warning, something will be depreated, doesn't tell where and what function exactly</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "density_threshold: 1\n",
      "rpn_config {\n",
      "  iou_2d_thresholds {\n",
      "    neg_iou_lo: 0.0\n",
      "    neg_iou_hi: 0.3\n",
      "    pos_iou_lo: 0.5\n",
      "    pos_iou_hi: 1.0\n",
      "  }\n",
      "  mini_batch_size: 64\n",
      "}\n",
      "avod_config {\n",
      "  iou_2d_thresholds {\n",
      "    neg_iou_lo: 0.0\n",
      "    neg_iou_hi: 0.55\n",
      "    pos_iou_lo: 0.65\n",
      "    pos_iou_hi: 1.0\n",
      "  }\n",
      "  mini_batch_size: 64\n",
      "}\n",
      "\n",
      "Clustering labels 126 / 126\n",
      "Finished reading labels, clustering data...\n",
      "\n",
      "\n",
      "Finished \n",
      " 9\n",
      "Model: \"bev_vgg\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 336, 336, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 336, 336, 32)      864       \n",
      "_________________________________________________________________\n",
      "batch1 (BatchNormalization)  (None, 336, 336, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 336, 336, 32)      9216      \n",
      "_________________________________________________________________\n",
      "batch2 (BatchNormalization)  (None, 336, 336, 32)      128       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 168, 168, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 168, 168, 64)      18432     \n",
      "_________________________________________________________________\n",
      "batch3 (BatchNormalization)  (None, 168, 168, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 168, 168, 64)      36864     \n",
      "_________________________________________________________________\n",
      "batch4 (BatchNormalization)  (None, 168, 168, 64)      256       \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 84, 84, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv5 (Conv2D)               (None, 84, 84, 128)       73728     \n",
      "_________________________________________________________________\n",
      "batch5 (BatchNormalization)  (None, 84, 84, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv6 (Conv2D)               (None, 84, 84, 128)       147456    \n",
      "_________________________________________________________________\n",
      "batch6 (BatchNormalization)  (None, 84, 84, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv7 (Conv2D)               (None, 84, 84, 128)       147456    \n",
      "_________________________________________________________________\n",
      "batch7 (BatchNormalization)  (None, 84, 84, 128)       512       \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 42, 42, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv8 (Conv2D)               (None, 42, 42, 256)       294912    \n",
      "_________________________________________________________________\n",
      "batch8 (BatchNormalization)  (None, 42, 42, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv9 (Conv2D)               (None, 42, 42, 256)       589824    \n",
      "_________________________________________________________________\n",
      "batch9 (BatchNormalization)  (None, 42, 42, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv10 (Conv2D)              (None, 42, 42, 256)       589824    \n",
      "_________________________________________________________________\n",
      "batch10 (BatchNormalization) (None, 42, 42, 256)       1024      \n",
      "=================================================================\n",
      "Total params: 1,913,952\n",
      "Trainable params: 1,911,264\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "Model: \"img_vgg\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1024, 1224, 3)]   0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 1024, 1224, 32)    864       \n",
      "_________________________________________________________________\n",
      "batch1 (BatchNormalization)  (None, 1024, 1224, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 1024, 1224, 32)    9216      \n",
      "_________________________________________________________________\n",
      "batch2 (BatchNormalization)  (None, 1024, 1224, 32)    128       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 512, 612, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 512, 612, 64)      18432     \n",
      "_________________________________________________________________\n",
      "batch3 (BatchNormalization)  (None, 512, 612, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 512, 612, 64)      36864     \n",
      "_________________________________________________________________\n",
      "batch4 (BatchNormalization)  (None, 512, 612, 64)      256       \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 256, 306, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv5 (Conv2D)               (None, 256, 306, 128)     73728     \n",
      "_________________________________________________________________\n",
      "batch5 (BatchNormalization)  (None, 256, 306, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv6 (Conv2D)               (None, 256, 306, 128)     147456    \n",
      "_________________________________________________________________\n",
      "batch6 (BatchNormalization)  (None, 256, 306, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv7 (Conv2D)               (None, 256, 306, 128)     147456    \n",
      "_________________________________________________________________\n",
      "batch7 (BatchNormalization)  (None, 256, 306, 128)     512       \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 128, 153, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv8 (Conv2D)               (None, 128, 153, 256)     294912    \n",
      "_________________________________________________________________\n",
      "batch8 (BatchNormalization)  (None, 128, 153, 256)     1024      \n",
      "_________________________________________________________________\n",
      "conv9 (Conv2D)               (None, 128, 153, 256)     589824    \n",
      "_________________________________________________________________\n",
      "batch9 (BatchNormalization)  (None, 128, 153, 256)     1024      \n",
      "_________________________________________________________________\n",
      "conv10 (Conv2D)              (None, 128, 153, 256)     589824    \n",
      "_________________________________________________________________\n",
      "batch10 (BatchNormalization) (None, 128, 153, 256)     1024      \n",
      "=================================================================\n",
      "Total params: 1,913,952\n",
      "Trainable params: 1,911,264\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "here Tensor(\"proposal_roi_fusion/Add:0\", shape=(?, 3, 3, 32), dtype=float32)\n",
      "Model: \"rpn_fusion_prediction_anchors\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 3, 3, 32)]        0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 1, 1, 32)          9248      \n",
      "_________________________________________________________________\n",
      "drop3 (Dropout)              (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv5 (Conv2D)               (None, 1, 1, 16)          528       \n",
      "_________________________________________________________________\n",
      "drop4 (Dropout)              (None, 1, 1, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv6 (Conv2D)               (None, 1, 1, 6)           102       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_anchor_predictor [(None, 6)]               0         \n",
      "=================================================================\n",
      "Total params: 9,878\n",
      "Trainable params: 9,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"objectness predictions\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 3, 3, 32)]        0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 1, 1, 32)          9248      \n",
      "_________________________________________________________________\n",
      "drop1 (Dropout)              (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 1, 1, 32)          1056      \n",
      "_________________________________________________________________\n",
      "drop2 (Dropout)              (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 1, 1, 2)           66        \n",
      "_________________________________________________________________\n",
      "tf_op_layer_anchor_predictor [(None, 2)]               0         \n",
      "=================================================================\n",
      "Total params: 10,370\n",
      "Trainable params: 10,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente1\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:810: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179200, 6) [[-40.  40.]\n",
      " [  0.  70.]]\n",
      "(179200, 1) (179200, 1) (179200, 1) (179200, 1)\n",
      "Tensor(\"rpn_losses/total_loss/add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import avod.builders.config_builder_util as config_build\n",
    "config_path = 'avod/configs/unittest_model.config'\n",
    "pipe_path = 'avod/configs/unittest_pipeline.config'\n",
    "model_config = config_build.get_model_config_from_file(config_path)\n",
    "pipeline_config=config_build.get_configs_from_pipeline_file(pipe_path, \"val\")\n",
    "\n",
    "print(pipeline_config[3].kitti_utils_config.mini_batch_config)\n",
    "rpn_model = RpnModel(model_config, pipeline_config[3],\n",
    "                         train_val_test=\"val\",\n",
    "                         dataset=level5data)\n",
    "\n",
    "predictions = rpn_model.build()\n",
    "\n",
    "loss, total_loss = rpn_model.loss(predictions)\n",
    "\n",
    "feed_dict = rpn_model.create_feed_dict(5)\n",
    "\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tf_agents.utils\n",
    "import time\n",
    "\n",
    "from avod.builders import optimizer_builder\n",
    "from avod.core import trainer_utils\n",
    "from avod.core import summary_utils\n",
    "\n",
    "def train(model, train_config):\n",
    "    \"\"\"Training function for detection models.\n",
    "\n",
    "    Args:\n",
    "        model: The detection model object.\n",
    "        train_config: a train_*pb2 protobuf.\n",
    "            training i.e. loading RPN weights onto AVOD model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = model\n",
    "    train_config = train_config\n",
    "    # Get model configurations\n",
    "    model_config = model.model_config\n",
    "\n",
    "    # Create a variable tensor to hold the global step\n",
    "    global_step_tensor = tf.Variable(\n",
    "        0, trainable=False, name='global_step')\n",
    "\n",
    "    #############################\n",
    "    # Get training configurations\n",
    "    #############################\n",
    "    max_iterations = train_config.max_iterations\n",
    "    summary_interval = train_config.summary_interval\n",
    "    checkpoint_interval = train_config.checkpoint_interval\n",
    "    max_checkpoints = train_config.max_checkpoints_to_keep\n",
    "\n",
    "    checkpoint_dir = \"avod/checkpoints\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    checkpoint_path = checkpoint_dir + '/' + model_config.checkpoint_name\n",
    "    \n",
    "    logdir = \"avod/configs\"\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    \n",
    "\n",
    "    global_summaries = set([])\n",
    "\n",
    "    # The model should return a dictionary of predictions\n",
    "    prediction_dict = model.build()\n",
    "\n",
    "    ##############################\n",
    "    # Setup loss\n",
    "    ##############################\n",
    "    losses_dict, total_loss = model.loss(prediction_dict)\n",
    "\n",
    "    # Optimizer\n",
    "    training_optimizer = optimizer_builder.build( train_config.optimizer, global_summaries, global_step_tensor)\n",
    "\n",
    "    # Create the train op\n",
    "    with tf.compat.v1.variable_scope('train_op'):\n",
    "        #train_op = slim.learning.create_train_op(\n",
    "         train_op = tf_agents.utils.eager_utils.create_train_op(\n",
    "             total_loss,\n",
    "             training_optimizer,\n",
    "             global_step=global_step_tensor)\n",
    "\n",
    "    # Save checkpoints regularly.\n",
    "    saver = tf.compat.v1.train.Saver(max_to_keep=max_checkpoints, pad_step_number=True)\n",
    "\n",
    "    # Add the result of the train_op to the summary\n",
    "    tf.compat.v1.summary.scalar(\"training_loss\", train_op)\n",
    "\n",
    "    # Add maximum memory usage summary op\n",
    "    # This op can only be run on device with gpu\n",
    "    # so it's skipped on travis\n",
    "\n",
    "    summaries = set(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.SUMMARIES))\n",
    "    summary_merged = summary_utils.summaries_to_keep(summaries, global_summaries, histograms=False, input_imgs=False, input_bevs=False)\n",
    "\n",
    "    allow_gpu_mem_growth = train_config.allow_gpu_mem_growth\n",
    "   \n",
    "    # GPU memory config\n",
    "    gpus = tf.compat.v1.config.experimental.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.4) \n",
    "    sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True, gpu_options=gpu_options))\n",
    "\n",
    "\n",
    "    # Create unique folder name using datetime for summary writer\n",
    "    datetime_str = str(datetime.datetime.now())\n",
    "    datetime_split = datetime_str.split()\n",
    "    logdir = logdir + '/train'\n",
    "    train_writer = tf.compat.v1.summary.FileWriter(logdir + '/' + datetime_split[0], sess.graph)\n",
    "\n",
    "    # Create init op\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "    # Continue from last saved checkpoint\n",
    "    #if not train_config.overwrite_checkpoints:  solve later, issue in the if. runtime error idk\n",
    "    if not True:\n",
    "        trainer_utils.load_checkpoints(checkpoint_dir, saver)\n",
    "        if len(saver.last_checkpoints) > 0:\n",
    "            checkpoint_to_restore = saver.last_checkpoints[-1]\n",
    "            saver.restore(sess, checkpoint_to_restore)\n",
    "        else:\n",
    "            # Initialize the variables\n",
    "            sess.run(init)\n",
    "    else:\n",
    "        # Initialize the variables\n",
    "        sess.run(init)\n",
    "\n",
    "    # Read the global step if restored\n",
    "    global_step = tf.compat.v1.train.global_step(sess, global_step_tensor)\n",
    "    print('Starting from step {} / {}'.format(global_step, max_iterations))\n",
    "\n",
    "    # Main Training Loop\n",
    "    last_time = time.time()\n",
    "    for step in range(global_step, max_iterations + 1):\n",
    "\n",
    "        # Save checkpoint\n",
    "        if step % checkpoint_interval == 0:\n",
    "            global_step = tf.compat.v1.train.global_step(sess, global_step_tensor)\n",
    "\n",
    "            saver.save(sess, save_path=checkpoint_path, global_step=global_step)\n",
    "\n",
    "            print('Step {} / {}, Checkpoint saved to {}-{:08d}'.format(\n",
    "                step, max_iterations,\n",
    "                checkpoint_path, global_step))\n",
    "\n",
    "        # Create feed_dict for inferencing\n",
    "        feed_dict = model.create_feed_dict(5)\n",
    "\n",
    "        # Write summaries and train op\n",
    "        if step % summary_interval == 0:\n",
    "            current_time = time.time()\n",
    "            time_elapsed = current_time - last_time\n",
    "            last_time = current_time\n",
    "\n",
    "            train_op_loss, summary_out = sess.run([train_op, summary_merged], feed_dict=feed_dict)\n",
    "\n",
    "            print('Step {}, Total Loss {:0.3f}, Time Elapsed {:0.3f} s'.format(step, train_op_loss, time_elapsed))\n",
    "            train_writer.add_summary(summary_out, step)\n",
    "\n",
    "        else:\n",
    "            # Run the train op only\n",
    "            sess.run(train_op, feed_dict)\n",
    "\n",
    "    # Close the summary writers\n",
    "    train_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bev_vgg\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 336, 336, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 336, 336, 32)      864       \n",
      "_________________________________________________________________\n",
      "batch1 (BatchNormalization)  (None, 336, 336, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 336, 336, 32)      9216      \n",
      "_________________________________________________________________\n",
      "batch2 (BatchNormalization)  (None, 336, 336, 32)      128       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 168, 168, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 168, 168, 64)      18432     \n",
      "_________________________________________________________________\n",
      "batch3 (BatchNormalization)  (None, 168, 168, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 168, 168, 64)      36864     \n",
      "_________________________________________________________________\n",
      "batch4 (BatchNormalization)  (None, 168, 168, 64)      256       \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 84, 84, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv5 (Conv2D)               (None, 84, 84, 128)       73728     \n",
      "_________________________________________________________________\n",
      "batch5 (BatchNormalization)  (None, 84, 84, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv6 (Conv2D)               (None, 84, 84, 128)       147456    \n",
      "_________________________________________________________________\n",
      "batch6 (BatchNormalization)  (None, 84, 84, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv7 (Conv2D)               (None, 84, 84, 128)       147456    \n",
      "_________________________________________________________________\n",
      "batch7 (BatchNormalization)  (None, 84, 84, 128)       512       \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 42, 42, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv8 (Conv2D)               (None, 42, 42, 256)       294912    \n",
      "_________________________________________________________________\n",
      "batch8 (BatchNormalization)  (None, 42, 42, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv9 (Conv2D)               (None, 42, 42, 256)       589824    \n",
      "_________________________________________________________________\n",
      "batch9 (BatchNormalization)  (None, 42, 42, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv10 (Conv2D)              (None, 42, 42, 256)       589824    \n",
      "_________________________________________________________________\n",
      "batch10 (BatchNormalization) (None, 42, 42, 256)       1024      \n",
      "=================================================================\n",
      "Total params: 1,913,952\n",
      "Trainable params: 1,911,264\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "Model: \"img_vgg\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 1024, 1224, 3)]   0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 1024, 1224, 32)    864       \n",
      "_________________________________________________________________\n",
      "batch1 (BatchNormalization)  (None, 1024, 1224, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 1024, 1224, 32)    9216      \n",
      "_________________________________________________________________\n",
      "batch2 (BatchNormalization)  (None, 1024, 1224, 32)    128       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 512, 612, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 512, 612, 64)      18432     \n",
      "_________________________________________________________________\n",
      "batch3 (BatchNormalization)  (None, 512, 612, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 512, 612, 64)      36864     \n",
      "_________________________________________________________________\n",
      "batch4 (BatchNormalization)  (None, 512, 612, 64)      256       \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 256, 306, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv5 (Conv2D)               (None, 256, 306, 128)     73728     \n",
      "_________________________________________________________________\n",
      "batch5 (BatchNormalization)  (None, 256, 306, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv6 (Conv2D)               (None, 256, 306, 128)     147456    \n",
      "_________________________________________________________________\n",
      "batch6 (BatchNormalization)  (None, 256, 306, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv7 (Conv2D)               (None, 256, 306, 128)     147456    \n",
      "_________________________________________________________________\n",
      "batch7 (BatchNormalization)  (None, 256, 306, 128)     512       \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 128, 153, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv8 (Conv2D)               (None, 128, 153, 256)     294912    \n",
      "_________________________________________________________________\n",
      "batch8 (BatchNormalization)  (None, 128, 153, 256)     1024      \n",
      "_________________________________________________________________\n",
      "conv9 (Conv2D)               (None, 128, 153, 256)     589824    \n",
      "_________________________________________________________________\n",
      "batch9 (BatchNormalization)  (None, 128, 153, 256)     1024      \n",
      "_________________________________________________________________\n",
      "conv10 (Conv2D)              (None, 128, 153, 256)     589824    \n",
      "_________________________________________________________________\n",
      "batch10 (BatchNormalization) (None, 128, 153, 256)     1024      \n",
      "=================================================================\n",
      "Total params: 1,913,952\n",
      "Trainable params: 1,911,264\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "here Tensor(\"proposal_roi_fusion_1/Add:0\", shape=(?, 3, 3, 32), dtype=float32)\n",
      "Model: \"rpn_fusion_prediction_anchors\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 3, 3, 32)]        0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 1, 1, 32)          9248      \n",
      "_________________________________________________________________\n",
      "drop3 (Dropout)              (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv5 (Conv2D)               (None, 1, 1, 16)          528       \n",
      "_________________________________________________________________\n",
      "drop4 (Dropout)              (None, 1, 1, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv6 (Conv2D)               (None, 1, 1, 6)           102       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_anchor_predictor [(None, 6)]               0         \n",
      "=================================================================\n",
      "Total params: 9,878\n",
      "Trainable params: 9,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"objectness predictions\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 3, 3, 32)]        0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 1, 1, 32)          9248      \n",
      "_________________________________________________________________\n",
      "drop1 (Dropout)              (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 1, 1, 32)          1056      \n",
      "_________________________________________________________________\n",
      "drop2 (Dropout)              (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 1, 1, 2)           66        \n",
      "_________________________________________________________________\n",
      "tf_op_layer_anchor_predictor [(None, 2)]               0         \n",
      "=================================================================\n",
      "Total params: 10,370\n",
      "Trainable params: 10,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 850M, pci bus id: 0000:0a:00.0, compute capability: 5.0\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_config = pipeline_config[1]\n",
    "train(rpn_model, train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> AVOD MODEL: </b> second stage detector for the AVOD algorithm. It uses FPN as feature extractors.\n",
    "<b> FPN: </b> Feature Pyramid Network (FPN) is a feature extractor designed for such pyramid concept with accuracy and speed in mind. It replaces the feature extractor of detectors like Faster R-CNN and generates multiple feature map layers (multi-scale feature maps) with better quality information than the regular feature pyramid for object detection. [Understanding Feature Pyramid Networks for object detection (FPN)](https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avod.core.models import avod_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_agents.utils\n",
    "train_op = tf_agents.utils.eager_utils.create_train_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n=0\n",
    "x= np.array([1,2,3])\n",
    "n=x\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST AVOD!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
