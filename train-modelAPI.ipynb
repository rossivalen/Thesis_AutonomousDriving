{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING\n",
    "### Get the dataset\n",
    "Load the dataset, split it in two for trainin and validation. As in the Reference model provided by [Lyft](https://level5.lyft.com/), a dataframe with one scene per row is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = 'v1.02-train'\n",
    "DATASET_ROOT = '../../nuscenes-devkit/data/'\n",
    "\n",
    "#The code will generate data, visualization and model checkpoints\n",
    "ARTIFACTS_FOLDER = \"./artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import glob\n",
    "#Disabled for numpy and opencv: avod has opencv and numpy versions for several methods\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import os\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import argparse\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 850M, pci bus id: 0000:0a:00.0, compute capability: 5.0\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.compat.v1.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.4) \n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True, gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "15991 instance,\n",
      "8 sensor,\n",
      "128 calibrated_sensor,\n",
      "149072 ego_pose,\n",
      "148 log,\n",
      "148 scene,\n",
      "18634 sample,\n",
      "149072 sample_data,\n",
      "539765 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 11.4 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 3.8 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "level5data = LyftDataset(json_path=DATASET_ROOT + \"/v1.02-train\", data_path=DATASET_ROOT, verbose=True)\n",
    "os.makedirs(ARTIFACTS_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">TRAINING</font>\n",
    "Based on the [AVOD algorithm](https://github.com/kujason/avod), we train the dataset. \n",
    "The goal is to study how the accuracy changes based on the type of sensors in input, and their number, thus changes to the AVOD algorithm have been made. Here we keep the two stage model.\n",
    "Will be divided in steps, to mimick the divisions made by AVOD's authors in the code.\n",
    "\n",
    "With respect to the original AVOD code, the following changes have been made:\n",
    "<li> Upgrades for compatibity issues with tensorflow 2.0: migrated from slim libs to keras Sequential</li>\n",
    "<li> Changes to support single type input </li>\n",
    "<li> VGGs take as input Lyft-style dataset </li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import avod\n",
    "from avod.core import trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>RPN MODEL</b>: It is the fist subnetwork that makes up the double stage AVOD algorithm. It uses two VGGs, one for images, one for LiDar, to find the bottleneck.\n",
    "Img VGG and Bev VGG have the same strucure, just have input from different sources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>VGG:</b> VGG is a convolutional neural network model. Here simplified model wrt K. Simonyan and A. Zisserman's model proposed in the paper \"Very Deep Convolutional Networks for Large-Scale Image Recognition\".\n",
    "Basically, it lacks dense layers at the end, and the last group of conv layers is smaller that theirs.\n",
    "Two VGGs, one for BEV, one for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anchor_helper\n",
    "from frame_helper import FrameCalibrationData\n",
    "import frame_helper\n",
    "import preproc_helper\n",
    "import bev_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from avod.core import anchor_filter\n",
    "from avod.core import anchor_projector\n",
    "from avod.core import box_3d_encoder\n",
    "from avod.core import constants\n",
    "from avod.core import losses\n",
    "from avod.core import model\n",
    "from avod.core import summary_utils\n",
    "from avod.core.anchor_generators import grid_anchor_3d_generator\n",
    "from avod.datasets.kitti import kitti_aug\n",
    "import avod.datasets.kitti.kitti_utils as kitti_utils\n",
    "from avod.core.label_cluster_utils import LabelClusterUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RpnModel(model.DetectionModel):\n",
    "    ##############################\n",
    "    # Keys for Placeholders\n",
    "    ##############################\n",
    "    PL_BEV_INPUT = 'bev_input_pl'\n",
    "    PL_IMG_INPUT = 'img_input_pl'\n",
    "    PL_ANCHORS = 'anchors_pl'\n",
    "\n",
    "    PL_BEV_ANCHORS = 'bev_anchors_pl'\n",
    "    PL_BEV_ANCHORS_NORM = 'bev_anchors_norm_pl'\n",
    "    PL_IMG_ANCHORS = 'img_anchors_pl'\n",
    "    PL_IMG_ANCHORS_NORM = 'img_anchors_norm_pl'\n",
    "    PL_LABEL_ANCHORS = 'label_anchors_pl'\n",
    "    PL_LABEL_BOXES_3D = 'label_boxes_3d_pl'\n",
    "    PL_LABEL_CLASSES = 'label_classes_pl'\n",
    "\n",
    "    PL_ANCHOR_IOUS = 'anchor_ious_pl'\n",
    "    PL_ANCHOR_OFFSETS = 'anchor_offsets_pl'\n",
    "    PL_ANCHOR_CLASSES = 'anchor_classes_pl'\n",
    "\n",
    "    # Sample info, including keys for projection to image space\n",
    "    # (e.g. camera matrix, image index, etc.)\n",
    "    PL_CALIB_P2 = 'frame_calib_p2'\n",
    "    PL_IMG_IDX = 'current_img_idx'\n",
    "    PL_GROUND_PLANE = 'ground_plane'\n",
    "\n",
    "    ##############################\n",
    "    # Keys for Predictions\n",
    "    ##############################\n",
    "    PRED_ANCHORS = 'rpn_anchors'\n",
    "\n",
    "    PRED_MB_OBJECTNESS_GT = 'rpn_mb_objectness_gt'\n",
    "    PRED_MB_OFFSETS_GT = 'rpn_mb_offsets_gt'\n",
    "\n",
    "    PRED_MB_MASK = 'rpn_mb_mask'\n",
    "    PRED_MB_OBJECTNESS = 'rpn_mb_objectness'\n",
    "    PRED_MB_OFFSETS = 'rpn_mb_offsets'\n",
    "\n",
    "    PRED_TOP_INDICES = 'rpn_top_indices'\n",
    "    PRED_TOP_ANCHORS = 'rpn_top_anchors'\n",
    "    PRED_TOP_OBJECTNESS_SOFTMAX = 'rpn_top_objectness_softmax'\n",
    "\n",
    "    ##############################\n",
    "    # Keys for Loss\n",
    "    ##############################\n",
    "    LOSS_RPN_OBJECTNESS = 'rpn_objectness_loss'\n",
    "    LOSS_RPN_REGRESSION = 'rpn_regression_loss'\n",
    "\n",
    "    def __init__(self, model_config, pipeline_config, train_val_test, dataset):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_config: configuration for the model\n",
    "            train_val_test: \"train\", \"val\", or \"test\"\n",
    "            dataset: the dataset that will provide samples and ground truth\n",
    "        \"\"\"\n",
    "\n",
    "        # Sets model configs (_config)\n",
    "        super(RpnModel, self).__init__(model_config)\n",
    "        self.dataset=dataset\n",
    "        self.pipeline_config = pipeline_config\n",
    "\n",
    "        if train_val_test not in [\"train\", \"val\", \"test\"]:\n",
    "            raise ValueError('Invalid train_val_test value,'\n",
    "                             'should be one of [\"train\", \"val\", \"test\"]')\n",
    "        self._train_val_test = train_val_test\n",
    "\n",
    "        self._is_training = (self._train_val_test == 'train')\n",
    "\n",
    "        # Input config\n",
    "        input_config = self._config.input_config\n",
    "        self._bev_pixel_size = np.asarray([input_config.bev_dims_h, input_config.bev_dims_w])\n",
    "        self._bev_depth = input_config.bev_depth\n",
    "        self._img_pixel_size = np.asarray([input_config.img_dims_h, input_config.img_dims_w])\n",
    "        self._img_depth = input_config.img_depth\n",
    "\n",
    "        # Rpn config\n",
    "        rpn_config = self._config.rpn_config\n",
    "        self.proposal_roi_crop_size = 3*2  #3*2\n",
    "        self._fusion_method = rpn_config.rpn_fusion_method\n",
    "        if self._train_val_test in [\"train\", \"val\"]:\n",
    "            self._nms_size = rpn_config.rpn_train_nms_size\n",
    "        else:\n",
    "            self._nms_size = rpn_config.rpn_test_nms_size\n",
    "\n",
    "        self._nms_iou_thresh = rpn_config.rpn_nms_iou_thresh\n",
    "\n",
    "#         # Network input placeholders\n",
    "#         self.placeholders = dict()\n",
    "#         # Inputs to network placeholders\n",
    "#         self._placeholder_inputs = dict()\n",
    "#         # Information about the current sample\n",
    "#         self.sample_info = dict()\n",
    "\n",
    "        # Dataset\n",
    "        classes=[]\n",
    "        for i in self.dataset.category:\n",
    "            classes.append(i.get(\"name\"))\n",
    "        self.classes = classes\n",
    "        self.dataset = dataset\n",
    "        self.dataset.train_val_test = self._train_val_test\n",
    "        area_extents = self.pipeline_config.kitti_utils_config.area_extents\n",
    "        self._area_extents = np.reshape(area_extents, (3, 2))\n",
    "        self._bev_extents = self._area_extents[[0, 2]]\n",
    "        \n",
    "        label_cluster_utils = LabelClusterUtils(self.dataset)\n",
    "        self._cluster_sizes, self._all_std = label_cluster_utils.get_clusters(5, self.dataset)\n",
    "        \n",
    "        anchor_strides = self.pipeline_config.kitti_utils_config.anchor_strides\n",
    "        self._anchor_strides= np.reshape(anchor_strides, (-1, 2))\n",
    "        self._anchor_generator = grid_anchor_3d_generator.GridAnchor3dGenerator()\n",
    "\n",
    "        self._path_drop_probabilities = self._config.path_drop_probabilities\n",
    "        self._train_on_all_samples = self._config.train_on_all_samples\n",
    "        self._eval_all_samples = self._config.eval_all_samples\n",
    "        \n",
    "        # Inputs-initialize empty, call method to fill. \n",
    "        self.img_input=[]\n",
    "        self.bev_input=[]\n",
    "        \n",
    "        # Anchors\n",
    "        self._anchors=[]\n",
    "        self._anchor_offsets=[]\n",
    "        self._anchor_ious=[]\n",
    "        self._anchor_classes=[]\n",
    "        self._bev_anchors=[]\n",
    "        self._img_anchors=[]\n",
    "        self._bev_anchors_norm=[]\n",
    "        self._img_anchors_norm=[]\n",
    "        \n",
    "        # Labels\n",
    "        self._label_anchors=[]\n",
    "        self._label_classes=[]\n",
    "        self._label_boxes_3d=[]\n",
    "        \n",
    "        # Other stuff\n",
    "        self._sample_name=[]\n",
    "        self._calib=[]\n",
    "        self._ground_plane=[]\n",
    "        \n",
    "\n",
    "        if self._train_val_test in [\"val\", \"test\"]:\n",
    "            # Disable path-drop, this should already be disabled inside the\n",
    "            # evaluator, but just in case.\n",
    "            self._path_drop_probabilities[0] = 1.0\n",
    "            self._path_drop_probabilities[1] = 1.0\n",
    "\n",
    "    def _add_placeholder(self, dtype, shape, name):\n",
    "        placeholder = tf.compat.v1.placeholder(dtype, shape, name)\n",
    "        self.placeholders[name] = placeholder\n",
    "        return placeholder\n",
    "\n",
    "    def _set_up_input_pls(self):\n",
    "        \"\"\"Sets up input placeholders by adding them to self._placeholders.\n",
    "        Keys are defined as self.PL_*.\n",
    "        \"\"\"\n",
    "        # Combine config data\n",
    "        bev_dims = np.append(self._bev_pixel_size, self._bev_depth)\n",
    "        \n",
    "\n",
    "        with tf.compat.v1.variable_scope('bev_input'):\n",
    "            # Placeholder for BEV image input, to be filled in with feed_dict\n",
    "            bev_input_placeholder = self._add_placeholder(tf.float32, bev_dims,\n",
    "                                                          self.PL_BEV_INPUT)\n",
    "\n",
    "            self._bev_input_batches = tf.expand_dims(\n",
    "                bev_input_placeholder, axis=0)\n",
    "\n",
    "            self._bev_preprocessed = tf.image.resize(self._bev_input_batches, self._bev_pixel_size)\n",
    "\n",
    "            # Summary Images\n",
    "            bev_summary_images = tf.split(bev_input_placeholder, self._bev_depth, axis=2)\n",
    "            tf.summary.image(\"bev_maps\", bev_summary_images, max_outputs=self._bev_depth)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('img_input'):\n",
    "            # Take variable size input images\n",
    "            img_input_placeholder = self._add_placeholder(tf.float32, [None, None, self._img_depth],self.PL_IMG_INPUT)\n",
    "\n",
    "            self._img_input_batches = tf.expand_dims(img_input_placeholder, axis=0)\n",
    "\n",
    "            self._img_preprocessed = tf.image.resize(self._img_input_batches, self._img_pixel_size)\n",
    "\n",
    "            # Summary Image\n",
    "            tf.summary.image(\"rgb_image\", self._img_preprocessed, max_outputs=2)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('pl_labels'):\n",
    "            self._add_placeholder(tf.float32, [None, 6], self.PL_LABEL_ANCHORS)\n",
    "            self._add_placeholder(tf.float32, [None, 7], self.PL_LABEL_BOXES_3D)\n",
    "            #self._add_placeholder(tf.float32, [None], self.PL_LABEL_CLASSES)\n",
    "\n",
    "        # Placeholders for anchors\n",
    "#         with tf.compat.v1.variable_scope('pl_anchors'):\n",
    "#             self._add_placeholder(tf.float32, [None, 6], self.PL_ANCHORS)\n",
    "#             self._add_placeholder(tf.float32, [None], self.PL_ANCHOR_IOUS)\n",
    "#             self._add_placeholder(tf.float32, [None, 6], self.PL_ANCHOR_OFFSETS)\n",
    "#             self._add_placeholder(tf.float32, [None], self.PL_ANCHOR_CLASSES)\n",
    "\n",
    "#             with tf.compat.v1.variable_scope('bev_anchor_projections'):\n",
    "#                 self._add_placeholder(tf.float32, [None, 4], self.PL_BEV_ANCHORS)\n",
    "#                 self._bev_anchors_norm_pl = self._add_placeholder( tf.float32, [None, 4], self.PL_BEV_ANCHORS_NORM)\n",
    "\n",
    "#             with tf.compat.v1.variable_scope('img_anchor_projections'):\n",
    "#                 self._add_placeholder(tf.float32, [None, 4], self.PL_IMG_ANCHORS)\n",
    "#                 self._img_anchors_norm_pl = self._add_placeholder( tf.float32, [None, 4], self.PL_IMG_ANCHORS_NORM)\n",
    "\n",
    "#             with tf.compat.v1.variable_scope('sample_info'):\n",
    "#                 # the calib matrix shape is (3 x 4)\n",
    "#                 self._add_placeholder( tf.float32, [3, 4], self.PL_CALIB_P2)\n",
    "#                 self._add_placeholder(tf.int32, shape=[1], name=self.PL_IMG_IDX)\n",
    "#                 self._add_placeholder(tf.float32, [4], self.PL_GROUND_PLANE)\n",
    "        \n",
    "\n",
    "    def build(self):\n",
    "        #call here fill input??\n",
    "        self.feed_input(5)\n",
    "        # Setup feature extractors: two models, one for each kind of input\n",
    "        weight_decay=0.0005\n",
    "        #shape due to shape provided by dataset. BEV could not be adapted: too sparse.\n",
    "        \n",
    "        print(\"\\nStarting with the extractors..\\n\")\n",
    "        with tf.device('/CPU:0'):\n",
    "            inputs_vgg = tf.keras.layers.Input(shape=(224,224,3))\n",
    "            vgg19 = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
    "            bev_data = tf.keras.applications.vgg19.preprocess_input(self.bev_input)\n",
    "            bev_features=vgg19.predict(bev_data)\n",
    "            #struggle with memory\n",
    "            img_data = tf.keras.applications.vgg19.preprocess_input(self.img_input)\n",
    "            img_features=vgg19.predict(img_data)\n",
    "\n",
    "            bev_proposal_input = self.bev_bottleneck\n",
    "            img_proposal_input = self.img_bottleneck\n",
    "\n",
    "            fusion_mean_div_factor = 2.0\n",
    "        print(\"\\nFINISHED FEATURE EXTRACTION\\n\")\n",
    "        \n",
    "        # Select what to keep btw imgs and bev\n",
    "        if not (self._path_drop_probabilities[0] ==\n",
    "                self._path_drop_probabilities[1] == 1.0):\n",
    "            with tf.compat.v1.variable_scope('rpn_path_drop'):\n",
    "\n",
    "                random_values = tf.random_uniform(shape=[3], minval=0.0, maxval=1.0)\n",
    "                img_mask, bev_mask = self.create_path_drop_masks(self._path_drop_probabilities[0], self._path_drop_probabilities[1], random_values)\n",
    "                img_proposal_input = tf.multiply(img_proposal_input, img_mask)\n",
    "                bev_proposal_input = tf.multiply(bev_proposal_input, bev_mask)\n",
    "                self.img_path_drop_mask = img_mask\n",
    "                self.bev_path_drop_mask = bev_mask\n",
    "\n",
    "                # Overwrite the division factor\n",
    "                fusion_mean_div_factor = img_mask + bev_mask\n",
    "        print(\"\\nStarting ROI pooling..\\n\")\n",
    "        \n",
    "        with tf.compat.v1.variable_scope('proposal_roi_pooling'):\n",
    "\n",
    "            with tf.compat.v1.variable_scope('box_indices'):\n",
    "                def get_box_indices(boxes):\n",
    "                    proposals_shape = boxes.get_shape().as_list()\n",
    "                    if any(dim is None for dim in proposals_shape):\n",
    "                        proposals_shape = tf.shape(boxes)\n",
    "                    ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)\n",
    "                    multiplier = tf.expand_dims(tf.range(start=0, limit=proposals_shape[0]), 1)\n",
    "                    return tf.reshape(ones_mat * multiplier, [-1])\n",
    "\n",
    "                bev_boxes_norm_batches = tf.expand_dims(self._bev_anchors_norm, axis=0)\n",
    "                tf_box_indices = get_box_indices(bev_boxes_norm_batches)\n",
    "            \n",
    "            proposal_roi_size_tf = [3,3]\n",
    "            # Do ROI Pooling on BEV eager ex on _bev_ancho\n",
    "            bev_proposal_rois = tf.image.crop_and_resize(bev_proposal_input,\n",
    "                                                         self._bev_anchors_norm, tf_box_indices, proposal_roi_size_tf)\n",
    "            # Do ROI Pooling on image\n",
    "            img_proposal_rois = tf.image.crop_and_resize(img_proposal_input,\n",
    "                                                         self._img_anchors_norm, tf_box_indices, proposal_roi_size_tf)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('proposal_roi_fusion'):\n",
    "            rpn_fusion_out = None\n",
    "            if self._fusion_method == 'mean':\n",
    "                tf_features_sum = tf.add(bev_proposal_rois, img_proposal_rois)\n",
    "                rpn_fusion_out = tf.divide(tf_features_sum, 2)\n",
    "            elif self._fusion_method == 'concat':\n",
    "                rpn_fusion_out = tf.concat([bev_proposal_rois, img_proposal_rois], axis=3)\n",
    "            else:\n",
    "                raise ValueError('Invalid fusion method', self._fusion_method)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('anchor_predictor', 'ap', [rpn_fusion_out]):\n",
    "            \n",
    "            print(\"here\", tf_features_sum)\n",
    "            # Rpn layers config\n",
    "            weight_decay = 0.005\n",
    "            \n",
    "            tensor_in = tf.keras.Input(shape=None, tensor=rpn_fusion_out)\n",
    "\n",
    "            # Use conv2d instead of fully_connected layers.\n",
    "            cls_fc6 = tf.keras.layers.Conv2D(filters=32, kernel_size = [3,3], kernel_initializer='ones', \n",
    "                                             kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='VALID', name=\"conv1\")(tensor_in)\n",
    "            cls_fc6_drop = tf.keras.layers.Dropout(rate = 0.5, name=\"drop1\")(cls_fc6)\n",
    "            cls_fc7 = tf.keras.layers.Conv2D(filters=32, kernel_size = [1,1], kernel_initializer='ones',\n",
    "                                             kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='VALID', name=\"conv2\")(cls_fc6_drop)\n",
    "            cls_fc7_drop = tf.keras.layers.Dropout(rate = 0.5, name=\"drop2\")(cls_fc7)\n",
    "            cls_fc8 = tf.keras.layers.Conv2D(filters=2, kernel_size = [1,1], kernel_initializer='ones',\n",
    "                                             kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='VALID', name=\"conv3\")(cls_fc7_drop)\n",
    "            objectness = tf.squeeze(cls_fc8, axis=[1,2], name='conv3/squeezed')\n",
    "\n",
    "            obj_model = tf.keras.models.Model(inputs = rpn_fusion_out, outputs = objectness, name=\"objectness predictions\")\n",
    "            obj_model.summary()\n",
    "            bev_vgg.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=[\"accuracy\"])\n",
    "            bev_hist = bev_vgg.fit(self.input_bev, batch_size=64, epochs=50, validation_split=0.2)\n",
    "\n",
    "            # Use conv2d instead of fully_connected layers.\n",
    "            reg_fc6 = tf.keras.layers.Conv2D(filters=32, kernel_size = [3,3], kernel_initializer=\"ones\", \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='VALID', name=\"conv4\")(tensor_in)\n",
    "\n",
    "            reg_fc6_drop = tf.keras.layers.Dropout(rate = 0.5, name=\"drop3\")(reg_fc6)\n",
    "\n",
    "            reg_fc7 = tf.keras.layers.Conv2D(filters = 16, kernel_size = [1, 1], kernel_initializer=\"ones\",\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding=\"same\", name=\"conv5\")(reg_fc6_drop)\n",
    "\n",
    "            reg_fc7_drop = tf.keras.layers.Dropout(rate = 0.5, name=\"drop4\")(reg_fc7)\n",
    "\n",
    "            reg_fc8 = tf.keras.layers.Conv2D(filters = 6,  kernel_size = [1, 1],  kernel_initializer=\"ones\",\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding=\"same\", name=\"conv6\")(reg_fc7_drop)\n",
    "\n",
    "            offsets = tf.squeeze(reg_fc8, axis=[1,2], name='conv6/squeezed')\n",
    "            \n",
    "            off_model = tf.keras.models.Model(inputs = rpn_fusion_out, outputs = offsets, name=\"rpn_fusion_prediction_anchors\")\n",
    "            off_model.summary()\n",
    "            bev_vgg.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=[\"accuracy\"])\n",
    "            bev_hist = bev_vgg.fit(self.input_bev, batch_size=64, epochs=50, validation_split=0.2)\n",
    "            \n",
    "            \n",
    "        # Return the proposals\n",
    "        with tf.compat.v1.variable_scope('proposals'):\n",
    "            #anchors = self.placeholders[self.PL_ANCHORS]\n",
    "            anchors = self._anchors\n",
    "\n",
    "            # Decode anchor regression offsets\n",
    "            with tf.compat.v1.variable_scope('decoding'):\n",
    "                regressed_anchors = anchor_helper.offset_to_anchor(anchors, offsets)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('bev_projection'):\n",
    "                _, bev_proposal_boxes_norm = anchor_projector.project_to_bev(regressed_anchors, self._bev_extents)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('softmax'):\n",
    "                objectness_softmax = tf.nn.softmax(objectness)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('nms'):\n",
    "                objectness_scores = objectness_softmax[:, 1]\n",
    "\n",
    "                # Do NMS on regressed anchors\n",
    "                top_indices = tf.image.non_max_suppression(\n",
    "                    bev_proposal_boxes_norm, objectness_scores,\n",
    "                    max_output_size=self._nms_size,\n",
    "                    iou_threshold=self._nms_iou_thresh)\n",
    "\n",
    "                top_anchors = tf.gather(regressed_anchors, top_indices)\n",
    "                top_objectness_softmax = tf.gather(objectness_scores,\n",
    "                                                   top_indices)\n",
    "                top_offsets = tf.gather(offsets, top_indices)\n",
    "                top_objectness = tf.gather(objectness, top_indices)\n",
    "\n",
    "        # Get mini batch #TODO eager execution \n",
    "        #all_ious_gt = self.placeholders[self.PL_ANCHOR_IOUS]\n",
    "        #all_offsets_gt = self.placeholders[self.PL_ANCHOR_OFFSETS]\n",
    "        #all_classes_gt = self.placeholders[self.PL_ANCHOR_CLASSES]\n",
    "        all_offsets_gt =  self._anchor_offsets\n",
    "        all_ious_gt = self._anchor_ios\n",
    "        all_classes_gt = self._anchor_classes\n",
    "\n",
    "        with tf.compat.v1.variable_scope('mini_batch'):\n",
    "            mini_batch_mask, _ = anchor_helper.sample_mini_batch(all_ious_gt, 64,[0, 0.3], [0.5,1])\n",
    "\n",
    "        # ROI summary images\n",
    "        rpn_mini_batch_size = 64\n",
    "        with tf.compat.v1.variable_scope('bev_rpn_rois'):\n",
    "            mb_bev_anchors_norm = tf.boolean_mask(self._bev_anchors_norm, mini_batch_mask)\n",
    "            mb_bev_box_indices = tf.zeros_like(tf.boolean_mask(all_classes_gt, mini_batch_mask), dtype=tf.int32)\n",
    "\n",
    "            # Show the ROIs of the BEV input density map\n",
    "            # for the mini batch anchors\n",
    "            bev_input_rois = tf.image.crop_and_resize(self.bev_input, mb_bev_anchors_norm, mb_bev_box_indices, (32, 32))\n",
    "\n",
    "            bev_input_roi_summary_images = tf.split(bev_input_rois, self._bev_depth, axis=3)\n",
    "            tf.summary.image('bev_rpn_rois', bev_input_roi_summary_images[-1], max_outputs=rpn_mini_batch_size)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('img_rpn_rois'):\n",
    "            # ROIs on image input TODO placeolder\n",
    "            mb_img_anchors_norm = tf.boolean_mask(self._img_anchors_norm, mini_batch_mask)\n",
    "            mb_img_box_indices = tf.zeros_like( tf.boolean_mask(all_classes_gt, mini_batch_mask), dtype=tf.int32)\n",
    "\n",
    "            # Do test ROI pooling on mini batch\n",
    "            img_input_rois = tf.image.crop_and_resize(self.img_input, mb_img_anchors_norm, mb_img_box_indices, (32, 32))\n",
    "            tf.summary.image('img_rpn_rois', img_input_rois, max_outputs=rpn_mini_batch_size)\n",
    "\n",
    "        # Ground Truth Tensors\n",
    "        with tf.compat.v1.variable_scope('one_hot_classes'):\n",
    "\n",
    "            # Anchor classification ground truth\n",
    "            # Object / Not Object\n",
    "            min_pos_iou = 0.5\n",
    "\n",
    "            objectness_classes_gt = tf.cast(tf.greater_equal(all_ious_gt, min_pos_iou), dtype=tf.int32)\n",
    "            objectness_gt = tf.one_hot(objectness_classes_gt, depth=2, on_value=1.0 - self._config.label_smoothing_epsilon,\n",
    "                                       off_value=self._config.label_smoothing_epsilon)\n",
    "\n",
    "        # Mask predictions for mini batch\n",
    "        with tf.compat.v1.variable_scope('prediction_mini_batch'):\n",
    "            objectness_masked = tf.boolean_mask(objectness, mini_batch_mask)\n",
    "            offsets_masked = tf.boolean_mask(offsets, mini_batch_mask)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('ground_truth_mini_batch'):\n",
    "            objectness_gt_masked = tf.boolean_mask(objectness_gt, mini_batch_mask)\n",
    "            offsets_gt_masked = tf.boolean_mask(all_offsets_gt, mini_batch_mask)\n",
    "\n",
    "        # Specify the tensors to evaluate\n",
    "        predictions = dict()\n",
    "\n",
    "        predictions[self.PRED_TOP_OBJECTNESS_SOFTMAX] = top_objectness_softmax\n",
    "\n",
    "\n",
    "    def feed_input(self, scene_index=None):\n",
    "        \"\"\" Fills in the placeholders with the actual input values.\n",
    "            Currently, only a scene is supported\n",
    "\n",
    "        Args:\n",
    "            scene_idx: TODO\n",
    "        Returns:\n",
    "            a feed_dict dictionary that can be used in a tensorflow session\n",
    "        \"\"\"\n",
    "        #TODO: read scenes up to scene_idx: modify preproc scipt accordingly\n",
    "\n",
    "        if scene_index is not None:\n",
    "            my_scene = self.dataset.scene[scene_index]\n",
    "        else:\n",
    "            raise TypeError('for testing you need to put a number! will change it later on once it works fully :) ')\n",
    "    \n",
    "        #general data.\n",
    "        \n",
    "        voxel_size = (0.4,0.4,1.5)\n",
    "        z_offset = -2.0\n",
    "        #arbitrary shape, must be square though!\n",
    "        bev_shape = (336,336, 3)\n",
    "        \n",
    "        # Only handle one sample at a time for now\n",
    "        my_sample_token = my_scene[\"first_sample_token\"]\n",
    "        last_sample_token = my_scene[\"last_sample_token\"]\n",
    "\n",
    "        while my_sample_token!=last_sample_token:\n",
    "\n",
    "            # We only need orientation from box_3\n",
    "            sample = self.dataset.get('sample', my_sample_token)\n",
    "            sample_name = sample.get(\"token\")\n",
    "            anchors_info, obj_classes, label_classes, label_anchors, label_boxes_3d = preproc_helper.load_sample_info(sample_name, self.classes, self.dataset)\n",
    "\n",
    "            # Network input data: loop to get batch info\n",
    "            img_input = self.dataset.get('sample_data', sample['data'][\"CAM_FRONT\"])\n",
    "            img_data=img_input\n",
    "            camera_token=img_input.get(\"token\")\n",
    "            file_name=self.dataset.get_sample_data_path(camera_token)\n",
    "            image = Image.open(file_name)\n",
    "            # convert image to numpy array\n",
    "            img_array = np.asarray(image)\n",
    "            self.img_input.append(img_array)\n",
    "            # Image shape (h, w), fixed, dunno if need to save\n",
    "            image_shape = [img_data.get(\"height\"), img_data.get(\"width\")]\n",
    "            \n",
    "            bev_input = self.dataset.get('sample_data', sample['data'][\"LIDAR_TOP\"])\n",
    "            bev_data = bev_input\n",
    "            bev_token= bev_input.get(\"token\")\n",
    "            lidar_data = self.dataset.get(\"sample_data\", bev_token)\n",
    "            lidar_filepath = self.dataset.get_sample_data_path(bev_token)\n",
    "            ego_pose = self.dataset.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "            calibrated_sensor_lidar = self.dataset.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n",
    "            global_from_car = transform_matrix(ego_pose['translation'], Quaternion(ego_pose['rotation']), inverse=False)\n",
    "            car_from_sensor_lidar = transform_matrix(calibrated_sensor_lidar['translation'], Quaternion(calibrated_sensor_lidar['rotation']),\n",
    "                                                      inverse=False)\n",
    "            lidar_pointcloud = LidarPointCloud.from_file(lidar_filepath)\n",
    "            lidar_pointcloud.transform(car_from_sensor_lidar)\n",
    "            map_mask = level5data.map[0][\"mask\"]\n",
    "            bev = bev_helper.create_voxel_pointcloud(lidar_pointcloud.points, bev_shape, voxel_size=voxel_size, z_offset=z_offset)\n",
    "            ego_centric_map = bev_helper.get_semantic_map_around_ego(map_mask, ego_pose, voxel_size=0.4, output_shape=(336,336)) \n",
    "            bev_array = bev_helper.normalize_voxel_intensities(bev)\n",
    "            self.bev_input.append(bev_array)\n",
    "\n",
    "            #ground plane shape (a,b,c,d) in kitti:\n",
    "            #no info on ground plane in nuscenes data, just global coordinate system\n",
    "            #which is given as x, y, z. Computed from the cameras position:\n",
    "            #suppose as in kitti that ground plane is as the same level with the cameras\n",
    "\n",
    "            cam_front_token = self.dataset.get('sample_data', sample['data'][\"CAM_FRONT\"])\n",
    "            cam_front_data = cam_front_token.get(\"calibrated_sensor_token\")\n",
    "            cam_front_calib = self.dataset.get(\"calibrated_sensor\", cam_front_data )\n",
    "            cam_front_coords = cam_front_calib.get(\"translation\")\n",
    "\n",
    "            cam_front_left_token = self.dataset.get('sample_data', sample['data'][\"CAM_FRONT_LEFT\"])\n",
    "            cam_front_left_data = cam_front_left_token.get(\"calibrated_sensor_token\")\n",
    "            cam_front_left_calib = self.dataset.get(\"calibrated_sensor\", cam_front_left_data )\n",
    "            cam_front_left_coords = cam_front_left_calib.get(\"translation\")\n",
    "\n",
    "            cam_front_right_token = self.dataset.get('sample_data', sample['data'][\"CAM_FRONT_RIGHT\"])\n",
    "            cam_front_right_data = cam_front_right_token.get(\"calibrated_sensor_token\")\n",
    "            cam_front_right_calib = self.dataset.get(\"calibrated_sensor\", cam_front_right_data )\n",
    "            cam_front_right_coords = cam_front_right_calib.get(\"translation\")\n",
    "\n",
    "            ground_plane = frame_helper.get_ground_plane_coeff(cam_front_coords, cam_front_left_coords, cam_front_right_coords)\n",
    "            \n",
    "            #only for cameras, of course lidars do not have instrinsic matrices\n",
    "            token=img_data.get(\"calibrated_sensor_token\") \n",
    "            stereo_calib_p2 = frame_helper.read_calibration(token, self.dataset)\n",
    "            \n",
    "            #get next sample to extract info\n",
    "            my_sample_token = self.dataset.get(\"sample\", my_sample_token)[\"next\"]\n",
    "\n",
    "            # Fill the placeholders for anchor information\n",
    "            self._fill_inputs(anchors_info=anchors_info,sample_token=bev_token, ground_plane=ground_plane,\n",
    "                                        image_shape=image_shape, stereo_calib_p2=stereo_calib_p2,\n",
    "                                        sample_name=sample_name)\n",
    "\n",
    "            self._label_anchors.append(label_anchors)\n",
    "            self._label_boxes_3d.append(label_boxes_3d)\n",
    "            self._label_classes.append(label_classes)\n",
    "\n",
    "            self._sample_name.append(str(sample_name))\n",
    "            self._calib.append(stereo_calib_p2)\n",
    "            self._ground_plane.append(ground_plane)\n",
    "            \n",
    "            return self.img_input\n",
    "\n",
    "    def _fill_inputs(self,\n",
    "                               sample_token,\n",
    "                               anchors_info,\n",
    "                               ground_plane,\n",
    "                               image_shape,\n",
    "                               stereo_calib_p2,\n",
    "                               sample_name):\n",
    "        \"\"\"\n",
    "        Fills anchor placeholder inputs with corresponding data\n",
    "\n",
    "        Args:\n",
    "            anchors_info: anchor info from mini_batch_utils\n",
    "            ground_plane: ground plane coefficients\n",
    "            image_shape: image shape (h, w), used for projecting anchors\n",
    "            sample_name: name of the sample, e.g. \"000001\"\n",
    "        \"\"\"\n",
    "\n",
    "        # Lists for merging anchors info\n",
    "        all_anchor_boxes_3d = []\n",
    "        anchors_ious = []\n",
    "        anchor_offsets = []\n",
    "        anchor_classes = []\n",
    "        \n",
    "        # Create anchors for each class\n",
    "        if len(self.classes) > 1:\n",
    "            for class_idx in range(len(self.classes)):\n",
    "                cluster_sizes = []\n",
    "                for i in self._cluster_sizes[class_idx]:\n",
    "                    if i!=[]:\n",
    "                        cluster_sizes.append(i)\n",
    "                if len(cluster_sizes)!=0:\n",
    "                # Generate anchors for all classes\n",
    "                    grid_anchor_boxes_3d = self._anchor_generator.generate(\n",
    "                        area_3d=self._area_extents,\n",
    "                        anchor_3d_sizes=cluster_sizes,\n",
    "                        anchor_stride=self._anchor_strides[0],\n",
    "                        ground_plane=ground_plane)\n",
    "                else:\n",
    "                    #no labels per class, no anchor per class\n",
    "                    grid_anchor_boxes_3d=[]\n",
    "                all_anchor_boxes_3d.append(grid_anchor_boxes_3d)\n",
    "            \n",
    "            all_anchor_boxes_3d = np.concatenate(all_anchor_boxes_3d, axis=None)\n",
    "        else:\n",
    "            # Don't loop for a single class\n",
    "            class_idx = 0\n",
    "            cluster_sizes[class_idx] = [x for x in self._cluster_sizes[class_idx] if x != []]\n",
    "            if self._cluster_sizes[class_idx]!=[]:\n",
    "                grid_anchor_boxes_3d = self._anchor_generator.generate(area_3d=self._area_extents, anchor_3d_sizes=cluster_sizes[class_idx], \n",
    "                                                                       anchor_stride=self._anchor_strides[0], ground_plane=ground_plane)\n",
    "                all_anchor_boxes_3d = grid_anchor_boxes_3d\n",
    "\n",
    "        # Filter empty anchors\n",
    "        # Skip if anchors_info is []\n",
    "        sample_has_labels = True\n",
    "\n",
    "        # Convert lists to ndarrays\n",
    "        #already filtered them before\n",
    "        anchor_boxes_3d_to_use = all_anchor_boxes_3d\n",
    "        anchors_ious = np.asarray(anchors_ious)\n",
    "        anchor_offsets = np.asarray(anchor_offsets)\n",
    "        anchor_classes = np.asarray(anchor_classes)\n",
    "\n",
    "        # Flip anchors and centroid x offsets for augmented samples\n",
    "#             if kitti_aug.AUG_FLIPPING in sample_augs:\n",
    "#                 anchor_boxes_3d_to_use = kitti_aug.flip_boxes_3d(anchor_boxes_3d_to_use, flip_ry=False)\n",
    "#                 if anchors_info:\n",
    "#                     anchor_offsets[:, 0] = -anchor_offsets[:, 0]\n",
    "\n",
    "        # Convert to anchors\n",
    "        anchors_to_use = box_3d_encoder.box_3d_to_anchor(anchor_boxes_3d_to_use)\n",
    "        num_anchors = len(anchors_to_use)\n",
    "\n",
    "        # Project anchors into bev\n",
    "        bev_anchors, bev_anchors_norm = anchor_projector.project_to_bev( anchors_to_use, self._bev_extents)\n",
    "\n",
    "        # Project box_3d anchors into image space\n",
    "        img_anchors, img_anchors_norm = anchor_projector.project_to_image_space(anchors_to_use, stereo_calib_p2, image_shape)\n",
    "\n",
    "        # Reorder into [y1, x1, y2, x2] for tf.crop_and_resize op\n",
    "        self._bev_anchors_norm.append(bev_anchors_norm[:, [1, 0, 3, 2]])\n",
    "        self._img_anchors_norm.append(img_anchors_norm[:, [1, 0, 3, 2]])\n",
    "\n",
    "        # Fill in placeholder inputs\n",
    "        self._anchors.append(anchors_to_use)\n",
    "        #self._placeholder_inputs[self.PL_ANCHORS] = anchors_to_use\n",
    "\n",
    "        # If we are in train/validation mode, and the anchor infos\n",
    "        # are not empty, store them. Checking for just anchors_ious\n",
    "        # to be non-empty should be enough.\n",
    "        if self._train_val_test in ['train', 'val'] and len(anchors_ious) > 0:\n",
    "            self._anchor_ious.append(anchors_ious)\n",
    "            self._anchor_offsets.append(anchor_offsets)\n",
    "            self._anchor_classes.append(anchor_classes)\n",
    "\n",
    "        # During test, or val when there is no anchor info\n",
    "        elif self._train_val_test in ['test'] or len(anchors_ious) == 0:\n",
    "            # During testing, or validation with no gt, fill these in with 0s\n",
    "#             self._placeholder_inputs[self.PL_ANCHOR_IOUS] = np.zeros(num_anchors)\n",
    "#             self._placeholder_inputs[self.PL_ANCHOR_OFFSETS] = np.zeros([num_anchors, 6])\n",
    "#             self._placeholder_inputs[self.PL_ANCHOR_CLASSES] = np.zeros(num_anchors)\n",
    "            self._anchor_ious.append(anchors_ious)\n",
    "            self._anchor_offsets.append(anchor_offsets)\n",
    "            self._anchor_classes.append(anchor_classes)\n",
    "                                                      \n",
    "        else:\n",
    "            raise ValueError('Got run mode {}, and non-empty anchor info'.format(self._train_val_test))\n",
    "\n",
    "        self._bev_anchors = bev_anchors\n",
    "        self._img_anchors = img_anchors\n",
    "                                                      \n",
    "\n",
    "    def create_path_drop_masks(self,\n",
    "                               p_img,\n",
    "                               p_bev,\n",
    "                               random_values):\n",
    "        \"\"\"Determines global path drop decision based on given probabilities.\n",
    "\n",
    "        Args:\n",
    "            p_img: A tensor of float32, probability of keeping image branch\n",
    "            p_bev: A tensor of float32, probability of keeping bev branch\n",
    "            random_values: A tensor of float32 of shape [3], the results\n",
    "                of coin flips, values should range from 0.0 - 1.0.\n",
    "\n",
    "        Returns:\n",
    "            final_img_mask: A constant tensor mask containing either one or zero\n",
    "                depending on the final coin flip probability.\n",
    "            final_bev_mask: A constant tensor mask containing either one or zero\n",
    "                depending on the final coin flip probability.\n",
    "        \"\"\"\n",
    "\n",
    "        def keep_branch(): return tf.constant(1.0)\n",
    "\n",
    "        def kill_branch(): return tf.constant(0.0)\n",
    "\n",
    "        # The logic works as follows:\n",
    "        # We have flipped 3 coins, first determines the chance of keeping\n",
    "        # the image branch, second determines keeping bev branch, the third\n",
    "        # makes the final decision in the case where both branches were killed\n",
    "        # off, otherwise the initial img and bev chances are kept.\n",
    "\n",
    "        img_chances = tf.case([(tf.less(random_values[0], p_img), keep_branch)], default=kill_branch)\n",
    "\n",
    "        bev_chances = tf.case([(tf.less(random_values[1], p_bev), keep_branch)], default=kill_branch)\n",
    "\n",
    "        # Decision to determine whether both branches were killed off\n",
    "        third_flip = tf.logical_or(tf.cast(img_chances, dtype=tf.bool), tf.cast(bev_chances, dtype=tf.bool))\n",
    "        third_flip = tf.cast(third_flip, dtype=tf.float32)\n",
    "\n",
    "        # Make a second choice, for the third case\n",
    "        # Here we use a 50/50 chance to keep either image or bev\n",
    "        # If its greater than 0.5, keep the image\n",
    "        img_second_flip = tf.case([(tf.greater(random_values[2], 0.5), keep_branch)], default=kill_branch)\n",
    "        # If its less than or equal to 0.5, keep bev\n",
    "        bev_second_flip = tf.case([(tf.less_equal(random_values[2], 0.5), keep_branch)],\n",
    "                                  default=kill_branch)\n",
    "\n",
    "        # Use lambda since this returns another condition and it needs to\n",
    "        # be callable\n",
    "        final_img_mask = tf.case([(tf.equal(third_flip, 1), lambda: img_chances)], default=lambda: img_second_flip)\n",
    "\n",
    "        final_bev_mask = tf.case([(tf.equal(third_flip, 1), lambda: bev_chances)], default=lambda: bev_second_flip)\n",
    "\n",
    "        return final_img_mask, final_bev_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test this part!!\n",
    "\n",
    "Tests and results for the RPN model part. The following changes to the model were done after errors/issues with testing:\n",
    "<li>Maybe change the use of placeholders in the future, to fit with eager execution (shorter code)</li>\n",
    "<li>Some internal keras os function rises a warning, something will be depreated, doesn't tell where and what function exactly</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import avod.builders.config_builder_util as config_build\n",
    "config_path = 'avod/configs/unittest_model.config'\n",
    "pipe_path = 'avod/configs/unittest_pipeline.config'\n",
    "model_config = config_build.get_model_config_from_file(config_path)\n",
    "pipeline_config=config_build.get_configs_from_pipeline_file(pipe_path, \"val\")\n",
    "\n",
    "#rpn_model = RpnModel(model_config, pipeline_config[3],\n",
    "#                          train_val_test=\"val\",\n",
    "#                          dataset=level5data)\n",
    "# array=rpn_model.feed_input(5)\n",
    "#predictions = rpn_model.build()\n",
    "#tensor=tf.convert_to_tensor(array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> AVOD MODEL: </b> second stage detector for the AVOD algorithm. It uses FPN as feature extractors.\n",
    "<b> FPN: </b> Feature Pyramid Network (FPN) is a feature extractor designed for such pyramid concept with accuracy and speed in mind. It replaces the feature extractor of detectors like Faster R-CNN and generates multiple feature map layers (multi-scale feature maps) with better quality information than the regular feature pyramid for object detection. [Understanding Feature Pyramid Networks for object detection (FPN)](https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avod.core.models import avod_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=level5data\n",
    "my_scene = dataset.scene[5]\n",
    "\n",
    "        \n",
    "voxel_size = (0.4,0.4,1.5)\n",
    "z_offset = -2.0\n",
    "#arbitrary shape, must be square though!\n",
    "bev_shape = (224,224, 3)\n",
    "        \n",
    "# Only handle one sample at a time for now\n",
    "my_sample_token = my_scene[\"first_sample_token\"]\n",
    "my_last_sample_token = my_scene[\"last_sample_token\"]\n",
    "sample = dataset.get('sample', my_sample_token)\n",
    "tok=sample['data'][\"CAM_FRONT\"]\n",
    "boxes=dataset.get_boxes(tok)        \n",
    "\n",
    "#anchors_info, obj_classes, label_classes, label_anchors, label_boxes_3d = preproc_helper.load_sample_info(sample_name, self.classes, self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network input data: loop to get batch info\n",
    "import box_helper\n",
    "import bev_helper\n",
    "import io\n",
    "from typing import Tuple, List\n",
    "\n",
    "classes=[]\n",
    "for i in dataset.category:\n",
    "    classes.append(i.get(\"name\"))\n",
    "\n",
    "img_input_list=[]\n",
    "img_target=[]\n",
    "bev_input_list=[]\n",
    "ground_plane_list=[]\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    while my_sample_token!=my_last_sample_token:\n",
    "        target_im = np.zeros([1024,1224,3], dtype=np.uint8)\n",
    "\n",
    "        sample = dataset.get('sample', my_sample_token)\n",
    "        sample_name = sample.get(\"token\")\n",
    "        img_data = dataset.get('sample_data', sample['data'][\"CAM_FRONT\"])\n",
    "        camera_token=img_data.get(\"token\")\n",
    "\n",
    "        tok=sample['data'][\"CAM_FRONT\"]\n",
    "        boxes=dataset.get_boxes(tok) \n",
    "\n",
    "        ego_pose = dataset.get(\"ego_pose\", img_data[\"ego_pose_token\"])\n",
    "        data_path, boxes, camera_intrinsic = dataset.get_sample_data(camera_token)\n",
    "        data = Image.open(data_path)\n",
    "\n",
    "        anchors_info, obj_classes, label_classes, label_anchors, label_boxes_3d = preproc_helper.load_sample_info(sample_name, classes, dataset)\n",
    "    #     fig, ax = plt.subplots(1, 1, figsize=(9, 16))\n",
    "\n",
    "    #     # Show image.\n",
    "    #     ax.imshow(data)\n",
    "\n",
    "    #     # Show boxes.\n",
    "    #     for box in boxes:\n",
    "    #         c = np.array(box_helper.get_color(box.name)) / 255.0\n",
    "    #         box.render(ax, view=camera_intrinsic, normalize=True, colors=(c, c, c))\n",
    "    #     ax.axis('off')\n",
    "    #     ax.set_title(img_data['channel'])\n",
    "    #     ax.set_aspect('equal')\n",
    "\n",
    "    #     # you can get a high-resolution image as numpy array!!\n",
    "    #     plot_img_np = box_helper.get_img_from_fig(fig)\n",
    "\n",
    "\n",
    "    #     # Limit visible range.\n",
    "    #     ax.set_xlim(0, data.size[0])\n",
    "    #     ax.set_ylim(data.size[1], 0)\n",
    "\n",
    "    #    img_target.append(plot_img_np)\n",
    "        bev_input = dataset.get('sample_data', sample['data'][\"LIDAR_TOP\"])\n",
    "        bev_data = bev_input\n",
    "        bev_token= bev_input.get(\"token\")\n",
    "        lidar_data = dataset.get(\"sample_data\", bev_token)\n",
    "        lidar_filepath = dataset.get_sample_data_path(bev_token)\n",
    "        ego_pose = dataset.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "        calibrated_sensor_lidar = dataset.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n",
    "        global_from_car = transform_matrix(ego_pose['translation'], Quaternion(ego_pose['rotation']), inverse=False)\n",
    "        car_from_sensor_lidar = transform_matrix(calibrated_sensor_lidar['translation'], Quaternion(calibrated_sensor_lidar['rotation']),\n",
    "                                                  inverse=False)\n",
    "        lidar_pointcloud = LidarPointCloud.from_file(lidar_filepath)\n",
    "        lidar_pointcloud.transform(car_from_sensor_lidar)\n",
    "        map_mask = level5data.map[0][\"mask\"]\n",
    "        bev = bev_helper.create_voxel_pointcloud(lidar_pointcloud.points, bev_shape, voxel_size=voxel_size, z_offset=z_offset)\n",
    "        ego_centric_map = bev_helper.get_semantic_map_around_ego(map_mask, ego_pose, voxel_size=0.4, output_shape=(336,336)) \n",
    "        bev_array = bev_helper.normalize_voxel_intensities(bev)\n",
    "        bev_input_list.append(bev_array)\n",
    "\n",
    "        file_name=dataset.get_sample_data_path(camera_token)\n",
    "        image = Image.open(file_name)\n",
    "        # compress image, it is too big\n",
    "        # convert image to numpy array\n",
    "        img_array = np.asarray(image)\n",
    "        if img_input_list is not None:\n",
    "            img_input_list.append(img_array)\n",
    "\n",
    "        cam_front_token = dataset.get('sample_data', sample['data'][\"CAM_FRONT\"])\n",
    "        cam_front_data = cam_front_token.get(\"calibrated_sensor_token\")\n",
    "        cam_front_calib = dataset.get(\"calibrated_sensor\", cam_front_data )\n",
    "        cam_front_coords = cam_front_calib.get(\"translation\")\n",
    "\n",
    "        cam_front_left_token = dataset.get('sample_data', sample['data'][\"CAM_FRONT_LEFT\"])\n",
    "        cam_front_left_data = cam_front_left_token.get(\"calibrated_sensor_token\")\n",
    "        cam_front_left_calib = dataset.get(\"calibrated_sensor\", cam_front_left_data )\n",
    "        cam_front_left_coords = cam_front_left_calib.get(\"translation\")\n",
    "\n",
    "        cam_front_right_token = dataset.get('sample_data', sample['data'][\"CAM_FRONT_RIGHT\"])\n",
    "        cam_front_right_data = cam_front_right_token.get(\"calibrated_sensor_token\")\n",
    "        cam_front_right_calib = dataset.get(\"calibrated_sensor\", cam_front_right_data )\n",
    "        cam_front_right_coords = cam_front_right_calib.get(\"translation\")\n",
    "\n",
    "        ground_plane = frame_helper.get_ground_plane_coeff(cam_front_coords, cam_front_left_coords, cam_front_right_coords)\n",
    "        ground_plane_list.append(ground_plane)\n",
    "        \n",
    "        token=img_data.get(\"calibrated_sensor_token\") \n",
    "        stereo_calib_p2 = frame_helper.read_calibration(token, dataset)\n",
    "\n",
    "        my_sample_token = dataset.get(\"sample\", my_sample_token)[\"next\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering labels 126 / 126\n",
      "Finished reading labels, clustering data...\n",
      "\n",
      "\n",
      "Finished \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente1\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "all_anchor_boxes_3d = []\n",
    "anchors_ious = []\n",
    "anchor_offsets = []\n",
    "anchor_classes = []\n",
    "_bev_anchors_norm=[]\n",
    "\n",
    "area_extents = pipeline_config[3].kitti_utils_config.area_extents\n",
    "\n",
    "_area_extents = np.reshape(area_extents, (3, 2))\n",
    "_bev_extents = _area_extents[[0, 2]]\n",
    "\n",
    "label_cluster_utils = LabelClusterUtils(dataset)\n",
    "_cluster_sizes, _all_std = label_cluster_utils.get_clusters(5, dataset)\n",
    "_anchor_generator = grid_anchor_3d_generator.GridAnchor3dGenerator()\n",
    "\n",
    "# Create anchors for each class\n",
    "if len(classes) > 1:\n",
    "    for class_idx in range(len(classes)):\n",
    "        cluster_sizes = []\n",
    "        for i in _cluster_sizes[class_idx]:\n",
    "            if i!=[]:\n",
    "                cluster_sizes.append(i)\n",
    "        if len(cluster_sizes)!=0:\n",
    "        # Generate anchors for all classes\n",
    "            grid_anchor_boxes_3d = _anchor_generator.generate(area_3d=_area_extents, anchor_3d_sizes=cluster_sizes, anchor_stride=[0.5,0.5], ground_plane=ground_plane_list[0])\n",
    "        else:\n",
    "            #no labels per class, no anchor per class\n",
    "            grid_anchor_boxes_3d=[]\n",
    "        all_anchor_boxes_3d.append(grid_anchor_boxes_3d)\n",
    "\n",
    "    all_anchor_boxes_3d = np.concatenate(all_anchor_boxes_3d, axis=None)\n",
    "else:\n",
    "    # Don't loop for a single class\n",
    "    class_idx = 0\n",
    "    cluster_sizes[class_idx] = [x for x in _cluster_sizes[class_idx] if x != []]\n",
    "    if _cluster_sizes[class_idx]!=[]:\n",
    "        grid_anchor_boxes_3d = _anchor_generator.generate(area_3d=_area_extents, anchor_3d_sizes=cluster_sizes[class_idx], \n",
    "                                                               anchor_stride=0.5, ground_plane=ground_plane_list[0])\n",
    "        all_anchor_boxes_3d = grid_anchor_boxes_3d\n",
    "\n",
    "# Filter empty anchors\n",
    "# Skip if anchors_info is []\n",
    "sample_has_labels = True\n",
    "\n",
    "# Convert lists to ndarrays\n",
    "#already filtered them before\n",
    "anchor_boxes_3d_to_use = all_anchor_boxes_3d\n",
    "anchors_ious = np.asarray(anchors_ious)\n",
    "anchor_offsets = np.asarray(anchor_offsets)\n",
    "anchor_classes = np.asarray(anchor_classes)\n",
    "\n",
    "# Convert to anchors\n",
    "anchors_to_use = box_3d_encoder.box_3d_to_anchor(anchor_boxes_3d_to_use)\n",
    "num_anchors = len(anchors_to_use)\n",
    "\n",
    "# Project anchors into bev\n",
    "bev_anchors, bev_anchors_norm = anchor_projector.project_to_bev( anchors_to_use, _bev_extents)\n",
    "\n",
    "# Reorder into [y1, x1, y2, x2] for tf.crop_and_resize op\n",
    "_bev_anchors_norm.append(bev_anchors_norm[:, [1, 0, 3, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input=np.asarray(img_input_list)\n",
    "bev_input=np.asarray(bev_input_list)\n",
    "#img_targetnp=np.asarray(img_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    #inputs_vgg = tf.keras.layers.Input(shape=(224,224,3))\n",
    "    vgg19 = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
    "    bev_data = tf.keras.applications.vgg19.preprocess_input(bev_input)\n",
    "    bev_features=vgg19.predict(bev_data)\n",
    "    #exclude fully connected\n",
    "    output = vgg19.layers[-3].output\n",
    "    #struggle with memory\n",
    "    #img_data = tf.keras.applications.vgg19.preprocess_input(img_input)\n",
    "    #img_features=vgg19.predict(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "random_values = tf.random.uniform(shape=[3], minval=0.0, maxval=1.0)\n",
    "img_mask, bev_mask = create_path_drop_masks(0.5, 0.5, random_values)\n",
    "bev_proposal_input = tf.multiply(bev_proposal_input, bev_mask)\n",
    "bev_path_drop_mask = bev_mask\n",
    "\n",
    "    # Overwrite the division factor\n",
    "fusion_mean_div_factor = img_mask + bev_mask\n",
    "print(\"\\nStarting ROI pooling..\\n\")\n",
    "def get_box_indices(boxes):\n",
    "    proposals_shape = boxes.get_shape().as_list()\n",
    "    if any(dim is None for dim in proposals_shape):\n",
    "        proposals_shape = tf.shape(boxes)\n",
    "    ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)\n",
    "    multiplier = tf.expand_dims(tf.range(start=0, limit=proposals_shape[0]), 1)\n",
    "    return tf.reshape(ones_mat * multiplier, [-1])\n",
    "\n",
    "# bev_boxes_norm_batches = tf.expand_dims(_bev_anchors_norm, axis=0)\n",
    "# tf_box_indices = get_box_indices(bev_boxes_norm_batches)\n",
    "\n",
    "# proposal_roi_size_tf = [3,3]\n",
    "# # Do ROI Pooling on BEV eager ex on _bev_ancho\n",
    "# bev_proposal_rois = tf.image.crop_and_resize(bev_proposal_input,\n",
    "#                                              _bev_anchors_norm, tf_box_indices, proposal_roi_size_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_drop_masks(self,\n",
    "                               p_img,\n",
    "                               p_bev,\n",
    "                               random_values):\n",
    "        \"\"\"Determines global path drop decision based on given probabilities.\n",
    "\n",
    "        Args:\n",
    "            p_img: A tensor of float32, probability of keeping image branch\n",
    "            p_bev: A tensor of float32, probability of keeping bev branch\n",
    "            random_values: A tensor of float32 of shape [3], the results\n",
    "                of coin flips, values should range from 0.0 - 1.0.\n",
    "\n",
    "        Returns:\n",
    "            final_img_mask: A constant tensor mask containing either one or zero\n",
    "                depending on the final coin flip probability.\n",
    "            final_bev_mask: A constant tensor mask containing either one or zero\n",
    "                depending on the final coin flip probability.\n",
    "        \"\"\"\n",
    "\n",
    "        def keep_branch(): return tf.constant(1.0)\n",
    "\n",
    "        def kill_branch(): return tf.constant(0.0)\n",
    "\n",
    "        # The logic works as follows:\n",
    "        # We have flipped 3 coins, first determines the chance of keeping\n",
    "        # the image branch, second determines keeping bev branch, the third\n",
    "        # makes the final decision in the case where both branches were killed\n",
    "        # off, otherwise the initial img and bev chances are kept.\n",
    "\n",
    "        img_chances = tf.case([(tf.less(random_values[0], p_img), keep_branch)], default=kill_branch)\n",
    "\n",
    "        bev_chances = tf.case([(tf.less(random_values[1], p_bev), keep_branch)], default=kill_branch)\n",
    "\n",
    "        # Decision to determine whether both branches were killed off\n",
    "        third_flip = tf.logical_or(tf.cast(img_chances, dtype=tf.bool), tf.cast(bev_chances, dtype=tf.bool))\n",
    "        third_flip = tf.cast(third_flip, dtype=tf.float32)\n",
    "\n",
    "        # Make a second choice, for the third case\n",
    "        # Here we use a 50/50 chance to keep either image or bev\n",
    "        # If its greater than 0.5, keep the image\n",
    "        img_second_flip = tf.case([(tf.greater(random_values[2], 0.5), keep_branch)], default=kill_branch)\n",
    "        # If its less than or equal to 0.5, keep bev\n",
    "        bev_second_flip = tf.case([(tf.less_equal(random_values[2], 0.5), keep_branch)],\n",
    "                                  default=kill_branch)\n",
    "\n",
    "        # Use lambda since this returns another condition and it needs to\n",
    "        # be callable\n",
    "        final_img_mask = tf.case([(tf.equal(third_flip, 1), lambda: img_chances)], default=lambda: img_second_flip)\n",
    "\n",
    "        final_bev_mask = tf.case([(tf.equal(third_flip, 1), lambda: bev_chances)], default=lambda: bev_second_flip)\n",
    "\n",
    "        return final_img_mask, final_bev_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
