{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING\n",
    "### Get the dataset\n",
    "Load the dataset, split it in two for trainin and validation. As in the Reference model provided by [Lyft](https://level5.lyft.com/), a dataframe with one scene per row is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = 'v1.02-train'\n",
    "DATASET_ROOT = '/srv/data/scanlab/Data/lyft/v1.02-train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import glob\n",
    "#Disabled for numpy and opencv: avod has opencv and numpy versions for several methods\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import os\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import argparse\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.compat.v1.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=1) \n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True, gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level5data = LyftDataset(json_path=DATASET_ROOT + \"/v1.02-train\", data_path=DATASET_ROOT, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">TRAINING</font>\n",
    "Based on the [AVOD algorithm](https://github.com/kujason/avod), we train the dataset. \n",
    "The goal is to study how the accuracy changes based on the type of sensors in input, and their number, thus changes to the AVOD algorithm have been made. Here we keep the two stage model.\n",
    "Will be divided in steps, to mimick the divisions made by AVOD's authors in the code.\n",
    "\n",
    "With respect to the original AVOD code, the following changes have been made:\n",
    "<li> Upgrades for compatibity issues with tensorflow 2.0: migrated from slim libs to keras Sequential</li>\n",
    "<li> Changes to support single type input </li>\n",
    "<li> VGGs take as input Lyft-style dataset </li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import avod\n",
    "from avod.core import trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>RPN MODEL</b>: It is the fist subnetwork that makes up the double stage AVOD algorithm. It uses two VGGs, one for images, one for LiDar, to find the bottleneck.\n",
    "Img VGG and Bev VGG have the same strucure, just have input from different sources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>VGG:</b> VGG is a convolutional neural network model. Here simplified model wrt K. Simonyan and A. Zisserman's model proposed in the paper \"Very Deep Convolutional Networks for Large-Scale Image Recognition\".\n",
    "Basically, it lacks dense layers at the end, and the last group of conv layers is smaller that theirs.\n",
    "Two VGGs, one for BEV, one for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.anchor_helper\n",
    "from utils.frame_helper import FrameCalibrationData\n",
    "import utils.frame_helper as frame_helper\n",
    "import utils.preproc_helper as preproc_helper\n",
    "import utils.bev_helper as bev_helper\n",
    "import utils.rpn_helper as rpn_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from avod.core import anchor_filter\n",
    "from avod.core import anchor_projector\n",
    "from avod.core import box_3d_encoder\n",
    "from avod.core import constants\n",
    "from avod.core import losses\n",
    "from avod.core import model\n",
    "from avod.core import summary_utils\n",
    "from avod.core.anchor_generators import grid_anchor_3d_generator\n",
    "from avod.datasets.kitti import kitti_aug\n",
    "import avod.datasets.kitti.kitti_utils as kitti_utils\n",
    "from avod.core.label_cluster_utils import LabelClusterUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test this part!!\n",
    "\n",
    "Tests and results for the RPN model part. The following changes to the model were done after errors/issues with testing:\n",
    "<li>Maybe change the use of placeholders in the future, to fit with eager execution (shorter code)</li>\n",
    "<li>Some internal keras os function rises a warning, something will be depreated, doesn't tell where and what function exactly</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import avod.builders.config_builder_util as config_build\n",
    "config_path = 'avod/configs/unittest_model.config'\n",
    "pipe_path = 'avod/configs/unittest_pipeline.config'\n",
    "model_config = config_build.get_model_config_from_file(config_path)\n",
    "pipeline_config=config_build.get_configs_from_pipeline_file(pipe_path, \"val\")\n",
    "#rpn_model = RpnModel(model_config, pipeline_config[3],\n",
    " #                        train_val_test=\"val\",\n",
    "  #                     dataset=level5data)\n",
    "# array=rpn_model.feed_input(5)\n",
    "#predictions = rpn_model.build()\n",
    "#tensor=tf.convert_to_tensor(array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = rpn_model.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a scene, show the different methods to get the boxes. Loop to get all the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level5data.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network input data: loop to get batch info\n",
    "import io\n",
    "from typing import Tuple, List\n",
    "\n",
    "dataset=level5data\n",
    "\n",
    "classes=[]\n",
    "for i in dataset.category:\n",
    "    classes.append(i.get(\"name\"))\n",
    "\n",
    "img_input_list=[]\n",
    "img_target=[]\n",
    "bev_input_list=[]\n",
    "ground_plane_list=[]\n",
    "lidar=[]\n",
    "boxes_c=[]\n",
    "boxes_b=[]\n",
    "\n",
    "dataset=level5data\n",
    "my_scene = dataset.scene[0]\n",
    "my_scene_sample_list = []\n",
    "\n",
    "        \n",
    "voxel_size = (0.4,0.4,1.5)\n",
    "z_offset = -2.0\n",
    "#arbitrary shape, must be square though!\n",
    "bev_shape = (336,336, 3)\n",
    "        \n",
    "# Only handle one sample at a time for now\n",
    "my_sample_token = my_scene[\"first_sample_token\"]\n",
    "my_last_sample_token = my_scene[\"last_sample_token\"]\n",
    "# sample = dataset.get('sample', my_sample_token)\n",
    "# tok=sample['data'][\"CAM_FRONT\"]\n",
    "# tok_bev=sample['data'][\"LIDAR_TOP\"]\n",
    "# iboxes=dataset.get_boxes(tok)   \n",
    "# bboxes=dataset.get_boxes(tok_bev)\n",
    "# data_path, boxes, camera_intrinsic = dataset.get_sample_data(\n",
    "#                 tok_bev)\n",
    "# data_path, uboxes, camera_intrinsic = dataset.get_sample_data(\n",
    "#                 tok)\n",
    "\n",
    "classes=[]\n",
    "for i in dataset.category:\n",
    "    classes.append(i.get(\"name\"))\n",
    "# print(iboxes[0], boxes[0].center/4)\n",
    "#anchors_info, obj_classes, label_classes, label_anchors, label_boxes_3d = preproc_helper.load_sample_info(tok, classes, dataset)\n",
    "i=0\n",
    "with tf.device('/GPU:0'):\n",
    "    while my_sample_token!=my_last_sample_token:\n",
    "        my_scene_sample_list.append(my_sample_token)\n",
    "        \n",
    "        sample = dataset.get('sample', my_sample_token)\n",
    "        sample_name = sample.get(\"token\")\n",
    "        img_data = dataset.get('sample_data', sample['data'][\"CAM_FRONT\"])\n",
    "        camera_token=img_data.get(\"token\")\n",
    "\n",
    "        tok=sample['data'][\"CAM_FRONT\"]\n",
    "        ego_pose = dataset.get(\"ego_pose\", img_data[\"ego_pose_token\"])\n",
    "        data_path, boxes_cam, camera_intrinsic = dataset.get_sample_data(camera_token)\n",
    "        data = Image.open(data_path)\n",
    "        boxes_c.append(np.asarray(boxes_cam))\n",
    "        #anchors_info, obj_classes, label_classes, label_anchors, label_boxes_3d = preproc_helper.load_sample_info(sample_name, classes, dataset)\n",
    "        #print(label_boxes_3d.shape)\n",
    "        bev_input = dataset.get('sample_data', sample['data'][\"LIDAR_TOP\"])\n",
    "        bev_data = bev_input\n",
    "        bev_token= bev_input.get(\"token\")\n",
    "        lidar_data = dataset.get(\"sample_data\", bev_token)\n",
    "        lidar_filepath = dataset.get_sample_data_path(bev_token)\n",
    "        ego_pose = dataset.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "        calibrated_sensor_lidar = dataset.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n",
    "        global_from_car = transform_matrix(ego_pose['translation'], Quaternion(ego_pose['rotation']), inverse=False)\n",
    "        car_from_sensor_lidar = transform_matrix(calibrated_sensor_lidar['translation'], Quaternion(calibrated_sensor_lidar['rotation']),\n",
    "                                                  inverse=False)\n",
    "        lidar_pointcloud = LidarPointCloud.from_file(lidar_filepath)\n",
    "        lidar_pointcloud.transform(car_from_sensor_lidar)\n",
    "        lidar.append(lidar_pointcloud.points)\n",
    "        x=sample[\"data\"][\"LIDAR_TOP\"]\n",
    "        bevboxes=dataset.get_boxes(x)\n",
    "        boxes_b.append(bevboxes)\n",
    "        bev = bev_helper.create_voxel_pointcloud(lidar_pointcloud.points, bev_shape, voxel_size=voxel_size, z_offset=z_offset)\n",
    "        #ego_centric_map = bev_helper.get_semantic_map_around_ego(map_mask, ego_pose, voxel_size=0.4, output_shape=(336,336)) \n",
    "        bev_array = bev_helper.normalize_voxel_intensities(bev)\n",
    "        bev_input_list.append(bev_array)\n",
    "\n",
    "        file_name=dataset.get_sample_data_path(camera_token)\n",
    "        image1 = Image.open(file_name)\n",
    "        # compress image, it is too big\n",
    "        # convert image to numpy array\n",
    "        image = image1.resize((306,256), Image.ANTIALIAS)\n",
    "        img_array = np.asarray(image)\n",
    "        img_input_list.append(img_array)\n",
    "\n",
    "        cam_front_token = dataset.get('sample_data', sample['data'][\"CAM_FRONT\"])\n",
    "        cam_front_data = cam_front_token.get(\"calibrated_sensor_token\")\n",
    "        cam_front_calib = dataset.get(\"calibrated_sensor\", cam_front_data )\n",
    "        cam_front_coords = cam_front_calib.get(\"translation\")\n",
    "\n",
    "        cam_front_left_token = dataset.get('sample_data', sample['data'][\"CAM_FRONT_LEFT\"])\n",
    "        cam_front_left_data = cam_front_left_token.get(\"calibrated_sensor_token\")\n",
    "        cam_front_left_calib = dataset.get(\"calibrated_sensor\", cam_front_left_data )\n",
    "        cam_front_left_coords = cam_front_left_calib.get(\"translation\")\n",
    "\n",
    "        cam_front_right_token = dataset.get('sample_data', sample['data'][\"CAM_FRONT_RIGHT\"])\n",
    "        cam_front_right_data = cam_front_right_token.get(\"calibrated_sensor_token\")\n",
    "        cam_front_right_calib = dataset.get(\"calibrated_sensor\", cam_front_right_data )\n",
    "        cam_front_right_coords = cam_front_right_calib.get(\"translation\")\n",
    "\n",
    "        ground_plane = frame_helper.get_ground_plane_coeff(cam_front_coords, cam_front_left_coords, cam_front_right_coords)\n",
    "        ground_plane_list.append(ground_plane)\n",
    "        \n",
    "        token=img_data.get(\"calibrated_sensor_token\") \n",
    "        stereo_calib_p2 = frame_helper.read_calibration(token, dataset)\n",
    "\n",
    "        my_sample_token = dataset.get(\"sample\", my_sample_token)[\"next\"]\n",
    "        i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boxes_c[124][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_list = dataset.scene\n",
    "# [scene_list.append(s) for s in dataset.scenes]\n",
    "len(scene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lyft_dataset_sdk.utils.geometry_utils import view_points\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input=np.asarray(img_input_list)\n",
    "bev_input=np.asarray(bev_input_list)\n",
    "#img_targetnp=np.asarray(img_target)\n",
    "img_input=tf.convert_to_tensor(img_input)\n",
    "print(img_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    #inputs_vgg = tf.keras.layers.Input(shape=(224,224,3))\n",
    "    vgg16 = tf.keras.applications.VGG16(include_top=False, input_shape=(256,306,3),weights='imagenet')\n",
    "    vgg16.summary()\n",
    "#    bev_data = tf.keras.applications.vgg16.preprocess_input(bev_input)\n",
    "#    bev_features=vgg16.predict(bev_data)\n",
    "#     #exclude fully connected\n",
    "#     output = vgg16.layers[-3].output\n",
    "#     #struggle with memory\n",
    "    img_data = tf.keras.applications.vgg16.preprocess_input(img_input)\n",
    "    img_features=vgg16.predict(img_data)\n",
    "    print(img_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create anchors, recalibrate camera matrix and gt boxes to fit the reshaping of the img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "w_stride=32\n",
    "h_stride=30.6\n",
    "f_map_img_h=8\n",
    "f_map_img_w=9\n",
    "img_h=256\n",
    "img_w=306\n",
    "s_x=np.arange(0,f_map_img_w)*w_stride\n",
    "s_y=np.arange(0,f_map_img_h)*h_stride\n",
    "s_x, s_y=np.meshgrid(s_x,s_y)\n",
    "shifts=np.vstack((s_x.ravel(), s_y.ravel(), s_x.ravel(), s_y.ravel())).transpose()\n",
    "\n",
    "\n",
    "\n",
    "base_anchors=rpn_helper.gen_anchors(w_stride,h_stride)\n",
    "print(base_anchors.shape)\n",
    "base_anchors=base_anchors/4\n",
    "num_feature_map=f_map_img_h*f_map_img_w\n",
    "num_anchors_per_tile=12\n",
    "all_anchors = (base_anchors.reshape((1, num_anchors_per_tile, 4)) + shifts.reshape((1, num_feature_map, 4)).transpose((1, 0, 2)))\n",
    "border=0\n",
    "total_anchors=num_feature_map*num_anchors_per_tile\n",
    "\n",
    "all_anchors = all_anchors.reshape((total_anchors, 4))\n",
    "border=0\n",
    "inds_inside = np.where(\n",
    "        (all_anchors[:, 0] >= -border) &\n",
    "        (all_anchors[:, 1] >= -border) &\n",
    "        (all_anchors[:, 2] < img_w+border ) &  # width\n",
    "        (all_anchors[:, 3] < img_h+border)    # height\n",
    ")[0]\n",
    "anchors=all_anchors[inds_inside]\n",
    "print(anchors, anchors.shape)\n",
    "c2=np.array([[221, 0, 157],[0, 221, 129],[0, 0, 1]])\n",
    "x=[camera_intrinsic[0]/4, camera_intrinsic[1]/4, camera_intrinsic[2]]\n",
    "x=np.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[camera_intrinsic[0]/4, camera_intrinsic[1]/4, camera_intrinsic[2]]\n",
    "x=np.asarray(x)\n",
    "gt_boxes_corners=[]\n",
    "print(len(boxes_c))\n",
    "for i in range(len(boxes_c)):    \n",
    "    corner_img_list=[]\n",
    "    for j in range(len(boxes_c[i])):\n",
    "        corners = view_points(boxes_c[i][j].corners(), view=x, normalize=True)[:2, :]\n",
    "        np_corners=corners.T[:4]\n",
    "        corners_norm=[np_corners[1][0], np_corners[1][1], np_corners[0][0], np_corners[2][1]]\n",
    "        corner_img_list.append(corners_norm)\n",
    "    gt_boxes_corners.append(np.asarray(corner_img_list))\n",
    "print(np.asarray(gt_boxes_corners).shape, gt_boxes_corners[124]) #125, num box in scene, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 124\n",
    "img_arr = img_input_list[sample_index]\n",
    "image = Image.fromarray(img_arr) \n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 16))\n",
    "ax.imshow(image)\n",
    "corners = view_points(boxes_c[sample_index][0].corners(), x, normalize=True)[:2, :]\n",
    "corners1 = view_points(boxes_c[sample_index][1].corners(), x, normalize=True)[:2, :]\n",
    "#print(corners)\n",
    "# Draw front (first 4 corners) and rear (last 4 corners) rectangles(3d)/lines(2d)\n",
    "np_corners=corners.T[:4]\n",
    "np_corners1=corners1.T[:4]\n",
    "boxes_c[sample_index][0]\n",
    "boxes_c[sample_index][0].render(ax, view=x, normalize=True)\n",
    "boxes_c[sample_index][1].render(ax, view=x, normalize=True)\n",
    "boxes_c[sample_index][2].render(ax, view=x, normalize=True)\n",
    "boxes_c[sample_index][3].render(ax, view=x, normalize=True)\n",
    "boxes_c[sample_index][4].render(ax, view=x, normalize=True)\n",
    "\n",
    "rect1 = patches.Rectangle((np_corners1[1][0], np_corners1[1][1]), np_corners1[0][0]-np_corners1[1][0], -np_corners1[1][1]+np_corners1[2][1],linewidth=1,edgecolor=\"r\")\n",
    "rect2 = patches.Rectangle((np_corners[1][0], np_corners[1][1]), np_corners[0][0]-np_corners[1][0], -np_corners[1][1]+np_corners[2][1],linewidth=1,edgecolor=\"r\")\n",
    "# Add the patch to the Axes\n",
    "# ax.add_patch(rect2)\n",
    "# ax.add_patch(rect1)\n",
    "box_corn=[]\n",
    "for box in boxes_c:\n",
    "    corner_img_list=[]\n",
    "    for i in range(len(box)):\n",
    "        corners = view_points(box[i].corners(), x, normalize=True)[:2, :]\n",
    "        corners_norm=[np_corners[1][0], np_corners[1][1], np_corners[0][0], np_corners[2][1]]\n",
    "        corner_img_list.append(corners_norm)\n",
    "    box_corn.append(np.asarray(corner_img_list))\n",
    "box_corn=np.asarray(box_corn)\n",
    "print(\"here\")\n",
    "print(corner_img_list)\n",
    "#for i in range(len(anchors)):\n",
    "anch=anchors[121:123]\n",
    "rect= patches.Rectangle([196.71593642105634,134.4775074759832],  3, 7,linewidth=1,edgecolor=\"c\")\n",
    "ax.add_patch(rect)\n",
    "c=[\"r\", \"b\", \"c\", \"p\", \"g\" ]\n",
    "for i in anch:\n",
    "    print( i)\n",
    "    rect= patches.Rectangle([i[0],i[1]],   i[2]-i[0], i[3]-i[1],linewidth=1,edgecolor=\"g\")\n",
    "    ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same done above for bev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lyft_dataset_sdk.utils.geometry_utils import BoxVisibility\n",
    "# fig, ax2 = plt.subplots(1, 1, figsize=(9, 16))\n",
    "# bev1 = bev_helper.normalize_voxel_intensities(bev)\n",
    "# bev = bev_helper.normalize_voxel_intensities(bev)\n",
    "# bev2=bev\n",
    "# ax2.imshow(bev1)\n",
    "# sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "# print(sample_lidar_token)\n",
    "# _, boxes, _ = dataset.get_sample_data(sample_lidar_token, box_vis_level=BoxVisibility.ANY, flat_vehicle_coordinates=True)\n",
    "# ego_p=dataset.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "# bev_helper.move_boxes_to_car_space(bevboxes, ego_pose)\n",
    "# bev_helper.scale_boxes(bevboxes, 0.8)\n",
    "# corners=bev_helper.draw_boxes(bev, voxel_size, boxes, classes, z_offset=z_offset)\n",
    "# fig, ax3 = plt.subplots(1, 1, figsize=(9, 16))\n",
    "# ax3.imshow(bev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corners_voxel_list=[]\n",
    "# for box in boxes:   \n",
    "#     corners = box.bottom_corners()\n",
    "#     corners_voxel = bev_helper.car_to_voxel_coords(corners, [336,336,3], voxel_size, z_offset).transpose(1,0)\n",
    "#     corners_voxel = corners_voxel[:,:2] # Drop z coord\n",
    "#     corners_voxel_list.append(corners_voxel)\n",
    "#     cv2.drawContours(bev, np.int0([corners_voxel]), 0, -1)\n",
    "# plt.imshow(bev)\n",
    "# print(corners_voxel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lyft_dataset_sdk.utils.geometry_utils import BoxVisibility, box_in_image, view_points\n",
    "# def visualize_lidar_of_sample(sample_token, axes_limit=80):\n",
    "#     sample = level5data.get(\"sample\", sample_token)\n",
    "#     sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "#     level5data.render_sample_data(sample_lidar_token, axes_limit=axes_limit)\n",
    "# #visualize_lidar_of_sample(sample_name)\n",
    "# my_last_sample_token = my_scene[\"last_sample_token\"]\n",
    "# sample = level5data.get(\"sample\", my_last_sample_token)\n",
    "# #sample = level5data.get(\"sample\", sample_name)\n",
    "# sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "# print(sample_lidar_token)\n",
    "# _, boxes, _ = dataset.get_sample_data(sample_lidar_token, box_vis_level=BoxVisibility.ANY, flat_vehicle_coordinates=True)\n",
    "# print(len(boxes))\n",
    "# sd_record = dataset.get(\"sample_data\", sample_lidar_token)\n",
    "# # Get aggregated point cloud in lidar frame.\n",
    "# sample_rec = dataset.get(\"sample\", sd_record[\"sample_token\"])\n",
    "# chan = sd_record[\"channel\"]\n",
    "# ref_chan = \"LIDAR_TOP\"\n",
    "# pc, times = LidarPointCloud.from_file_multisweep(\n",
    "#     dataset, sample_rec, chan, ref_chan, num_sweeps=1\n",
    "# )\n",
    "\n",
    "# # Compute transformation matrices for lidar point cloud\n",
    "# cs_record = dataset.get(\"calibrated_sensor\", sd_record[\"calibrated_sensor_token\"])\n",
    "# pose_record = dataset.get(\"ego_pose\", sd_record[\"ego_pose_token\"])\n",
    "# vehicle_from_sensor = np.eye(4)\n",
    "# vehicle_from_sensor[:3, :3] = Quaternion(cs_record[\"rotation\"]).rotation_matrix\n",
    "# vehicle_from_sensor[:3, 3] = cs_record[\"translation\"]\n",
    "\n",
    "# ego_yaw = Quaternion(pose_record[\"rotation\"]).yaw_pitch_roll[0]\n",
    "# rot_vehicle_flat_from_vehicle = np.dot(\n",
    "#     Quaternion(scalar=np.cos(ego_yaw / 2), vector=[0, 0, np.sin(ego_yaw / 2)]).rotation_matrix,\n",
    "#     Quaternion(pose_record[\"rotation\"]).inverse.rotation_matrix,\n",
    "# )\n",
    "\n",
    "# vehicle_flat_from_vehicle = np.eye(4)\n",
    "# vehicle_flat_from_vehicle[:3, :3] = rot_vehicle_flat_from_vehicle\n",
    "\n",
    "# # Init axes.\n",
    "# _, ax = plt.subplots(1, 1, figsize=(9, 9))\n",
    "\n",
    "\n",
    "# # Show point cloud.\n",
    "# points = view_points(\n",
    "#     pc.points[:3, :], np.dot(vehicle_flat_from_vehicle, vehicle_from_sensor), normalize=False\n",
    "# )\n",
    "\n",
    "# dists = np.sqrt(np.sum(pc.points[:2, :] ** 2, axis=0))\n",
    "# colors = np.minimum(1, dists / 40 / np.sqrt(2))\n",
    "# ax.scatter(points[0, :], points[1, :], c=colors, s=0.2)\n",
    "\n",
    "# # Show ego vehicle.\n",
    "# ax.plot(0, 0, \"x\", color=\"red\")\n",
    "\n",
    "# # Show boxes.\n",
    "# for box in boxes:\n",
    "#     box.render(ax, view=np.eye(4), colors=(\"c\", \"c\", \"c\"))\n",
    "\n",
    "# # Limit visible range.\n",
    "# ax.set_xlim(-70, 70)\n",
    "# ax.set_ylim(-70, 70)\n",
    "# ax.axis(\"off\")\n",
    "# ax.set_title(sd_record[\"channel\"])\n",
    "# ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute rpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    #now build the rpn and get proposals from the anchors\n",
    "    k=6 #anchor number for each point\n",
    "    feature_map_tile = tf.keras.layers.Input(shape=(3,3,512))\n",
    "    convolution_3x3 = tf.keras.layers.Conv2D(\n",
    "        filters=512,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(3,3),\n",
    "        padding='same',\n",
    "        name=\"3x3\"\n",
    "    )(feature_map_tile)\n",
    "\n",
    "    output_deltas = tf.keras.layers.Conv2D(\n",
    "        filters= 4 * k,\n",
    "        kernel_size=(1, 1),\n",
    "        activation=\"linear\",\n",
    "        kernel_initializer=\"uniform\",\n",
    "        name=\"deltas1\"\n",
    "    )(convolution_3x3)\n",
    "\n",
    "    output_scores = tf.keras.layers.Conv2D(\n",
    "        filters=1 * k,\n",
    "        kernel_size=(1, 1),\n",
    "        activation=\"sigmoid\",\n",
    "        kernel_initializer=\"uniform\",\n",
    "        name=\"scores1\"\n",
    "    )(convolution_3x3)\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    rpn_model = tf.keras.Model(inputs=[feature_map_tile], outputs=[output_scores, output_deltas])\n",
    "    #rpn_model.summary()\n",
    "    rpn_model.compile(optimizer='adam', loss={'scores1':rpn_helper.loss_cls, 'deltas1':rpn_helper.smoothL1})\n",
    "\n",
    "img_feat=np.asarray(img_features)\n",
    "#img_feat=tf.convert_to_tensor(x[0])\n",
    "#print(img_feat.shape)\n",
    "# print(img_feat.shape)\n",
    "# res=rpn_model.predict(img_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feat[124].shape\n",
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy.random as npr\n",
    "\n",
    "batch_tiles_list=[]\n",
    "batch_label_targets_list=[]\n",
    "batch_bbox_targets_list=[]\n",
    "print(img_feat[45].shape)\n",
    "print(len(gt_boxes_corners))\n",
    "for i in range(8,11):\n",
    "    gt_boxes=np.asarray(gt_boxes_corners[i])\n",
    "    overlaps = rpn_helper.bbox_overlaps(anchors, gt_boxes)\n",
    "    #print(overlaps)\n",
    "    # find the gt box with biggest overlap to each anchors,\n",
    "    # and the overlap ratio. result (len(anchors),)\n",
    "    argmax_overlaps = overlaps.argmax(axis=1)\n",
    "    max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n",
    "    # find the anchor with biggest overlap to each gt boxes,\n",
    "    # and the overlap ratio. result (len(gt_boxes),)\n",
    "    gt_argmax_overlaps = overlaps.argmax(axis=0)\n",
    "    #print(gt_argmax_overlaps)\n",
    "    gt_max_overlaps = overlaps[gt_argmax_overlaps,\n",
    "                                np.arange(overlaps.shape[1])]\n",
    "\n",
    "    gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n",
    "    labels = np.empty((len(inds_inside), ), dtype=np.float32)\n",
    "    labels.fill(-2)\n",
    "    #print(gt_argmax_overlaps, max_overlaps)\n",
    "    labels[gt_argmax_overlaps] = 1\n",
    "\n",
    "    labels[max_overlaps >= .1] = 1\n",
    "    # set negative labels\n",
    "    labels[max_overlaps <= .01] = -1\n",
    "    #print(labels==1)\n",
    "    # subsample positive labels if we have too many\n",
    "    #     num_fg = int(RPN_FG_FRACTION * RPN_BATCHSIZE)\n",
    "    fg_inds = np.where(labels == 1)[0]\n",
    "    num_bg = int(len(fg_inds) * 2)\n",
    "    bg_inds = np.where(labels == 0)[0]\n",
    "    if len(bg_inds) > num_bg:\n",
    "        disable_inds = npr.choice(\n",
    "            bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n",
    "        labels[disable_inds] = -1\n",
    "    #\n",
    "    batch_inds=inds_inside[labels!=-1]\n",
    "    batch_inds=(batch_inds / 6).astype(np.int)\n",
    "    full_labels = rpn_helper.unmap(labels, total_anchors, inds_inside, fill=-1)\n",
    "    batch_label_targets=full_labels.reshape(-1,1,1,1*6)[batch_inds]\n",
    "    bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "    bbox_targets = rpn_helper.bbox_transform(anchors, gt_boxes[argmax_overlaps, :])\n",
    "    pos_anchors=all_anchors[inds_inside[labels==1]]\n",
    "    bbox_targets = rpn_helper.bbox_transform(pos_anchors, gt_boxes[argmax_overlaps, :][labels==1])\n",
    "    bbox_targets = rpn_helper.unmap(bbox_targets, total_anchors, inds_inside[labels==1], fill=0)\n",
    "    batch_bbox_targets = bbox_targets.reshape(-1,1,1,4*6)[batch_inds]\n",
    "    feature_map=tf.expand_dims(img_feat[i], axis=0)\n",
    "    padded_fcmap=np.pad(feature_map,((0,0),(1,1),(1,1),(0,0)),mode='constant')\n",
    "    padded_fcmap=np.squeeze(padded_fcmap)\n",
    "    print(batch_bbox_targets.shape, batch_label_targets.shape)\n",
    "    batch_tiles=[]\n",
    "    print(batch_inds)\n",
    "    \n",
    "    ##credo sia qui l'errore!!!\n",
    "    for ind in batch_inds:\n",
    "        x = ind % f_map_img_w\n",
    "        y = int(ind/f_map_img_w)\n",
    "        print(x,y)\n",
    "        fc_3x3=padded_fcmap[y,x,:]\n",
    "        print(fc_3x3)\n",
    "        batch_tiles.append(fc_3x3)\n",
    "    print(batch_tiles)\n",
    "    #print(i,tf.convert_to_tensor(batch_tiles).shape)\n",
    "    batch_tiles_list.append(batch_tiles)\n",
    "    batch_label_targets_list.append(batch_label_targets)\n",
    "    batch_bbox_targets_list.append(batch_bbox_targets)\n",
    "# batch_tiles_list=np.asarray(batch_tiles_list)\n",
    "# batch_label_targets_list=np.asarray(batch_label_targets_list)\n",
    "# batch_bbox_targets_list=np.asarray(batch_bbox_targets_list)\n",
    "print(len(batch_tiles_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_label_targets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_tiles_list[50])\n",
    "print(batch_label_targets_list[10].shape ,batch_bbox_targets_list[10].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def input_generator():\n",
    "    for i in range(len(batch_tiles_list)):\n",
    "        if len(batch_tiles_list[i])!=0:\n",
    "            a=np.asarray(batch_tiles_list[i])\n",
    "            b=np.asarray(batch_label_targets_list[i])\n",
    "            c=np.asarray(batch_bbox_targets_list[i])\n",
    "            yield a, [b,c]\n",
    "print(np.asarray(batch_tiles_list[10]).shape, np.asarray(batch_label_targets_list[i]).shape, np.asarray(batch_bbox_targets_list[i]).shape)\n",
    "with tf.device('/CPU:0'):\n",
    "    #rpn_model.compile(optimizer='adam', loss={'deltas1':'mse', 'scores1':'binary_crossentropy'})\n",
    "    rpn_model.summary()\n",
    "    rpn_model.fit(np.asarray(batch_tiles_list[10]), [np.asarray(batch_label_targets_list[10]),np.asarray(batch_bbox_targets_list[10])], epochs=10)\n",
    "    stuff=rpn_model.predict(batch_tiles_list)\n",
    "    print(stuff[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with tf.device('/CPU:0'):\n",
    "#     #rpn_model.compile(optimizer='adam', loss={'deltas1':'mse', 'scores1':'binary_crossentropy'})\n",
    "#     #rpn_model.summary()\n",
    "#     #rpn_model.fit(input_generator(), epochs=3)\n",
    "#     stuff=rpn_model.predict(batch_tiles_list[0:30])\n",
    "#     print(stuff[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray(batch_tiles).shape)\n",
    "print(-1.81e+01==-18.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-gpu (Singularity)",
   "language": "python",
   "name": "tf_gpu_sif"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
