{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will have to change the filenames, i.e from kitti_utils to a more general name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import avod\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from avod.builders.dataset_builder import DatasetBuilder\n",
    "from avod.core.mini_batch_utils import MiniBatchUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "15991 instance,\n",
      "8 sensor,\n",
      "128 calibrated_sensor,\n",
      "149072 ego_pose,\n",
      "148 log,\n",
      "148 scene,\n",
      "18634 sample,\n",
      "149072 sample_data,\n",
      "539765 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 11.2 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 3.7 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "DATASET_VERSION = 'v1.02-train'\n",
    "DATASET_ROOT = '../../nuscenes-devkit/data/'\n",
    "\n",
    "level5data = LyftDataset(json_path=DATASET_ROOT + \"/v1.02-train\", data_path=DATASET_ROOT, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_preprocessing(dataset, indices):\n",
    "    \n",
    "    mini_batch_utils = MiniBatchUtils(level5data)\n",
    "    print(\"Generating mini batches in {}\".format(mini_batch_utils.avod_iou_type))\n",
    "\n",
    "    # Generate all mini-batches, this can take a long time\n",
    "    mini_batch_utils.preprocess_rpn_mini_batches(dataset, indices)\n",
    "\n",
    "    print(\"Mini batches generated\")\n",
    "\n",
    "\n",
    "def split_indices(dataset, num_children):\n",
    "    \"\"\"Splits indices between children\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset object\n",
    "        num_children: Number of children to split samples between\n",
    "\n",
    "    Returns:\n",
    "        indices_split: A list of evenly split indices\n",
    "    \"\"\"\n",
    "\n",
    "    all_indices = np.arange(dataset.num_samples)\n",
    "\n",
    "    # Pad indices to divide evenly\n",
    "    length_padding = (-len(all_indices)) % num_children\n",
    "    padded_indices = np.concatenate((all_indices,\n",
    "                                     np.zeros(length_padding,\n",
    "                                              dtype=np.int32)))\n",
    "\n",
    "    # Split and trim last set of indices to original length\n",
    "    indices_split = np.split(padded_indices, num_children)\n",
    "    indices_split[-1] = np.trim_zeros(indices_split[-1])\n",
    "\n",
    "    return indices_split\n",
    "\n",
    "\n",
    "def split_work(all_child_pids, dataset, indices_split, num_children):\n",
    "    \"\"\"Spawns children to do work\n",
    "\n",
    "    Args:\n",
    "        all_child_pids: List of child pids are appended here, the parent\n",
    "            process should use this list to wait for all children to finish\n",
    "        dataset: Dataset object\n",
    "        indices_split: List of indices to split between children\n",
    "        num_children: Number of children\n",
    "    \"\"\"\n",
    "\n",
    "    for child_idx in range(num_children):\n",
    "        new_pid = os.fork()\n",
    "        if new_pid:\n",
    "            all_child_pids.append(new_pid)\n",
    "        else:\n",
    "            indices = indices_split[child_idx]\n",
    "            print('child', dataset.classes,\n",
    "                  indices_split[child_idx][0],\n",
    "                  indices_split[child_idx][-1])\n",
    "            do_preprocessing(dataset, indices)\n",
    "            os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done clustering!! yay\n",
      "start processing..\n",
      "Generating mini batches in 2d\n",
      "Mini batches generated\n",
      "All Done (Serial) :)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generates anchors info which is used for mini batch sampling.\n",
    "\n",
    "Processing on 'Cars' can be split into multiple processes, see the Options\n",
    "section for configuration.\n",
    "\n",
    "Args:\n",
    "dataset: KittiDataset (optional)\n",
    "    If dataset is provided, only generate info for that dataset.\n",
    "    If no dataset provided, generates info for all 3 classes.\n",
    "\"\"\"\n",
    "\n",
    "# if level5data is not None:\n",
    "# do_preprocessing(level5data, None)\n",
    "# return\n",
    "\n",
    "car_dataset_config_path = avod.root_dir() + \\\n",
    "'/configs/mb_preprocessing/rpn_cars.config'\n",
    "ped_dataset_config_path = avod.root_dir() + \\\n",
    "'/configs/mb_preprocessing/rpn_pedestrians.config'\n",
    "cyc_dataset_config_path = avod.root_dir() + \\\n",
    "'/configs/mb_preprocessing/rpn_cyclists.config'\n",
    "ppl_dataset_config_path = avod.root_dir() + \\\n",
    "'/configs/mb_preprocessing/rpn_people.config'\n",
    "\n",
    "##############################\n",
    "# Options\n",
    "##############################\n",
    "# Serial vs parallel processing\n",
    "in_parallel = True\n",
    "\n",
    "process_car = True   # Cars\n",
    "process_ped = False  # Pedestrians\n",
    "process_cyc = False  # Cyclists\n",
    "process_ppl = True   # People (Pedestrians + Cyclists)\n",
    "\n",
    "# Number of child processes to fork, samples will\n",
    "#  be divided evenly amongst the processes (in_parallel must be True)\n",
    "num_car_children = 8\n",
    "num_ped_children = 8\n",
    "num_cyc_children = 8\n",
    "num_ppl_children = 8\n",
    "\n",
    "##############################\n",
    "# Dataset setup\n",
    "##############################\n",
    "car_dataset = DatasetBuilder.load_dataset_from_config( car_dataset_config_path, level5data)\n",
    "ppl_dataset = DatasetBuilder.load_dataset_from_config( ppl_dataset_config_path, level5data)\n",
    "\n",
    "print(\"done clustering!! yay\")\n",
    "\n",
    "#############################\n",
    "# Serial Processing\n",
    "#############################\n",
    "#if not in_parallel:\n",
    "print(\"start processing..\")\n",
    "if process_car:\n",
    "    do_preprocessing(level5data, None)\n",
    "# if process_ped:\n",
    "#     do_preprocessing(ped_dataset, None)\n",
    "# if process_cyc:\n",
    "#     do_preprocessing(cyc_dataset, None)\n",
    "# if process_ppl:\n",
    "#     do_preprocessing(ppl_dataset, None)\n",
    "\n",
    "print('All Done (Serial) :)')\n",
    "\n",
    "##############################\n",
    "# Parallel Processing\n",
    "##############################\n",
    "#else:\n",
    "\n",
    "# List of all child pids to wait on\n",
    "#all_child_pids = []\n",
    "\n",
    "# Cars\n",
    "# if process_car:\n",
    "#     car_indices_split = split_indices(car_dataset, num_car_children)\n",
    "#     split_work(\n",
    "#         all_child_pids,\n",
    "#         car_dataset,\n",
    "#         car_indices_split,\n",
    "#         num_car_children)\n",
    "\n",
    "# # Pedestrians\n",
    "# if process_ped:\n",
    "#     ped_indices_split = split_indices(ped_dataset, num_ped_children)\n",
    "#     split_work(\n",
    "#         all_child_pids,\n",
    "#         ped_dataset,\n",
    "#         ped_indices_split,\n",
    "#         num_ped_children)\n",
    "\n",
    "# # Cyclists\n",
    "# if process_cyc:\n",
    "#     cyc_indices_split = split_indices(cyc_dataset, num_cyc_children)\n",
    "#     split_work(\n",
    "#         all_child_pids,\n",
    "#         cyc_dataset,\n",
    "#         cyc_indices_split,\n",
    "#         num_cyc_children)\n",
    "\n",
    "# # People (Pedestrians + Cyclists)\n",
    "# if process_ppl:\n",
    "#     ppl_indices_split = split_indices(ppl_dataset, num_ppl_children)\n",
    "#     split_work(\n",
    "#         all_child_pids,\n",
    "#         ppl_dataset,\n",
    "#         ppl_indices_split,\n",
    "#         num_ppl_children)\n",
    "\n",
    "# # Wait to child processes to finish\n",
    "# print('num children:', len(all_child_pids))\n",
    "# for i, child_pid in enumerate(all_child_pids):\n",
    "#     os.waitpid(child_pid, 0)\n",
    "\n",
    "# print('All Done (Parallel)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
