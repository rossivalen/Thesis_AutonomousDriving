{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING\n",
    "### Get the dataset\n",
    "Load the dataset, split it in two for trainin and validation. As in the Reference model provided by [Lyft](https://level5.lyft.com/), a dataframe with one scene per row is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = 'v1.02-train'\n",
    "DATASET_ROOT = '../../nuscenes-devkit/data/'\n",
    "\n",
    "#The code will generate data, visualization and model checkpoints\n",
    "ARTIFACTS_FOLDER = \"./artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import glob\n",
    "#Disabled for numpy and opencv: avod has opencv and numpy versions for several methods\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import os\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 850M, pci bus id: 0000:0a:00.0, compute capability: 5.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.compat.v1.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.4) \n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True, gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "15991 instance,\n",
      "8 sensor,\n",
      "128 calibrated_sensor,\n",
      "149072 ego_pose,\n",
      "148 log,\n",
      "148 scene,\n",
      "18634 sample,\n",
      "149072 sample_data,\n",
      "539765 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 16.0 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 3.4 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "level5data = LyftDataset(json_path=DATASET_ROOT + \"/v1.02-train\", data_path=DATASET_ROOT, verbose=True)\n",
    "os.makedirs(ARTIFACTS_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Panda's dataframe with one scene per row. Useful for selecting the sensors later\n",
    "records = [(level5data.get('sample', record['first_sample_token'])['timestamp'], record) for record in\n",
    "        level5data.scene]\n",
    "\n",
    "entries = []\n",
    "\n",
    "for start_time, record in sorted(records):\n",
    "    start_time = level5data.get('sample', record['first_sample_token'])['timestamp'] / 1000000\n",
    "\n",
    "    token = record['token']\n",
    "    name = record['name']\n",
    "    date = datetime.utcfromtimestamp(start_time)\n",
    "    host = \"-\".join(record['name'].split(\"-\")[:2])\n",
    "    first_sample_token = record[\"first_sample_token\"]\n",
    "\n",
    "    entries.append((host, name, date, token, first_sample_token))\n",
    "            \n",
    "df = pd.DataFrame(entries, columns=[\"host\", \"scene_name\", \"date\", \"scene_token\", \"first_sample_token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host\n",
      "host-a004    42\n",
      "host-a005     1\n",
      "host-a006     3\n",
      "host-a007    26\n",
      "host-a008     5\n",
      "host-a009     9\n",
      "host-a011    51\n",
      "host-a012     2\n",
      "host-a015     6\n",
      "host-a017     3\n",
      "Name: scene_token, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "host_count_df = df.groupby(\"host\")['scene_token'].count()\n",
    "print(host_count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_hosts = [\"host-a007\", \"host-a008\", \"host-a009\"]\n",
    "\n",
    "validation_df = df[df[\"host\"].isin(validation_hosts)]\n",
    "vi = validation_df.index\n",
    "train_df = df[~df.index.isin(vi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \"other_vehicle\", \"animal\", \"emergency_vehicle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_token = train_df.first_sample_token.values[0]\n",
    "sample = level5data.get(\"sample\", sample_token)\n",
    "\n",
    "sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "\n",
    "ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "calibrated_sensor = level5data.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n",
    "\n",
    "# Homogeneous transformation matrix from car frame to world frame.\n",
    "global_from_car = transform_matrix(ego_pose['translation'],\n",
    "                                   Quaternion(ego_pose['rotation']), inverse=False)\n",
    "\n",
    "# Homogeneous transformation matrix from sensor coordinate frame to ego car frame.\n",
    "car_from_sensor = transform_matrix(calibrated_sensor['translation'], Quaternion(calibrated_sensor['rotation']),\n",
    "                                    inverse=False)\n",
    "lidar_pointcloud = LidarPointCloud.from_file(lidar_filepath)\n",
    "\n",
    "# The lidar pointcloud is defined in the sensor's reference frame.\n",
    "# We want it in the car's reference frame, so we transform each point\n",
    "lidar_pointcloud.transform(car_from_sensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the BEV for the LiDAR pointcloud data\n",
    "AVOD processes the point clouds from the Velodyne in Bird's Eye View."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bev_helper\n",
    "map_mask = level5data.map[0][\"mask\"]\n",
    "\n",
    "voxel_size = (0.4,0.4,1.5)\n",
    "z_offset = -2.0\n",
    "bev_shape = (336,336, 3)\n",
    "Quaternion()\n",
    "lidar_pointcloud = LidarPointCloud.from_file(lidar_filepath)\n",
    "bev = bev_helper.create_voxel_pointcloud(lidar_pointcloud.points, bev_shape, voxel_size=voxel_size, z_offset=z_offset)\n",
    "\n",
    "ego_centric_map = bev_helper.get_semantic_map_around_ego(map_mask, ego_pose, voxel_size=0.4, output_shape=(336,336)) \n",
    "# So that the values in the voxels range from 0,1 we set a maximum intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bev = bev_helper.normalize_voxel_intensities(bev)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(np.hstack((bev, ego_centric_map)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some hyperparameters we'll need to define for the system\n",
    "voxel_size = (0.4, 0.4, 1.5)\n",
    "z_offset = -2.0\n",
    "bev_shape = (336,336, 3)\n",
    "\n",
    "# We scale down each box so they are more separated when projected into our coarse voxel space.\n",
    "box_scale = 0.8\n",
    "\n",
    "# \"bev\" stands for birds eye view\n",
    "train_data_folder = os.path.join(ARTIFACTS_FOLDER, \"bev_train_data\")\n",
    "validation_data_folder = os.path.join(ARTIFACTS_FOLDER, \"./bev_validation_data\")\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: Need to do the following cell just once! uncomment for Linux or Mac, issues in Windows due to multiprocessing library of Anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import training_data\n",
    "# for df, data_folder in [(train_df, train_data_folder), (validation_df, validation_data_folder)]:\n",
    "#     print(\"Preparing data into {} using {} workers\".format(data_folder, NUM_WORKERS))\n",
    "#     first_samples = df.first_sample_token.values\n",
    "#     os.makedirs(data_folder, exist_ok=True)\n",
    "#     process_func = partial(training_data.prepare_training_data_for_scene,\n",
    "#                            output_folder=data_folder, bev_shape=bev_shape, voxel_size=voxel_size, z_offset=z_offset, box_scale=box_scale, level5data=level5data)\n",
    "\n",
    "#     print(len(first_samples))\n",
    "\n",
    "# #     pool = Pool(NUM_WORKERS)\n",
    "# #     for _ in tqdm_notebook(process_func(first_samples), total=len(first_samples)):\n",
    "# #         pass\n",
    "# #     pool.close()\n",
    "\n",
    "#     for i in range(len(first_samples)):\n",
    "#         process_func(first_samples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_samples = df.first_sample_token.values\n",
    "\n",
    "# training_data.prepare_training_data_for_scene(first_samples[18], output_folder=train_data_folder, bev_shape=bev_shape, voxel_size=voxel_size, z_offset=z_offset, box_scale=box_scale, level5data=level5data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">TRAINING</font>\n",
    "Based on the [AVOD algorithm](https://github.com/kujason/avod), we train the dataset. \n",
    "The goal is to study how the accuracy changes based on the type of sensors in input, and their number, thus changes to the AVOD algorithm have been made. Here we keep the two stage model.\n",
    "Will be divided in steps, to mimick the divisions made by AVOD's authors in the code.\n",
    "\n",
    "With respect to the original AVOD code, the following changes have been made:\n",
    "<li> Upgrades for compatibity issues with tensorflow 2.0: migrated from slim libs to keras Sequential</li>\n",
    "<li> Changes to support single type input </li>\n",
    "<li> VGGs take as input Lyft-style dataset </li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import avod\n",
    "from avod.core import trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>RPN MODEL</b>: It is the fist subnetwork that makes up the double stage AVOD algorithm. It uses two VGGs, one for images, one for LiDar, to find the bottleneck.\n",
    "Img VGG and Bev VGG have the same strucure, just have input from different sources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>VGG:</b> VGG is a convolutional neural network model. Here simplified model wrt K. Simonyan and A. Zisserman's model proposed in the paper \"Very Deep Convolutional Networks for Large-Scale Image Recognition\".\n",
    "Basically, it lacks dense layers at the end, and the last group of conv layers is smaller that theirs.\n",
    "Two VGGs, one for BEV, one for Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> helper functions:</b> [todo] fucking move them to separate classes, but just keep the stuff that you used, not like avod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anchor_helper\n",
    "from frame_helper import FrameCalibrationData\n",
    "import frame_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from avod.core import anchor_filter\n",
    "from avod.core import anchor_projector\n",
    "from avod.core import box_3d_encoder\n",
    "from avod.core import constants\n",
    "from avod.core import losses\n",
    "from avod.core import model\n",
    "from avod.core import summary_utils\n",
    "from avod.core.anchor_generators import grid_anchor_3d_generator\n",
    "from avod.datasets.kitti import kitti_aug\n",
    "import avod.datasets.kitti.kitti_utils as kitti_utils\n",
    "from avod.core.label_cluster_utils import LabelClusterUtils\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "class RpnModel(model.DetectionModel):\n",
    "    ##############################\n",
    "    # Keys for Placeholders\n",
    "    ##############################\n",
    "    PL_BEV_INPUT = 'bev_input_pl'\n",
    "    PL_IMG_INPUT = 'img_input_pl'\n",
    "    PL_ANCHORS = 'anchors_pl'\n",
    "\n",
    "    PL_BEV_ANCHORS = 'bev_anchors_pl'\n",
    "    PL_BEV_ANCHORS_NORM = 'bev_anchors_norm_pl'\n",
    "    PL_IMG_ANCHORS = 'img_anchors_pl'\n",
    "    PL_IMG_ANCHORS_NORM = 'img_anchors_norm_pl'\n",
    "    #PL_LABEL_ANCHORS = 'label_anchors_pl'\n",
    "    PL_LABEL_BOXES_3D = 'label_boxes_3d_pl'\n",
    "    #PL_LABEL_CLASSES = 'label_classes_pl'\n",
    "\n",
    "    PL_ANCHOR_IOUS = 'anchor_ious_pl'\n",
    "    PL_ANCHOR_OFFSETS = 'anchor_offsets_pl'\n",
    "    PL_ANCHOR_CLASSES = 'anchor_classes_pl'\n",
    "\n",
    "    # Sample info, including keys for projection to image space\n",
    "    # (e.g. camera matrix, image index, etc.)\n",
    "    PL_CALIB_P2 = 'frame_calib_p2'\n",
    "    PL_IMG_IDX = 'current_img_idx'\n",
    "    PL_GROUND_PLANE = 'ground_plane'\n",
    "\n",
    "    ##############################\n",
    "    # Keys for Predictions\n",
    "    ##############################\n",
    "    PRED_ANCHORS = 'rpn_anchors'\n",
    "\n",
    "    PRED_MB_OBJECTNESS_GT = 'rpn_mb_objectness_gt'\n",
    "    PRED_MB_OFFSETS_GT = 'rpn_mb_offsets_gt'\n",
    "\n",
    "    PRED_MB_MASK = 'rpn_mb_mask'\n",
    "    PRED_MB_OBJECTNESS = 'rpn_mb_objectness'\n",
    "    PRED_MB_OFFSETS = 'rpn_mb_offsets'\n",
    "\n",
    "    PRED_TOP_INDICES = 'rpn_top_indices'\n",
    "    PRED_TOP_ANCHORS = 'rpn_top_anchors'\n",
    "    PRED_TOP_OBJECTNESS_SOFTMAX = 'rpn_top_objectness_softmax'\n",
    "\n",
    "    ##############################\n",
    "    # Keys for Loss\n",
    "    ##############################\n",
    "    LOSS_RPN_OBJECTNESS = 'rpn_objectness_loss'\n",
    "    LOSS_RPN_REGRESSION = 'rpn_regression_loss'\n",
    "\n",
    "    def __init__(self, model_config, pipeline_config, train_val_test, dataset):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_config: configuration for the model\n",
    "            train_val_test: \"train\", \"val\", or \"test\"\n",
    "            dataset: the dataset that will provide samples and ground truth\n",
    "        \"\"\"\n",
    "\n",
    "        # Sets model configs (_config)\n",
    "        super(RpnModel, self).__init__(model_config)\n",
    "        self.pipeline_config = pipeline_config\n",
    "\n",
    "        if train_val_test not in [\"train\", \"val\", \"test\"]:\n",
    "            raise ValueError('Invalid train_val_test value,'\n",
    "                             'should be one of [\"train\", \"val\", \"test\"]')\n",
    "        self._train_val_test = train_val_test\n",
    "\n",
    "        self._is_training = (self._train_val_test == 'train')\n",
    "\n",
    "        # Input config\n",
    "        input_config = self._config.input_config\n",
    "        self._bev_pixel_size = np.asarray([input_config.bev_dims_h,\n",
    "                                           input_config.bev_dims_w])\n",
    "        self._bev_depth = input_config.bev_depth\n",
    "\n",
    "        self._img_pixel_size = np.asarray([input_config.img_dims_h,\n",
    "                                           input_config.img_dims_w])\n",
    "        self._img_depth = input_config.img_depth\n",
    "\n",
    "        # Rpn config\n",
    "        rpn_config = self._config.rpn_config\n",
    "        self.proposal_roi_crop_size = 3*2  #3*2\n",
    "        self._fusion_method = rpn_config.rpn_fusion_method\n",
    "\n",
    "        if self._train_val_test in [\"train\", \"val\"]:\n",
    "            self._nms_size = rpn_config.rpn_train_nms_size\n",
    "        else:\n",
    "            self._nms_size = rpn_config.rpn_test_nms_size\n",
    "\n",
    "        self._nms_iou_thresh = rpn_config.rpn_nms_iou_thresh\n",
    "\n",
    "        # Network input placeholders\n",
    "        self.placeholders = dict()\n",
    "\n",
    "        # Inputs to network placeholders\n",
    "        self._placeholder_inputs = dict()\n",
    "\n",
    "        # Information about the current sample\n",
    "        self.sample_info = dict()\n",
    "\n",
    "        # Dataset\n",
    "        self.classes = classes = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \"other_vehicle\", \"animal\", \"emergency_vehicle\"]\n",
    "        self.dataset = dataset\n",
    "        self.dataset.train_val_test = self._train_val_test\n",
    "        area_extents = self.pipeline_config.kitti_utils_config.area_extents\n",
    "        self._area_extents = np.reshape(area_extents, (3, 2))\n",
    "        self._bev_extents = self._area_extents[[0, 2]]\n",
    "        #solve the preproc issue before this!\n",
    "        label_cluster_utils = LabelClusterUtils(self.dataset)\n",
    "        self._cluster_sizes = label_cluster_utils.get_clusters() #issue fuckmemyselfandi\n",
    "        \n",
    "        anchor_strides = self.pipeline_config.kitti_utils_config.anchor_strides\n",
    "        self._anchor_strides= np.reshape(anchor_strides, (-1, 2))\n",
    "        self._anchor_generator = grid_anchor_3d_generator.GridAnchor3dGenerator()\n",
    "\n",
    "        self._path_drop_probabilities = self._config.path_drop_probabilities\n",
    "        self._train_on_all_samples = self._config.train_on_all_samples\n",
    "        self._eval_all_samples = self._config.eval_all_samples\n",
    "\n",
    "        if self._train_val_test in [\"val\", \"test\"]:\n",
    "            # Disable path-drop, this should already be disabled inside the\n",
    "            # evaluator, but just in case.\n",
    "            self._path_drop_probabilities[0] = 1.0\n",
    "            self._path_drop_probabilities[1] = 1.0\n",
    "\n",
    "    def _add_placeholder(self, dtype, shape, name):\n",
    "        placeholder = tf.compat.v1.placeholder(dtype, shape, name)\n",
    "        self.placeholders[name] = placeholder\n",
    "        return placeholder\n",
    "\n",
    "    def _set_up_input_pls(self):\n",
    "        \"\"\"Sets up input placeholders by adding them to self._placeholders.\n",
    "        Keys are defined as self.PL_*.\n",
    "        \"\"\"\n",
    "        # Combine config data\n",
    "        bev_dims = np.append(self._bev_pixel_size, self._bev_depth)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('bev_input'):\n",
    "            # Placeholder for BEV image input, to be filled in with feed_dict\n",
    "            bev_input_placeholder = self._add_placeholder(tf.float32, bev_dims,\n",
    "                                                          self.PL_BEV_INPUT)\n",
    "\n",
    "            self._bev_input_batches = tf.expand_dims(\n",
    "                bev_input_placeholder, axis=0)\n",
    "\n",
    "            self._bev_preprocessed = tf.image.resize(self._bev_input_batches, self._bev_pixel_size)\n",
    "\n",
    "            # Summary Images\n",
    "            bev_summary_images = tf.split(\n",
    "                bev_input_placeholder, self._bev_depth, axis=2)\n",
    "            tf.summary.image(\"bev_maps\", bev_summary_images,\n",
    "                             max_outputs=self._bev_depth)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('img_input'):\n",
    "            # Take variable size input images\n",
    "            img_input_placeholder = self._add_placeholder(\n",
    "                tf.float32,\n",
    "                [None, None, self._img_depth],\n",
    "                self.PL_IMG_INPUT)\n",
    "\n",
    "            self._img_input_batches = tf.expand_dims(\n",
    "                img_input_placeholder, axis=0)\n",
    "\n",
    "            self._img_preprocessed = tf.image.resize(self._img_input_batches, self._img_pixel_size)\n",
    "\n",
    "            # Summary Image\n",
    "            tf.summary.image(\"rgb_image\", self._img_preprocessed,\n",
    "                             max_outputs=2)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('pl_labels'):\n",
    "            #self._add_placeholder(tf.float32, [None, 6], self.PL_LABEL_ANCHORS)\n",
    "            self._add_placeholder(tf.float32, [None, 7], self.PL_LABEL_BOXES_3D)\n",
    "            #self._add_placeholder(tf.float32, [None], self.PL_LABEL_CLASSES)\n",
    "\n",
    "        # Placeholders for anchors\n",
    "        with tf.compat.v1.variable_scope('pl_anchors'):\n",
    "            self._add_placeholder(tf.float32, [None, 6], self.PL_ANCHORS)\n",
    "            self._add_placeholder(tf.float32, [None], self.PL_ANCHOR_IOUS)\n",
    "            self._add_placeholder(tf.float32, [None, 6], self.PL_ANCHOR_OFFSETS)\n",
    "            self._add_placeholder(tf.float32, [None], self.PL_ANCHOR_CLASSES)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('bev_anchor_projections'):\n",
    "                self._add_placeholder(tf.float32, [None, 4], self.PL_BEV_ANCHORS)\n",
    "                self._bev_anchors_norm_pl = self._add_placeholder( tf.float32, [None, 4], self.PL_BEV_ANCHORS_NORM)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('img_anchor_projections'):\n",
    "                self._add_placeholder(tf.float32, [None, 4], self.PL_IMG_ANCHORS)\n",
    "                self._img_anchors_norm_pl = self._add_placeholder( tf.float32, [None, 4], self.PL_IMG_ANCHORS_NORM)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('sample_info'):\n",
    "                # the calib matrix shape is (3 x 4)\n",
    "                self._add_placeholder( tf.float32, [3, 4], self.PL_CALIB_P2)\n",
    "                self._add_placeholder(tf.int32, shape=[1], name=self.PL_IMG_IDX)\n",
    "                self._add_placeholder(tf.float32, [4], self.PL_GROUND_PLANE)\n",
    "\n",
    "    def _set_up_feature_extractors(self):\n",
    "        \"\"\"Sets up feature extractors and stores feature maps and\n",
    "        bottlenecks as member variables.\n",
    "        \"\"\"\n",
    "        weight_decay=0.0005\n",
    "        #shape due to shape provided by dataset. BEV could not be adapted: too sparse.\n",
    "        \n",
    "        inputs_img = tf.keras.layers.Input(batch_shape=(None,1024,1224,3))\n",
    "        net = tf.keras.layers.Conv2D(filters = 32, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), \n",
    "                                     activation=tf.nn.relu, kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv1\")(inputs_img)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch1\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 32, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), \n",
    "                                     use_bias=False, padding='same', name=\"conv2\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch2\")(net)   \n",
    "\n",
    "        net = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool1\")(net)\n",
    "\n",
    "        net = tf.keras.layers.Conv2D(filters = 64, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv3\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch3\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 64, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv4\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch4\")(net)\n",
    "\n",
    "        net = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool2\")(net)\n",
    "\n",
    "        net = tf.keras.layers.Conv2D(filters = 128,kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv5\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch5\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 128, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), \n",
    "                                     use_bias=False, padding='same', name=\"conv6\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch6\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 128, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv7\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch7\")(net)\n",
    "\n",
    "        net = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool3\")(net)\n",
    "\n",
    "        net = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv8\") (net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch8\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv9\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch9\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv10\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch10\")(net)\n",
    "        \n",
    "        img_vgg = tf.keras.models.Model(inputs = inputs_img, outputs = net, name=\"img_vgg\")\n",
    "        \n",
    "        self.img_bottleneck = tf.keras.layers.Conv2D(filters = 32, kernel_size = [1,1], strides =(1,1), padding='same', name=\"bottleneck\")(net)\n",
    "        self.img_bottleneck= tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros',\n",
    "                                            gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', \n",
    "                                            beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(self.img_bottleneck)\n",
    "        \n",
    "         #shape due to shape provided by dataset. BEV could not be adapted: too sparse.\n",
    "        \n",
    "        inputs_bev = tf.keras.layers.Input(batch_shape=(None,336,336,3))\n",
    "        out = tf.keras.layers.Conv2D(filters = 32, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), \n",
    "                                     activation=tf.nn.relu, kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv1\")(inputs_bev)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch1\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 32, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), \n",
    "                                     use_bias=False, padding='same', name=\"conv2\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch2\")(out)   \n",
    "\n",
    "        out = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool1\")(out)\n",
    "\n",
    "        out = tf.keras.layers.Conv2D(filters = 64, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv3\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch3\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 64, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv4\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch4\")(out)\n",
    "\n",
    "        out = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool2\")(out)\n",
    "\n",
    "        out = tf.keras.layers.Conv2D(filters = 128,kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv5\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch5\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 128, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), \n",
    "                                     use_bias=False, padding='same', name=\"conv6\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch6\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 128, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv7\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch7\")(out)\n",
    "\n",
    "        out = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool3\")(out)\n",
    "\n",
    "        out = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv8\") (out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch8\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv9\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch9\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv10\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch10\")(out)\n",
    "        \n",
    "        bev_vgg = tf.keras.models.Model(inputs = inputs_bev, outputs = out, name=\"bev_vgg\")\n",
    "        \n",
    "        self.bev_bottleneck = tf.keras.layers.Conv2D(filters = 32, kernel_size = [1,1], strides =(1,1), padding='same', name=\"bottleneck\")(out)\n",
    "        self.bev_bottleneck= tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros',\n",
    "                                            gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', \n",
    "                                            beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(self.bev_bottleneck)\n",
    "\n",
    "        # # Visualize the end point feature maps being used\n",
    "        bev_vgg.summary()\n",
    "        bev_end_point=bev_vgg.get_config()\n",
    "        img_vgg.summary()\n",
    "        img_end_point=img_vgg.get_config()\n",
    "\n",
    "    def build(self):\n",
    "\n",
    "        # Setup input placeholders\n",
    "        self._set_up_input_pls()\n",
    "\n",
    "        # Setup feature extractors\n",
    "        self._set_up_feature_extractors()\n",
    "\n",
    "        bev_proposal_input = self.bev_bottleneck\n",
    "        img_proposal_input = self.img_bottleneck\n",
    "\n",
    "        fusion_mean_div_factor = 2.0\n",
    "\n",
    "        # If both img and bev probabilites are set to 1.0, don't do\n",
    "        # path drop.\n",
    "        if not (self._path_drop_probabilities[0] ==\n",
    "                self._path_drop_probabilities[1] == 1.0):\n",
    "            with tf.compat.v1.variable_scope('rpn_path_drop'):\n",
    "\n",
    "                random_values = tf.random_uniform(shape=[3],\n",
    "                                                  minval=0.0,\n",
    "                                                  maxval=1.0)\n",
    "\n",
    "                img_mask, bev_mask = self.create_path_drop_masks(\n",
    "                    self._path_drop_probabilities[0],\n",
    "                    self._path_drop_probabilities[1],\n",
    "                    random_values)\n",
    "\n",
    "                img_proposal_input = tf.multiply(img_proposal_input,\n",
    "                                                 img_mask)\n",
    "\n",
    "                bev_proposal_input = tf.multiply(bev_proposal_input,\n",
    "                                                 bev_mask)\n",
    "\n",
    "                self.img_path_drop_mask = img_mask\n",
    "                self.bev_path_drop_mask = bev_mask\n",
    "\n",
    "                # Overwrite the division factor\n",
    "                fusion_mean_div_factor = img_mask + bev_mask\n",
    "\n",
    "        with tf.compat.v1.variable_scope('proposal_roi_pooling'):\n",
    "\n",
    "            with tf.compat.v1.variable_scope('box_indices'):\n",
    "                def get_box_indices(boxes):\n",
    "                    proposals_shape = boxes.get_shape().as_list()\n",
    "                    if any(dim is None for dim in proposals_shape):\n",
    "                        proposals_shape = tf.shape(boxes)\n",
    "                    ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)\n",
    "                    multiplier = tf.expand_dims(\n",
    "                        tf.range(start=0, limit=proposals_shape[0]), 1)\n",
    "                    return tf.reshape(ones_mat * multiplier, [-1])\n",
    "\n",
    "                bev_boxes_norm_batches = tf.expand_dims(\n",
    "                    self._bev_anchors_norm_pl, axis=0)\n",
    "\n",
    "                # These should be all 0's since there is only 1 image\n",
    "                tf_box_indices = get_box_indices(bev_boxes_norm_batches)\n",
    "            \n",
    "            proposal_roi_size_tf = [3,3]\n",
    "            # Do ROI Pooling on BEV\n",
    "            bev_proposal_rois = tf.image.crop_and_resize(\n",
    "                bev_proposal_input,\n",
    "                self._bev_anchors_norm_pl,\n",
    "                tf_box_indices,\n",
    "                proposal_roi_size_tf)\n",
    "            # Do ROI Pooling on image\n",
    "            img_proposal_rois = tf.image.crop_and_resize(\n",
    "                img_proposal_input,\n",
    "                self._bev_anchors_norm_pl,\n",
    "                tf_box_indices,\n",
    "                proposal_roi_size_tf)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('proposal_roi_fusion'):\n",
    "            rpn_fusion_out = None\n",
    "            if self._fusion_method == 'mean':\n",
    "                tf_features_sum = tf.add(bev_proposal_rois, img_proposal_rois)\n",
    "                #rpn_fusion_out = tf.divide(tf_features_sum, fusion_mean_div_factor)\n",
    "                rpn_fusion_out = tf.divide(tf_features_sum, 2)\n",
    "            elif self._fusion_method == 'concat':\n",
    "                rpn_fusion_out = tf.concat(\n",
    "                    [bev_proposal_rois, img_proposal_rois], axis=3)\n",
    "            else:\n",
    "                raise ValueError('Invalid fusion method', self._fusion_method)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('anchor_predictor', 'ap', [rpn_fusion_out]):\n",
    "            #None because unknown\n",
    "            tensor_in = tf.keras.Input(shape=None, tensor=rpn_fusion_out)\n",
    "            print(\"here\", tf_features_sum)\n",
    "            # Rpn layers config\n",
    "            weight_decay = 0.005\n",
    "\n",
    "            # Use conv2d instead of fully_connected layers.\n",
    "            cls_fc6 = tf.keras.layers.Conv2D(filters=32, kernel_size = [3,3], kernel_initializer='ones', \n",
    "                                             kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='VALID', name=\"conv1\")(tensor_in)\n",
    "\n",
    "            cls_fc6_drop = tf.keras.layers.Dropout(rate = 0.5, name=\"drop1\")(cls_fc6)\n",
    "\n",
    "            cls_fc7 = tf.keras.layers.Conv2D(filters=32, kernel_size = [1,1], kernel_initializer='ones',\n",
    "                                             kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='VALID', name=\"conv2\")(cls_fc6_drop)\n",
    "\n",
    "            cls_fc7_drop = tf.keras.layers.Dropout(rate = 0.5, name=\"drop2\")(cls_fc7)\n",
    "\n",
    "            cls_fc8 = tf.keras.layers.Conv2D(filters=2, kernel_size = [1,1], kernel_initializer='ones',\n",
    "                                             kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='VALID', name=\"conv3\")(cls_fc7_drop)\n",
    "\n",
    "            objectness = tf.squeeze(cls_fc8, axis=[1,2], name='conv3/squeezed')\n",
    "\n",
    "            # Use conv2d instead of fully_connected layers.\n",
    "            reg_fc6 = tf.keras.layers.Conv2D(filters=32, kernel_size = [3,3], kernel_initializer=\"ones\", \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='VALID', name=\"conv4\")(tensor_in)\n",
    "\n",
    "            reg_fc6_drop = tf.keras.layers.Dropout(rate = 0.5, name=\"drop3\")(reg_fc6)\n",
    "\n",
    "            reg_fc7 = tf.keras.layers.Conv2D(filters = 16, kernel_size = [1, 1], kernel_initializer=\"ones\",\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding=\"same\", name=\"conv5\")(reg_fc6_drop)\n",
    "\n",
    "            reg_fc7_drop = tf.keras.layers.Dropout(rate = 0.5, name=\"drop4\")(reg_fc7)\n",
    "\n",
    "            reg_fc8 = tf.keras.layers.Conv2D(filters = 6,  kernel_size = [1, 1],  kernel_initializer=\"ones\",\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding=\"same\", name=\"conv6\")(reg_fc7_drop)\n",
    "\n",
    "            offsets = tf.squeeze(reg_fc8, axis=[1,2], name='conv6/squeezed')\n",
    "            \n",
    "            model = tf.keras.models.Model(inputs = rpn_fusion_out, outputs = offsets, name=\"rpn_fusion_prediction_anchors\")\n",
    "            model1 = tf.keras.models.Model(inputs = rpn_fusion_out, outputs = objectness, name=\"objectness predictions\")\n",
    "            model.summary()\n",
    "            model1.summary()\n",
    "        # Return the proposals\n",
    "        with tf.compat.v1.variable_scope('proposals'):\n",
    "            anchors = self.placeholders[self.PL_ANCHORS]\n",
    "\n",
    "            # Decode anchor regression offsets\n",
    "            with tf.compat.v1.variable_scope('decoding'):\n",
    "                regressed_anchors = anchor_helper.offset_to_anchor( anchors, offsets)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('bev_projection'):\n",
    "                _, bev_proposal_boxes_norm = anchor_projector.project_to_bev(regressed_anchors, self._bev_extents)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('softmax'):\n",
    "                objectness_softmax = tf.nn.softmax(objectness)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('nms'):\n",
    "                objectness_scores = objectness_softmax[:, 1]\n",
    "\n",
    "                # Do NMS on regressed anchors\n",
    "                top_indices = tf.image.non_max_suppression(\n",
    "                    bev_proposal_boxes_norm, objectness_scores,\n",
    "                    max_output_size=self._nms_size,\n",
    "                    iou_threshold=self._nms_iou_thresh)\n",
    "\n",
    "                top_anchors = tf.gather(regressed_anchors, top_indices)\n",
    "                top_objectness_softmax = tf.gather(objectness_scores,\n",
    "                                                   top_indices)\n",
    "                # top_offsets = tf.gather(offsets, top_indices)\n",
    "                # top_objectness = tf.gather(objectness, top_indices)\n",
    "\n",
    "        # Get mini batch\n",
    "        all_ious_gt = self.placeholders[self.PL_ANCHOR_IOUS]\n",
    "        all_offsets_gt = self.placeholders[self.PL_ANCHOR_OFFSETS]\n",
    "        all_classes_gt = self.placeholders[self.PL_ANCHOR_CLASSES]\n",
    "\n",
    "        with tf.compat.v1.variable_scope('mini_batch'):\n",
    "            mini_batch_mask, _ = anchor_helper.sample_mini_batch(all_ious_gt, 64,[0, 0.3], [0.5,1])\n",
    "\n",
    "        # ROI summary images\n",
    "        rpn_mini_batch_size =64\n",
    "        with tf.compat.v1.variable_scope('bev_rpn_rois'):\n",
    "            mb_bev_anchors_norm = tf.boolean_mask(self._bev_anchors_norm_pl,\n",
    "                                                  mini_batch_mask)\n",
    "            mb_bev_box_indices = tf.zeros_like(\n",
    "                tf.boolean_mask(all_classes_gt, mini_batch_mask),\n",
    "                dtype=tf.int32)\n",
    "\n",
    "            # Show the ROIs of the BEV input density map\n",
    "            # for the mini batch anchors\n",
    "            bev_input_rois = tf.image.crop_and_resize(self._bev_preprocessed,\n",
    "                                                      mb_bev_anchors_norm, mb_bev_box_indices, (32, 32))\n",
    "\n",
    "            bev_input_roi_summary_images = tf.split(bev_input_rois, self._bev_depth, axis=3)\n",
    "            tf.summary.image('bev_rpn_rois', bev_input_roi_summary_images[-1], max_outputs=rpn_mini_batch_size)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('img_rpn_rois'):\n",
    "            # ROIs on image input\n",
    "            mb_img_anchors_norm = tf.boolean_mask(self._img_anchors_norm_pl, mini_batch_mask)\n",
    "            mb_img_box_indices = tf.zeros_like( tf.boolean_mask(all_classes_gt, mini_batch_mask), dtype=tf.int32)\n",
    "\n",
    "            # Do test ROI pooling on mini batch\n",
    "            img_input_rois = tf.image.crop_and_resize( self._img_preprocessed,\n",
    "                                                      mb_img_anchors_norm, mb_img_box_indices, (32, 32))\n",
    "\n",
    "            tf.summary.image('img_rpn_rois', img_input_rois, max_outputs=rpn_mini_batch_size)\n",
    "\n",
    "        # Ground Truth Tensors\n",
    "        with tf.compat.v1.variable_scope('one_hot_classes'):\n",
    "\n",
    "            # Anchor classification ground truth\n",
    "            # Object / Not Object\n",
    "            min_pos_iou = 0.5\n",
    "\n",
    "            objectness_classes_gt = tf.cast(tf.greater_equal(all_ious_gt, min_pos_iou), dtype=tf.int32)\n",
    "            objectness_gt = tf.one_hot(objectness_classes_gt, depth=2, on_value=1.0 - self._config.label_smoothing_epsilon,\n",
    "                                       off_value=self._config.label_smoothing_epsilon)\n",
    "\n",
    "        # Mask predictions for mini batch\n",
    "        with tf.compat.v1.variable_scope('prediction_mini_batch'):\n",
    "            objectness_masked = tf.boolean_mask(objectness, mini_batch_mask)\n",
    "            offsets_masked = tf.boolean_mask(offsets, mini_batch_mask)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('ground_truth_mini_batch'):\n",
    "            objectness_gt_masked = tf.boolean_mask(objectness_gt, mini_batch_mask)\n",
    "            offsets_gt_masked = tf.boolean_mask(all_offsets_gt, mini_batch_mask)\n",
    "\n",
    "        # Specify the tensors to evaluate\n",
    "        predictions = dict()\n",
    "\n",
    "        # Temporary predictions for debugging\n",
    "#         predictions['anchor_ious'] = anchor_ious\n",
    "#         predictions['anchor_offsets'] = all_offsets_gt\n",
    "\n",
    "        if self._train_val_test in ['train', 'val']:\n",
    "            # All anchors\n",
    "            predictions[self.PRED_ANCHORS] = anchors\n",
    "\n",
    "            # Mini-batch masks\n",
    "            predictions[self.PRED_MB_MASK] = mini_batch_mask\n",
    "            # Mini-batch predictions\n",
    "            predictions[self.PRED_MB_OBJECTNESS] = objectness_masked\n",
    "            predictions[self.PRED_MB_OFFSETS] = offsets_masked\n",
    "\n",
    "            # Mini batch ground truth\n",
    "            predictions[self.PRED_MB_OFFSETS_GT] = offsets_gt_masked\n",
    "            predictions[self.PRED_MB_OBJECTNESS_GT] = objectness_gt_masked\n",
    "\n",
    "            # Proposals after nms\n",
    "            predictions[self.PRED_TOP_INDICES] = top_indices\n",
    "            predictions[self.PRED_TOP_ANCHORS] = top_anchors\n",
    "            predictions[\n",
    "                self.PRED_TOP_OBJECTNESS_SOFTMAX] = top_objectness_softmax\n",
    "\n",
    "        else:\n",
    "            # self._train_val_test == 'test'\n",
    "            predictions[self.PRED_TOP_ANCHORS] = top_anchors\n",
    "            predictions[\n",
    "                self.PRED_TOP_OBJECTNESS_SOFTMAX] = top_objectness_softmax\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def create_feed_dict(self, scene_index=None):\n",
    "        \"\"\" Fills in the placeholders with the actual input values.\n",
    "            Currently, only a batch size of 1 is supported\n",
    "\n",
    "        Args:\n",
    "            sample_index: optional, only used when train_val_test == 'test',\n",
    "                a particular sample index in the dataset\n",
    "                sample list to build the feed_dict for\n",
    "\n",
    "        Returns:\n",
    "            a feed_dict dictionary that can be used in a tensorflow session\n",
    "        \"\"\"\n",
    "#TODO fix to have multiple batches\n",
    "#         if self._train_val_test in [\"train\", \"val\"]:\n",
    "\n",
    "#             # sample_index should be None\n",
    "#             if sample_index is not None:\n",
    "#                 raise ValueError('sample_index should be None. Do not load '\n",
    "#                                  'particular samples during train or val')\n",
    "\n",
    "#             # During training/validation, we need a valid sample\n",
    "#             # with anchor info for loss calculation\n",
    "#             sample = None\n",
    "#             anchors_info = []\n",
    "\n",
    "#             valid_sample = False\n",
    "#             while not valid_sample:\n",
    "#                 if self._train_val_test == \"train\":\n",
    "#                     # Get the a random sample from the remaining epoch\n",
    "#                     samples = self.dataset.next_batch(batch_size=1)\n",
    "\n",
    "#                 else:  # self._train_val_test == \"val\"\n",
    "#                     # Load samples in order for validation\n",
    "#                     samples = self.dataset.next_batch(batch_size=1, shuffle=False)\n",
    "\n",
    "#                 # Only handle one sample at a time for now\n",
    "#                 sample = samples[0]\n",
    "#                 anchors_info = sample.get(constants.KEY_ANCHORS_INFO)\n",
    "\n",
    "#                 # When training, if the mini batch is empty, go to the next\n",
    "#                 # sample. Otherwise carry on with found the valid sample.\n",
    "#                 # For validation, even if 'anchors_info' is empty, keep the\n",
    "#                 # sample (this will help penalize false positives.)\n",
    "#                 # We will substitue the necessary info with zeros later on.\n",
    "#                 # Note: Training/validating all samples can be switched off.\n",
    "#                 train_cond = (self._train_val_test == \"train\" and self._train_on_all_samples)\n",
    "#                 eval_cond = (self._train_val_test == \"val\" and self._eval_all_samples)\n",
    "#                 if anchors_info or train_cond or eval_cond:\n",
    "#                     valid_sample = True\n",
    "#         else:\n",
    "        # For testing, any sample should work\n",
    "        if scene_index is not None:\n",
    "            my_scene = self.dataset.scene[scene_index]\n",
    "        else:\n",
    "            raise TypeError('for testing you need to put a number! will change it later on once it works fully :) ')\n",
    "            \n",
    "\n",
    "        # Only handle one sample at a time for now\n",
    "        my_sample_token = my_scene[\"first_sample_token\"]\n",
    "        sample = level5data.get('sample', my_sample_token)\n",
    "        #anchors_info = sample.get(constants.KEY_ANCHORS_INFO)\n",
    "        anchors_info = []\n",
    "        sample_name = sample.get(\"token\")\n",
    "\n",
    "        # Get ground truth data connected to anchor info?????\n",
    "        #label_anchors = sample.get(constants.KEY_LABEL_ANCHORS) \n",
    "        # class_type = self.dataset.get(\"category\", sample)\n",
    "        # classes_category = class_type.get(\"name\")\n",
    "        # label_classes = classes.index(classes_category, start, end)\n",
    "        \n",
    "        # We only need orientation from box_3d\n",
    "        #label_boxes_3d = sample.get(constants.KEY_LABEL_BOXES_3D) #issue 5\n",
    "        label_boxes_3d = []\n",
    "\n",
    "        # Network input data\n",
    "        img_input = self.dataset.get('sample_data', sample['data'][\"CAM_FRONT\"])\n",
    "        img_data=img_input\n",
    "        camera_token=img_input.get(\"token\")\n",
    "        file_name=level5data.get_sample_data_path(camera_token)\n",
    "        image = Image.open(file_name)\n",
    "        # convert image to numpy array\n",
    "        img_input = np.asarray(image)\n",
    "        bev_input = self.dataset.get('sample_data', sample['data'][\"LIDAR_TOP\"])\n",
    "        bev_token = bev_input\n",
    "        #to feed the bev [todo, move this part in a preproc file]\n",
    "        lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "        lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "        ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "        calibrated_sensor_lidar = level5data.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n",
    "        global_from_car = transform_matrix(ego_pose['translation'], Quaternion(ego_pose['rotation']), inverse=False)\n",
    "        car_from_sensor_lidar = transform_matrix(calibrated_sensor_lidar['translation'], Quaternion(calibrated_sensor_lidar['rotation']),\n",
    "                                                  inverse=False)\n",
    "        lidar_pointcloud = LidarPointCloud.from_file(lidar_filepath)\n",
    "        lidar_pointcloud.transform(car_from_sensor_lidar)\n",
    "        map_mask = level5data.map[0][\"mask\"]\n",
    "        voxel_size = (0.4,0.4,1.5)\n",
    "        z_offset = -2.0\n",
    "        #arbitrary shape, must be square though!\n",
    "        bev_shape = (336,336, 3)\n",
    "        bev = bev_helper.create_voxel_pointcloud(lidar_pointcloud.points, bev_shape, voxel_size=voxel_size, z_offset=z_offset)\n",
    "        ego_centric_map = bev_helper.get_semantic_map_around_ego(map_mask, ego_pose, voxel_size=0.4, output_shape=(336,336)) \n",
    "        bev_input = bev_helper.normalize_voxel_intensities(bev)\n",
    "\n",
    "        # Image shape (h, w)\n",
    "        image_shape = [img_data.get(\"height\"), img_data.get(\"width\")]\n",
    "        \n",
    "        #ground plane shape (a,b,c,d) in kitti:\n",
    "        #no info on ground plane in nuscenes data, just global coordinate system\n",
    "        #which is given as x, y, z. Computed from the cameras position:\n",
    "        #suppose as in kitti that ground plane is as the same level with the cameras\n",
    "                       \n",
    "        cam_front_token = self.dataset.get('sample_data', sample['data'][\"CAM_FRONT\"])\n",
    "        cam_front_data = cam_front_token.get(\"calibrated_sensor_token\")\n",
    "        cam_front_calib = self.dataset.get(\"calibrated_sensor\", cam_front_data )\n",
    "        cam_front_coords = cam_front_calib.get(\"translation\")\n",
    "\n",
    "        cam_front_left_token = self.dataset.get('sample_data', sample['data'][\"CAM_FRONT_LEFT\"])\n",
    "        cam_front_left_data = cam_front_left_token.get(\"calibrated_sensor_token\")\n",
    "        cam_front_left_calib = self.dataset.get(\"calibrated_sensor\", cam_front_left_data )\n",
    "        cam_front_left_coords = cam_front_left_calib.get(\"translation\")\n",
    "\n",
    "        cam_front_right_token = self.dataset.get('sample_data', sample['data'][\"CAM_FRONT_RIGHT\"])\n",
    "        cam_front_right_data = cam_front_right_token.get(\"calibrated_sensor_token\")\n",
    "        cam_front_right_calib = self.dataset.get(\"calibrated_sensor\", cam_front_right_data )\n",
    "        cam_front_right_coords = cam_front_right_calib.get(\"translation\")\n",
    "        \n",
    "        ground_plane = frame_helper.get_ground_plane_coeff(cam_front_coords, cam_front_left_coords, cam_front_right_coords)\n",
    "        \n",
    "        #only for cameras, of course lidars do not have instrinsic matrices\n",
    "        token=img_data.get(\"calibrated_sensor_token\") \n",
    "        stereo_calib_p2 = frame_helper.read_calibration(token, self.dataset)\n",
    "\n",
    "        # Fill the placeholders for anchor information\n",
    "        self._fill_anchor_pl_inputs(anchors_info=anchors_info,sample_token=bev_token, ground_plane=ground_plane,\n",
    "                                    image_shape=image_shape, stereo_calib_p2=stereo_calib_p2,\n",
    "                                    sample_name=sample_name)\n",
    "\n",
    "        # this is a list to match the explicit shape for the placeholder\n",
    "        self._placeholder_inputs[self.PL_IMG_IDX] = [int(sample_name)]\n",
    "\n",
    "        # Fill in the rest\n",
    "        self._placeholder_inputs[self.PL_BEV_INPUT] = bev_input\n",
    "        self._placeholder_inputs[self.PL_IMG_INPUT] = image_input\n",
    "\n",
    "        #self._placeholder_inputs[self.PL_LABEL_ANCHORS] = label_anchors\n",
    "        self._placeholder_inputs[self.PL_LABEL_BOXES_3D] = label_boxes_3d\n",
    "        #self._placeholder_inputs[self.PL_LABEL_CLASSES] = label_classes\n",
    "\n",
    "        # Sample Info\n",
    "        # img_idx is a list to match the placeholder shape\n",
    "        self._placeholder_inputs[self.PL_IMG_IDX] = [int(sample_name)]\n",
    "        self._placeholder_inputs[self.PL_CALIB_P2] = stereo_calib_p2\n",
    "        self._placeholder_inputs[self.PL_GROUND_PLANE] = ground_plane\n",
    "\n",
    "        # Temporary sample info for debugging\n",
    "        self.sample_info.clear()\n",
    "        self.sample_info['sample_name'] = sample\n",
    "        self.sample_info['rpn_mini_batch'] = anchors_info\n",
    "\n",
    "        # Create a feed_dict and fill it with input values\n",
    "        feed_dict = dict()\n",
    "        for key, value in self.placeholders.items():\n",
    "            feed_dict[value] = self._placeholder_inputs[key]\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def _fill_anchor_pl_inputs(self,\n",
    "                               sample_token,\n",
    "                               anchors_info,\n",
    "                               ground_plane,\n",
    "                               image_shape,\n",
    "                               stereo_calib_p2,\n",
    "                               sample_name):\n",
    "        \"\"\"\n",
    "        Fills anchor placeholder inputs with corresponding data\n",
    "\n",
    "        Args:\n",
    "            anchors_info: anchor info from mini_batch_utils\n",
    "            ground_plane: ground plane coefficients\n",
    "            image_shape: image shape (h, w), used for projecting anchors\n",
    "            sample_name: name of the sample, e.g. \"000001\"\n",
    "        \"\"\"\n",
    "\n",
    "        # Lists for merging anchors info\n",
    "        all_anchor_boxes_3d = []\n",
    "        anchors_ious = []\n",
    "        anchor_offsets = []\n",
    "        anchor_classes = []\n",
    "\n",
    "        # Create anchors for each class\n",
    "        if len(self.classes) > 1:\n",
    "            for class_idx in range(len(self.classes)):\n",
    "                # Generate anchors for all classes\n",
    "                grid_anchor_boxes_3d = self._anchor_generator.generate(\n",
    "                    area_3d=self._area_extents,\n",
    "                    anchor_3d_sizes=self._cluster_sizes[class_idx],\n",
    "                    anchor_stride=self._anchor_strides[class_idx],\n",
    "                    ground_plane=ground_plane)\n",
    "                all_anchor_boxes_3d.append(grid_anchor_boxes_3d)\n",
    "            all_anchor_boxes_3d = np.concatenate(all_anchor_boxes_3d)\n",
    "        else:\n",
    "            # Don't loop for a single class\n",
    "            class_idx = 0\n",
    "            grid_anchor_boxes_3d = self._anchor_generator.generate(\n",
    "                area_3d=self._area_extents,\n",
    "                anchor_3d_sizes=self._cluster_sizes[class_idx],\n",
    "                anchor_stride=self._anchor_strides[class_idx],\n",
    "                ground_plane=ground_plane)\n",
    "            all_anchor_boxes_3d = grid_anchor_boxes_3d\n",
    "\n",
    "        # Filter empty anchors\n",
    "        # Skip if anchors_info is []\n",
    "        sample_has_labels = True\n",
    "        if self._train_val_test in ['train', 'val']:\n",
    "            # Read in anchor info during training / validation\n",
    "            if anchors_info:\n",
    "                anchor_indices, anchors_ious, anchor_offsets, \\\n",
    "                    anchor_classes = anchors_info\n",
    "\n",
    "                anchor_boxes_3d_to_use = all_anchor_boxes_3d[anchor_indices]\n",
    "            else:\n",
    "                train_cond = (self._train_val_test == \"train\" and\n",
    "                              self._train_on_all_samples)\n",
    "                eval_cond = (self._train_val_test == \"val\" and\n",
    "                             self._eval_all_samples)\n",
    "                if train_cond or eval_cond:\n",
    "                    sample_has_labels = False\n",
    "        else:\n",
    "            sample_has_labels = False\n",
    "\n",
    "        if not sample_has_labels:\n",
    "            # During testing, or validation with no anchor info, manually\n",
    "            # filter empty anchors\n",
    "            # TODO: share voxel_grid_2d with BEV generation if possible\n",
    "            voxel_grid_2d = create_sliced_voxel_grid_2d(self.dataset, sample_token, \"lidar\", ground_plane, image_shape=image_shape)\n",
    "\n",
    "            # Convert to anchors and filter\n",
    "            anchors_to_use = box_3d_encoder.box_3d_to_anchor(all_anchor_boxes_3d)\n",
    "            empty_filter = anchor_filter.get_empty_anchor_filter_2d(anchors_to_use, voxel_grid_2d, density_threshold=1)\n",
    "\n",
    "            anchor_boxes_3d_to_use = all_anchor_boxes_3d[empty_filter]\n",
    "\n",
    "        # Convert lists to ndarrays\n",
    "        anchor_boxes_3d_to_use = np.asarray(anchor_boxes_3d_to_use)\n",
    "        anchors_ious = np.asarray(anchors_ious)\n",
    "        anchor_offsets = np.asarray(anchor_offsets)\n",
    "        anchor_classes = np.asarray(anchor_classes)\n",
    "\n",
    "        # Flip anchors and centroid x offsets for augmented samples\n",
    "#             if kitti_aug.AUG_FLIPPING in sample_augs:\n",
    "#                 anchor_boxes_3d_to_use = kitti_aug.flip_boxes_3d(anchor_boxes_3d_to_use, flip_ry=False)\n",
    "#                 if anchors_info:\n",
    "#                     anchor_offsets[:, 0] = -anchor_offsets[:, 0]\n",
    "\n",
    "        # Convert to anchors\n",
    "        anchors_to_use = box_3d_encoder.box_3d_to_anchor( anchor_boxes_3d_to_use)\n",
    "        num_anchors = len(anchors_to_use)\n",
    "\n",
    "        # Project anchors into bev\n",
    "        bev_anchors, bev_anchors_norm = anchor_projector.project_to_bev( anchors_to_use, self._bev_extents)\n",
    "\n",
    "        # Project box_3d anchors into image space\n",
    "        img_anchors, img_anchors_norm = anchor_projector.project_to_image_space(anchors_to_use, stereo_calib_p2, image_shape)\n",
    "\n",
    "        # Reorder into [y1, x1, y2, x2] for tf.crop_and_resize op\n",
    "        self._bev_anchors_norm = bev_anchors_norm[:, [1, 0, 3, 2]]\n",
    "        self._img_anchors_norm = img_anchors_norm[:, [1, 0, 3, 2]]\n",
    "\n",
    "        # Fill in placeholder inputs\n",
    "        self._placeholder_inputs[self.PL_ANCHORS] = anchors_to_use\n",
    "\n",
    "        # If we are in train/validation mode, and the anchor infos\n",
    "        # are not empty, store them. Checking for just anchors_ious\n",
    "        # to be non-empty should be enough.\n",
    "        if self._train_val_test in ['train', 'val'] and \\\n",
    "                len(anchors_ious) > 0:\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_IOUS] = anchors_ious\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_OFFSETS] = anchor_offsets\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_CLASSES] = anchor_classes\n",
    "\n",
    "        # During test, or val when there is no anchor info\n",
    "        elif self._train_val_test in ['test'] or \\\n",
    "                len(anchors_ious) == 0:\n",
    "            # During testing, or validation with no gt, fill these in with 0s\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_IOUS] = \\\n",
    "                np.zeros(num_anchors)\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_OFFSETS] = \\\n",
    "                np.zeros([num_anchors, 6])\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_CLASSES] = \\\n",
    "                np.zeros(num_anchors)\n",
    "        else:\n",
    "            raise ValueError('Got run mode {}, and non-empty anchor info'.\n",
    "                             format(self._train_val_test))\n",
    "\n",
    "        self._placeholder_inputs[self.PL_BEV_ANCHORS] = bev_anchors\n",
    "        self._placeholder_inputs[self.PL_BEV_ANCHORS_NORM] = self._bev_anchors_norm\n",
    "        self._placeholder_inputs[self.PL_IMG_ANCHORS] = img_anchors\n",
    "        self._placeholder_inputs[self.PL_IMG_ANCHORS_NORM] = self._img_anchors_norm\n",
    "\n",
    "    def loss(self, prediction_dict):\n",
    "\n",
    "        # these should include mini-batch values only\n",
    "        objectness_gt = prediction_dict[self.PRED_MB_OBJECTNESS_GT]\n",
    "        offsets_gt = prediction_dict[self.PRED_MB_OFFSETS_GT]\n",
    "\n",
    "        # Predictions\n",
    "        with tf.compat.v1.variable_scope('rpn_prediction_mini_batch'):\n",
    "            objectness = prediction_dict[self.PRED_MB_OBJECTNESS]\n",
    "            offsets = prediction_dict[self.PRED_MB_OFFSETS]\n",
    "\n",
    "        with tf.compat.v1.variable_scope('rpn_losses'):\n",
    "            with tf.compat.v1.variable_scope('objectness'):\n",
    "                cls_loss = losses.WeightedSoftmaxLoss()\n",
    "                cls_loss_weight = self._config.loss_config.cls_loss_weight\n",
    "                objectness_loss = cls_loss(objectness, objectness_gt, weight=cls_loss_weight)\n",
    "\n",
    "                with tf.compat.v1.variable_scope('obj_norm'):\n",
    "                    # normalize by the number of anchor mini-batches\n",
    "                    objectness_loss = objectness_loss / tf.cast( tf.shape(objectness_gt)[0], dtype=tf.float32)\n",
    "                    tf.summary.scalar('objectness', objectness_loss)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('regression'):\n",
    "                reg_loss = losses.WeightedSmoothL1Loss()\n",
    "                reg_loss_weight = self._config.loss_config.reg_loss_weight\n",
    "                anchorwise_localization_loss = reg_loss(offsets, offsets_gt, weight=reg_loss_weight)\n",
    "                masked_localization_loss = anchorwise_localization_loss * objectness_gt[:, 1]\n",
    "                localization_loss = tf.reduce_sum(masked_localization_loss)\n",
    "\n",
    "                with tf.compat.v1.variable_scope('reg_norm'):\n",
    "                    # normalize by the number of positive objects\n",
    "                    num_positives = tf.reduce_sum(objectness_gt[:, 1])\n",
    "                    # Assert the condition `num_positives > 0`\n",
    "                    with tf.control_dependencies([tf.debugging.assert_positive(num_positives)]):\n",
    "                        localization_loss = localization_loss / num_positives\n",
    "                        tf.summary.scalar('regression', localization_loss)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('total_loss'):\n",
    "                total_loss = objectness_loss + localization_loss\n",
    "\n",
    "        loss_dict = {\n",
    "            self.LOSS_RPN_OBJECTNESS: objectness_loss,\n",
    "            self.LOSS_RPN_REGRESSION: localization_loss,\n",
    "        }\n",
    "\n",
    "        return loss_dict, total_loss\n",
    "\n",
    "    def create_path_drop_masks(self,\n",
    "                               p_img,\n",
    "                               p_bev,\n",
    "                               random_values):\n",
    "        \"\"\"Determines global path drop decision based on given probabilities.\n",
    "\n",
    "        Args:\n",
    "            p_img: A tensor of float32, probability of keeping image branch\n",
    "            p_bev: A tensor of float32, probability of keeping bev branch\n",
    "            random_values: A tensor of float32 of shape [3], the results\n",
    "                of coin flips, values should range from 0.0 - 1.0.\n",
    "\n",
    "        Returns:\n",
    "            final_img_mask: A constant tensor mask containing either one or zero\n",
    "                depending on the final coin flip probability.\n",
    "            final_bev_mask: A constant tensor mask containing either one or zero\n",
    "                depending on the final coin flip probability.\n",
    "        \"\"\"\n",
    "\n",
    "        def keep_branch(): return tf.constant(1.0)\n",
    "\n",
    "        def kill_branch(): return tf.constant(0.0)\n",
    "\n",
    "        # The logic works as follows:\n",
    "        # We have flipped 3 coins, first determines the chance of keeping\n",
    "        # the image branch, second determines keeping bev branch, the third\n",
    "        # makes the final decision in the case where both branches were killed\n",
    "        # off, otherwise the initial img and bev chances are kept.\n",
    "\n",
    "        img_chances = tf.case([(tf.less(random_values[0], p_img), keep_branch)], default=kill_branch)\n",
    "\n",
    "        bev_chances = tf.case([(tf.less(random_values[1], p_bev), keep_branch)], default=kill_branch)\n",
    "\n",
    "        # Decision to determine whether both branches were killed off\n",
    "        third_flip = tf.logical_or(tf.cast(img_chances, dtype=tf.bool), tf.cast(bev_chances, dtype=tf.bool))\n",
    "        third_flip = tf.cast(third_flip, dtype=tf.float32)\n",
    "\n",
    "        # Make a second choice, for the third case\n",
    "        # Here we use a 50/50 chance to keep either image or bev\n",
    "        # If its greater than 0.5, keep the image\n",
    "        img_second_flip = tf.case([(tf.greater(random_values[2], 0.5), keep_branch)], default=kill_branch)\n",
    "        # If its less than or equal to 0.5, keep bev\n",
    "        bev_second_flip = tf.case([(tf.less_equal(random_values[2], 0.5), keep_branch)],\n",
    "                                  default=kill_branch)\n",
    "\n",
    "        # Use lambda since this returns another condition and it needs to\n",
    "        # be callable\n",
    "        final_img_mask = tf.case([(tf.equal(third_flip, 1), lambda: img_chances)], default=lambda: img_second_flip)\n",
    "\n",
    "        final_bev_mask = tf.case([(tf.equal(third_flip, 1), lambda: bev_chances)], default=lambda: bev_second_flip)\n",
    "\n",
    "        return final_img_mask, final_bev_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test this part!! (and cry)\n",
    "\n",
    "Tests and results for the RPN model part. The following changes to the model were done after errors/issues with testing:\n",
    "<li>Maybe change the use of placeholders in the future, to fit with eager execution (shorter code)</li>\n",
    "<li>Some internal keras os function rises a warning, something will be depreated, doesn't tell where and what function exactly</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "density_threshold: 1\n",
      "rpn_config {\n",
      "  iou_2d_thresholds {\n",
      "    neg_iou_lo: 0.0\n",
      "    neg_iou_hi: 0.3\n",
      "    pos_iou_lo: 0.5\n",
      "    pos_iou_hi: 1.0\n",
      "  }\n",
      "  mini_batch_size: 64\n",
      "}\n",
      "avod_config {\n",
      "  iou_2d_thresholds {\n",
      "    neg_iou_lo: 0.0\n",
      "    neg_iou_hi: 0.55\n",
      "    pos_iou_lo: 0.65\n",
      "    pos_iou_hi: 1.0\n",
      "  }\n",
      "  mini_batch_size: 64\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RpnModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-c8dc5265f450>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkitti_utils_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmini_batch_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m rpn_model = RpnModel(model_config, pipeline_config[3],\n\u001b[0m\u001b[0;32m      9\u001b[0m                          \u001b[0mtrain_val_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                          dataset=level5data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RpnModel' is not defined"
     ]
    }
   ],
   "source": [
    "import avod.builders.config_builder_util as config_build\n",
    "config_path = 'avod/configs/unittest_model.config'\n",
    "pipe_path = 'avod/configs/unittest_pipeline.config'\n",
    "model_config = config_build.get_model_config_from_file(config_path)\n",
    "pipeline_config=config_build.get_configs_from_pipeline_file(pipe_path, \"val\")\n",
    "\n",
    "print(pipeline_config[3].kitti_utils_config.mini_batch_config)\n",
    "rpn_model = RpnModel(model_config, pipeline_config[3],\n",
    "                         train_val_test=\"val\",\n",
    "                         dataset=level5data)\n",
    "\n",
    "predictions = rpn_model.build()\n",
    "\n",
    "loss, total_loss = rpn_model.loss(predictions)\n",
    "\n",
    "feed_dict = rpn_model.create_feed_dict(5)\n",
    "\n",
    "# with self.test_session() as sess:\n",
    "#         init = tf.global_variables_initializer()\n",
    "#         sess.run(init)\n",
    "#         loss_dict_out = sess.run(loss, feed_dict=feed_dict)\n",
    "#         print('Losses ', loss_dict_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> AVOD MODEL: </b> second stage detector for the AVOD algorithm. It uses FPN as feature extractors.\n",
    "<b> FPN: </b> Feature Pyramid Network (FPN) is a feature extractor designed for such pyramid concept with accuracy and speed in mind. It replaces the feature extractor of detectors like Faster R-CNN and generates multiple feature map layers (multi-scale feature maps) with better quality information than the regular feature pyramid for object detection. [Understanding Feature Pyramid Networks for object detection (FPN)](https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avod.core.models import avod_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST AVOD!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
