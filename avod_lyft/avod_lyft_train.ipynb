{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING\n",
    "### Get the dataset\n",
    "Load the dataset, split it in two for trainin and validation. As in the Reference model provided by [Lyft](https://level5.lyft.com/), a dataframe with one scene per row is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = 'v1.02-train'\n",
    "DATASET_ROOT = '../../nuscenes-devkit/data/'\n",
    "\n",
    "#The code will generate data, visualization and model checkpoints\n",
    "ARTIFACTS_FOLDER = \"./artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import glob\n",
    "#Disabled for numpy and opencv: avod has opencv and numpy versions for several methods\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import os\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 850M, pci bus id: 0000:0a:00.0, compute capability: 5.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.compat.v1.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.4) \n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True, gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "15991 instance,\n",
      "8 sensor,\n",
      "128 calibrated_sensor,\n",
      "149072 ego_pose,\n",
      "148 log,\n",
      "148 scene,\n",
      "18634 sample,\n",
      "149072 sample_data,\n",
      "539765 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 27.9 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 4.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "level5data = LyftDataset(json_path=DATASET_ROOT + \"/v1.02-train\", data_path=DATASET_ROOT, verbose=True)\n",
    "os.makedirs(ARTIFACTS_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Panda's dataframe with one scene per row. Useful for selecting the sensors later\n",
    "records = [(level5data.get('sample', record['first_sample_token'])['timestamp'], record) for record in\n",
    "        level5data.scene]\n",
    "\n",
    "entries = []\n",
    "\n",
    "for start_time, record in sorted(records):\n",
    "    start_time = level5data.get('sample', record['first_sample_token'])['timestamp'] / 1000000\n",
    "\n",
    "    token = record['token']\n",
    "    name = record['name']\n",
    "    date = datetime.utcfromtimestamp(start_time)\n",
    "    host = \"-\".join(record['name'].split(\"-\")[:2])\n",
    "    first_sample_token = record[\"first_sample_token\"]\n",
    "\n",
    "    entries.append((host, name, date, token, first_sample_token))\n",
    "            \n",
    "df = pd.DataFrame(entries, columns=[\"host\", \"scene_name\", \"date\", \"scene_token\", \"first_sample_token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host\n",
      "host-a004    42\n",
      "host-a005     1\n",
      "host-a006     3\n",
      "host-a007    26\n",
      "host-a008     5\n",
      "host-a009     9\n",
      "host-a011    51\n",
      "host-a012     2\n",
      "host-a015     6\n",
      "host-a017     3\n",
      "Name: scene_token, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "host_count_df = df.groupby(\"host\")['scene_token'].count()\n",
    "print(host_count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_hosts = [\"host-a007\", \"host-a008\", \"host-a009\"]\n",
    "\n",
    "validation_df = df[df[\"host\"].isin(validation_hosts)]\n",
    "vi = validation_df.index\n",
    "train_df = df[~df.index.isin(vi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \"other_vehicle\", \"animal\", \"emergency_vehicle\"]\n",
    "classes_avod = [\"car\", \"pedestrian\", \"bicycle\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the BEV for the LiDAR pointcloud data\n",
    "AVOD processes the point clouds from the Velodyne in Bird's Eye View."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_token = train_df.first_sample_token.values[0]\n",
    "sample = level5data.get(\"sample\", sample_token)\n",
    "\n",
    "sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "sample_rgb_token = sample[\"data\"][\"CAM_FRONT\"]\n",
    "lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "rgb_data = level5data.get(\"sample_data\", sample_rgb_token)\n",
    "lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "img_filepath = level5data.get_sample_data_path(sample_rgb_token)\n",
    "\n",
    "ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "calibrated_sensor_lidar = level5data.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n",
    "calibrated_sensor_rgb = level5data.get(\"calibrated_sensor\", rgb_data[\"calibrated_sensor_token\"])\n",
    "\n",
    "#Homogeneous transformation matrix from car frame to world frame.\n",
    "global_from_car = transform_matrix(ego_pose['translation'],\n",
    "                                   Quaternion(ego_pose['rotation']), inverse=False)\n",
    "\n",
    "#Homogeneous transformation matrix from sensor coordinate frame to ego car frame.\n",
    "car_from_sensor_lidar = transform_matrix(calibrated_sensor_lidar['translation'], Quaternion(calibrated_sensor_lidar['rotation']),\n",
    "                                    inverse=False)\n",
    "\n",
    "lidar_pointcloud = LidarPointCloud.from_file(lidar_filepath)\n",
    "\n",
    "#The lidar pointcloud is defined in the sensor's reference frame.\n",
    "#It must be in the car's reference frame, so we transform each point\n",
    "lidar_pointcloud.transform(car_from_sensor_lidar)\n",
    "\n",
    "#A sanity check, the points should be centered around 0 in car space.\n",
    "plt.hist(lidar_pointcloud.points[0], alpha=0.5, bins=30, label=\"X\")\n",
    "plt.hist(lidar_pointcloud.points[1], alpha=0.5, bins=30, label=\"Y\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Distance from car along axis\")\n",
    "plt.ylabel(\"Amount of points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_mask = level5data.map[0][\"mask\"]\n",
    "\n",
    "def get_semantic_map_around_ego(map_mask, ego_pose, voxel_size, output_shape):\n",
    "\n",
    "    def crop_image(image: np.array,\n",
    "                           x_px: int,\n",
    "                           y_px: int,\n",
    "                           axes_limit_px: int) -> np.array:\n",
    "                x_min = int(x_px - axes_limit_px)\n",
    "                x_max = int(x_px + axes_limit_px)\n",
    "                y_min = int(y_px - axes_limit_px)\n",
    "                y_max = int(y_px + axes_limit_px)\n",
    "\n",
    "                cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "                return cropped_image\n",
    "\n",
    "    pixel_coords = map_mask.to_pixel_coords(ego_pose['translation'][0], ego_pose['translation'][1])\n",
    "\n",
    "    extent = voxel_size*output_shape[0]*0.5\n",
    "    scaled_limit_px = int(extent * (1.0 / (map_mask.resolution)))\n",
    "    mask_raster = map_mask.mask()\n",
    "\n",
    "    cropped = crop_image(mask_raster, pixel_coords[0], pixel_coords[1], int(scaled_limit_px * np.sqrt(2)))\n",
    "\n",
    "    ypr_rad = Quaternion(ego_pose['rotation']).yaw_pitch_roll\n",
    "    yaw_deg = -np.degrees(ypr_rad[0])\n",
    "\n",
    "    rotated_cropped = np.array(Image.fromarray(cropped).rotate(yaw_deg))\n",
    "    ego_centric_map = crop_image(rotated_cropped, rotated_cropped.shape[1] / 2, rotated_cropped.shape[0] / 2,\n",
    "                                 scaled_limit_px)[::-1]\n",
    "    \n",
    "    ego_centric_map = cv2.resize(ego_centric_map, output_shape[:2], cv2.INTER_NEAREST)\n",
    "    return ego_centric_map.astype(np.float32)/255\n",
    "\n",
    "def create_transformation_matrix_to_voxel_space(shape, voxel_size, offset):\n",
    "    \"\"\"\n",
    "    Constructs a transformation matrix given an output voxel shape such that (0,0,0) ends up in the center.\n",
    "    Voxel_size defines how large every voxel is in world coordinate, (1,1,1) would be the same as Minecraft voxels.\n",
    "    \n",
    "    An offset per axis in world coordinates (metric) can be provided, this is useful for Z (up-down) in lidar points.\n",
    "    \"\"\"\n",
    "    \n",
    "    shape, voxel_size, offset = np.array(shape), np.array(voxel_size), np.array(offset)\n",
    "    \n",
    "    tm = np.eye(4, dtype=np.float32)\n",
    "    translation = shape/2 + offset/voxel_size\n",
    "    \n",
    "    tm = tm * np.array(np.hstack((1/voxel_size, [1])))\n",
    "\n",
    "    tm[:3, 3] = np.transpose(translation)\n",
    "    return tm\n",
    "\n",
    "def transform_points(points, transf_matrix):\n",
    "    \"\"\"\n",
    "    Transform (3,N) or (4,N) points using transformation matrix.\n",
    "    \"\"\"\n",
    "    if points.shape[0] not in [3,4]:\n",
    "        raise Exception(\"Points input should be (3,N) or (4,N) shape, received {}\".format(points.shape))\n",
    "    return transf_matrix.dot(np.vstack((points[:3, :], np.ones(points.shape[1]))))[:3, :]\n",
    "\n",
    "def car_to_voxel_coords(points, shape, voxel_size, z_offset=0):\n",
    "    if len(shape) != 3:\n",
    "        raise Exception(\"Voxel volume shape should be 3 dimensions (x,y,z)\")\n",
    "        \n",
    "    if len(points.shape) != 2 or points.shape[0] not in [3, 4]:\n",
    "        raise Exception(\"Input points should be (3,N) or (4,N) in shape, found {}\".format(points.shape))\n",
    "\n",
    "    tm = create_transformation_matrix_to_voxel_space(shape, voxel_size, (0, 0, z_offset))\n",
    "    p = transform_points(points, tm)\n",
    "    return p\n",
    "\n",
    "def create_voxel_pointcloud(points, shape, voxel_size=(0.5,0.5,1), z_offset=0):\n",
    "\n",
    "    points_voxel_coords = car_to_voxel_coords(points.copy(), shape, voxel_size, z_offset)\n",
    "    points_voxel_coords = points_voxel_coords[:3].transpose(1,0)\n",
    "    points_voxel_coords = np.int0(points_voxel_coords)\n",
    "    \n",
    "    bev = np.zeros(shape, dtype=np.float32)\n",
    "    bev_shape = np.array(shape)\n",
    "\n",
    "    within_bounds = (np.all(points_voxel_coords >= 0, axis=1) * np.all(points_voxel_coords < bev_shape, axis=1))\n",
    "    \n",
    "    points_voxel_coords = points_voxel_coords[within_bounds]\n",
    "    coord, count = np.unique(points_voxel_coords, axis=0, return_counts=True)\n",
    "        \n",
    "    # Note X and Y are flipped:\n",
    "    bev[coord[:,1], coord[:,0], coord[:,2]] = count\n",
    "    \n",
    "    return bev\n",
    "\n",
    "def normalize_voxel_intensities(bev, max_intensity=16):\n",
    "    return (bev/max_intensity).clip(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "voxel_size = (0.4,0.4,1.5)\n",
    "z_offset = -2.0\n",
    "bev_shape = (336,336, 3)\n",
    "\n",
    "bev = create_voxel_pointcloud(lidar_pointcloud.points, bev_shape, voxel_size=voxel_size, z_offset=z_offset)\n",
    "\n",
    "ego_centric_map = get_semantic_map_around_ego(map_mask, ego_pose, voxel_size=0.4, output_shape=(336,336)) \n",
    "# So that the values in the voxels range from 0,1 we set a maximum intensity.\n",
    "bev = normalize_voxel_intensities(bev)\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(np.hstack((bev, ego_centric_map)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxes = level5data.get_boxes(sample_lidar_token)\n",
    "#print(boxes)\n",
    "target_im = np.zeros(bev.shape[:3], dtype=np.uint8)\n",
    "\n",
    "def move_boxes_to_car_space(boxes, ego_pose):\n",
    "    \"\"\"\n",
    "    Move boxes from world space to car space.\n",
    "    Note: mutates input boxes.\n",
    "    \"\"\"\n",
    "    translation = -np.array(ego_pose['translation'])\n",
    "    rotation = Quaternion(ego_pose['rotation']).inverse\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Bring box to car space\n",
    "        box.translate(translation)\n",
    "        box.rotate(rotation)\n",
    "        \n",
    "def scale_boxes(boxes, factor):\n",
    "    \"\"\"\n",
    "    Note: mutates input boxes\n",
    "    \"\"\"\n",
    "    for box in boxes:\n",
    "        box.wlh = box.wlh * factor\n",
    "\n",
    "def draw_boxes(im, voxel_size, boxes, classes, z_offset=0.0):\n",
    "    for box in boxes:\n",
    "        # We only care about the bottom corners\n",
    "        corners = box.bottom_corners()\n",
    "        corners_voxel = car_to_voxel_coords(corners, im.shape, voxel_size, z_offset).transpose(1,0)\n",
    "        corners_voxel = corners_voxel[:,:2] # Drop z coord\n",
    "\n",
    "        class_color = classes.index(box.name) + 1\n",
    "        \n",
    "        if class_color == 0:\n",
    "            raise Exception(\"Unknown class: {}\".format(box.name))\n",
    "\n",
    "        cv2.drawContours(im, np.int0([corners_voxel]), 0, (class_color, class_color, class_color), -1)\n",
    "\n",
    "move_boxes_to_car_space(boxes, ego_pose)\n",
    "scale_boxes(boxes, 0.8)\n",
    "draw_boxes(target_im, voxel_size, boxes, classes, z_offset=z_offset)\n",
    "#target_im = target_im[:,:,0] # take one channel only\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow((target_im > 0).astype(np.float32), cmap='Set2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some hyperparameters we'll need to define for the system\n",
    "voxel_size = (0.4, 0.4, 1.5)\n",
    "z_offset = -2.0\n",
    "bev_shape = (336,336, 3)\n",
    "\n",
    "# We scale down each box so they are more separated when projected into our coarse voxel space.\n",
    "box_scale = 0.8\n",
    "\n",
    "# \"bev\" stands for birds eye view\n",
    "train_data_folder = os.path.join(ARTIFACTS_FOLDER, \"bev_train_data\")\n",
    "validation_data_folder = os.path.join(ARTIFACTS_FOLDER, \"./bev_validation_data\")\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def prepare_training_data_for_scene(first_sample_token, output_folder, bev_shape, voxel_size, z_offset, box_scale, level5data):\n",
    "    \"\"\"\n",
    "    Given a first sample token (in a scene), output rasterized input volumes and targets in birds-eye-view perspective.\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    sample_token = first_sample_token\n",
    "    \n",
    "    while sample_token:\n",
    "        \n",
    "        sample = level5data.get(\"sample\", sample_token)\n",
    "\n",
    "        sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "        lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "        lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "\n",
    "        ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "        calibrated_sensor = level5data.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n",
    "\n",
    "\n",
    "        global_from_car = transform_matrix(ego_pose['translation'],\n",
    "                                           Quaternion(ego_pose['rotation']), inverse=False)\n",
    "\n",
    "        car_from_sensor = transform_matrix(calibrated_sensor['translation'], Quaternion(calibrated_sensor['rotation']),\n",
    "                                            inverse=False)\n",
    "\n",
    "        try:\n",
    "            lidar_pointcloud = LidarPointCloud.from_file(lidar_filepath)\n",
    "            lidar_pointcloud.transform(car_from_sensor)\n",
    "        except Exception as e:\n",
    "            print (\"Failed to load Lidar Pointcloud for {}: {}:\".format(sample_token, e))\n",
    "            sample_token = sample[\"next\"]\n",
    "            continue\n",
    "        \n",
    "        bev = create_voxel_pointcloud(lidar_pointcloud.points, bev_shape, voxel_size=voxel_size, z_offset=z_offset)\n",
    "        bev = normalize_voxel_intensities(bev)\n",
    "\n",
    "        \n",
    "        boxes = level5data.get_boxes(sample_lidar_token)\n",
    "\n",
    "        target = np.zeros(bev.shape[:3], dtype=np.uint8)\n",
    "        classes = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \"other_vehicle\", \"animal\", \"emergency_vehicle\"]\n",
    "        move_boxes_to_car_space(boxes, ego_pose)\n",
    "        scale_boxes(boxes, box_scale)\n",
    "        draw_boxes(target, voxel_size, boxes=boxes, classes=classes, z_offset=z_offset)\n",
    "\n",
    "        bev_im = np.round(bev*255).astype(np.uint8)\n",
    "        target_im = target[:,:,0] # take one channel only\n",
    "        \n",
    "        semantic_im = get_semantic_map_around_ego(map_mask, ego_pose, voxel_size[0], target_im.shape)\n",
    "        semantic_im = np.round(semantic_im*255).astype(np.uint8)\n",
    "        \n",
    "        #bev projection withou boxes\n",
    "        cv2.imwrite(os.path.join(output_folder, \"{}_input.png\".format(sample_token)), bev_im)\n",
    "        #mage with the boxes\n",
    "        cv2.imwrite(os.path.join(output_folder, \"{}_target.png\".format(sample_token)), target)\n",
    "        #semantic map of the scene, to be used for visualization after training\n",
    "        cv2.imwrite(os.path.join(output_folder, \"{}_map.png\".format(sample_token)), semantic_im)\n",
    "        \n",
    "        sample_token = sample[\"next\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: Need to do the following cell just once! uncomment for Linux or Mac, issues in Windows due to multiprocessing library of Anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df, data_folder in [(train_df, train_data_folder), (validation_df, validation_data_folder)]:\n",
    "#     print(\"Preparing data into {} using {} workers\".format(data_folder, NUM_WORKERS))\n",
    "#     first_samples = df.first_sample_token.values\n",
    "#     os.makedirs(data_folder, exist_ok=True)\n",
    "#     process_func = partial(prepare_training_data_for_scene,\n",
    "#                            output_folder=data_folder, bev_shape=bev_shape, voxel_size=voxel_size, z_offset=z_offset, box_scale=box_scale, level5data=level5data)\n",
    "\n",
    "#     print(len(first_samples))\n",
    "\n",
    "# #     pool = Pool(NUM_WORKERS)\n",
    "# #     for _ in tqdm_notebook(process_func(first_samples), total=len(first_samples)):\n",
    "# #         pass\n",
    "# #     pool.close()\n",
    "\n",
    "#     for i in range(len(first_samples)):\n",
    "#         process_func(first_samples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_samples = df.first_sample_token.values\n",
    "\n",
    "# prepare_training_data_for_scene(first_samples[18], output_folder=train_data_folder, bev_shape=bev_shape, voxel_size=voxel_size, z_offset=z_offset, box_scale=box_scale, level5data=level5data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13594\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "class BEVImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    Takes as input the filepaths, and outputs the tensors corresponding to the bev, the target with the bounding boxes\n",
    "    and the semantic map of the sourrounding area.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, input_filepaths, target_filepaths, map_filepaths=None):\n",
    "        self.input_filepaths = input_filepaths\n",
    "        self.target_filepaths = target_filepaths\n",
    "        self.map_filepaths = map_filepaths\n",
    "        \n",
    "        if map_filepaths is not None:\n",
    "            assert len(input_filepaths) == len(map_filepaths)\n",
    "        \n",
    "        assert len(input_filepaths) == len(target_filepaths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_filepath = self.input_filepaths[idx]\n",
    "        target_filepath = self.target_filepaths[idx]\n",
    "        \n",
    "        sample_token = input_filepath.split(\"/\")[-1].replace(\"_input.png\",\"\")\n",
    "        \n",
    "        im = cv2.imread(input_filepath, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        if self.map_filepaths:\n",
    "            map_filepath = self.map_filepaths[idx]\n",
    "            map_im = cv2.imread(map_filepath, cv2.IMREAD_UNCHANGED)\n",
    "            im = np.concatenate((im, map_im), axis=2)\n",
    "        \n",
    "        target = cv2.imread(target_filepath, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        im = im.astype(np.float32)/255\n",
    "        target = target.astype(np.int64)\n",
    "        \n",
    "        im = tf.convert_to_tensor(im.transpose(2,0,1))\n",
    "        target = tf.convert_to_tensor(target)\n",
    "        \n",
    "        return im, target, sample_token\n",
    "\n",
    "input_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_input.png\")))\n",
    "target_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_target.png\")))\n",
    "map_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_map.png\")))\n",
    "\n",
    "train_dataset = BEVImageDataset(input_filepaths, target_filepaths, map_filepaths)\n",
    "print(len(train_dataset))\n",
    "im, targets, sample_tokens = train_dataset[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">TRAINING</font>\n",
    "Based on the [AVOD algorithm](https://github.com/kujason/avod), we train the dataset. \n",
    "The goal is to study how the accuracy changes based on the type of sensors in input, and their number, thus changes to the AVOD algorithm have been made. Here we keep the two stage model.\n",
    "Will be divided in steps, to mimick the divisions made by AVOD's authors in the code.\n",
    "\n",
    "With respect to the original AVOD code, the following changes have been made:\n",
    "<li> Upgrades for compatibity issues with tensorflow 2.0: migrated from slim libs to keras Sequential</li>\n",
    "<li> Changes to support single type input </li>\n",
    "<li> VGGs take as input Lyft-style dataset </li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import avod\n",
    "#from avod.builders.dataset_builder import DatasetBuilder\n",
    "#from avod.core.models.avod_model import AvodModel\n",
    "#from avod.core.models.rpn_model import RpnModel\n",
    "from avod.core import trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>RPN MODEL</b>: It is the fist subnetwork that makes up the double stage AVOD algorithm. It uses two VGGs, one for images, one for LiDar, to find the bottleneck.\n",
    "Img VGG and Bev VGG have the same strucure, just have input from different sources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>VGG:</b> VGG is a convolutional neural network model. Here simplified model wrt K. Simonyan and A. Zisserman's model proposed in the paper \"Very Deep Convolutional Networks for Large-Scale Image Recognition\".\n",
    "Basically, it lacks dense layers at the end, and the last group of conv layers is smaller that theirs.\n",
    "Two VGGs, one for BEV, one for Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> helper functions:</b> [todo] fucking move them to separate classes, but just keep the stuff that you used, not like avod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import avod.core.format_checker as fc\n",
    "from avod.core.minibatch_samplers import balanced_positive_negative_sampler\n",
    "def sample_mini_batch(max_ious,\n",
    "                          mini_batch_size,\n",
    "                          negative_iou_range,\n",
    "                          positive_iou_range):\n",
    "        \"\"\"\n",
    "        Samples a mini batch based on anchor ious with ground truth\n",
    "\n",
    "        Args:\n",
    "            max_ious: a tensor of max ious with ground truth in\n",
    "                the shape (N,)\n",
    "            mini_batch_size: size of the mini batch to return\n",
    "            negative_iou_range: iou range to consider an anchor as negative\n",
    "            positive_iou_range: iou range to consider an anchor as positive\n",
    "\n",
    "        Returns:\n",
    "            mb_sampled: a boolean mask where True indicates anchors sampled\n",
    "                for the mini batch\n",
    "            mb_pos_sampled: a boolean mask where True indicates positive anchors\n",
    "        \"\"\"\n",
    "\n",
    "        bkg_and_neg_labels = tf.less(max_ious, negative_iou_range[1])\n",
    "        pos_labels = tf.greater(max_ious, positive_iou_range[0])\n",
    "        indicator = tf.logical_or(pos_labels, bkg_and_neg_labels)\n",
    "\n",
    "        if negative_iou_range[0] > 0.0:\n",
    "            # If neg_iou_lo is > 0.0, the mini batch may be empty.\n",
    "            # In that case, use all background and negative labels\n",
    "            neg_labels = tf.logical_and(bkg_and_neg_labels, tf.greater_equal(max_ious, negative_iou_range[0]))\n",
    "\n",
    "            new_indicator = tf.logical_or(pos_labels, neg_labels)\n",
    "\n",
    "            num_valid = tf.reduce_sum(tf.cast(indicator, tf.int32))\n",
    "            indicator = tf.cond(tf.greater(num_valid, 0), true_fn=lambda: tf.identity(new_indicator), \n",
    "                                false_fn=lambda: tf.identity(bkg_and_neg_labels))\n",
    "\n",
    "        sampler = balanced_positive_negative_sampler.BalancedPositiveNegativeSampler()\n",
    "        mb_sampled, mb_pos_sampled = sampler.subsample( indicator, mini_batch_size, pos_labels)\n",
    "\n",
    "        return mb_sampled, mb_pos_sampled\n",
    "    \n",
    "def anchor_to_offset(anchors, ground_truth):\n",
    "    \"\"\"Encodes the anchor regression predictions with the\n",
    "    ground truth.\n",
    "\n",
    "    Args:\n",
    "        anchors: A numpy array of shape (N, 6) representing\n",
    "            the generated anchors.\n",
    "        ground_truth: A numpy array of shape (6,) containing\n",
    "            the label boxes in the anchor format.\n",
    "\n",
    "    Returns:\n",
    "        anchor_offsets: A numpy array of shape (N, 6)\n",
    "            encoded/normalized with the ground-truth, representing the\n",
    "            offsets.\n",
    "    \"\"\"\n",
    "\n",
    "    fc.check_anchor_format(anchors)\n",
    "\n",
    "    anchors = np.asarray(anchors).reshape(-1, 6)\n",
    "    ground_truth = np.reshape(ground_truth, (6,))\n",
    "\n",
    "    # t_x_gt = (x_gt - x_anch)/dim_x_anch\n",
    "    t_x_gt = (ground_truth[0] - anchors[:, 0]) / anchors[:, 3]\n",
    "    # t_y_gt = (y_gt - y_anch)/dim_y_anch\n",
    "    t_y_gt = (ground_truth[1] - anchors[:, 1]) / anchors[:, 4]\n",
    "    # t_z_gt = (z_gt - z_anch)/dim_z_anch\n",
    "    t_z_gt = (ground_truth[2] - anchors[:, 2]) / anchors[:, 5]\n",
    "    # t_dx_gt = log(dim_x_gt/dim_x_anch)\n",
    "    t_dx_gt = np.log(ground_truth[3] / anchors[:, 3])\n",
    "    # t_dy_gt = log(dim_y_gt/dim_y_anch)\n",
    "    t_dy_gt = np.log(ground_truth[4] / anchors[:, 4])\n",
    "    # t_dz_gt = log(dim_z_gt/dim_z_anch)\n",
    "    t_dz_gt = np.log(ground_truth[5] / anchors[:, 5])\n",
    "    anchor_offsets = np.stack((t_x_gt, t_y_gt, t_z_gt, t_dx_gt, t_dy_gt, t_dz_gt), axis=1)\n",
    "    return anchor_offsets\n",
    "\n",
    "\n",
    "def tf_anchor_to_offset(anchors, ground_truth):\n",
    "    \"\"\"Encodes the anchor regression predictions with the\n",
    "    ground truth.\n",
    "\n",
    "    This function assumes the ground_truth tensor has been arranged\n",
    "    in a way that each corresponding row in ground_truth, is matched\n",
    "    with that anchor according to the highest IoU.\n",
    "    For instance, the ground_truth might be a matrix of shape (256, 6)\n",
    "    of repeated entries for the original ground truth of shape (x, 6),\n",
    "    where each entry has been selected as the highest IoU match with that\n",
    "    anchor. This is different from the same function in numpy format, where\n",
    "    we loop through all the ground truth anchors, and calculate IoUs for\n",
    "    each and then select the match with the highest IoU.\n",
    "\n",
    "    Args:\n",
    "        anchors: A tensor of shape (N, 6) representing\n",
    "            the generated anchors.\n",
    "        ground_truth: A tensor of shape (N, 6) containing\n",
    "            the label boxes in the anchor format. Each ground-truth entry\n",
    "            has been matched with the anchor in the same entry as having\n",
    "            the highest IoU.\n",
    "\n",
    "    Returns:\n",
    "        anchor_offsets: A tensor of shape (N, 6)\n",
    "            encoded/normalized with the ground-truth, representing the\n",
    "            offsets.\n",
    "    \"\"\"\n",
    "\n",
    "    fc.check_anchor_format(anchors)\n",
    "\n",
    "    # Make sure anchors and anchor_gts have the same shape\n",
    "    dim_cond = tf.equal(tf.shape(anchors), tf.shape(ground_truth))\n",
    "\n",
    "    with tf.control_dependencies([dim_cond]):\n",
    "        t_x_gt = (ground_truth[:, 0] - anchors[:, 0]) / anchors[:, 3]\n",
    "        t_y_gt = (ground_truth[:, 1] - anchors[:, 1]) / anchors[:, 4]\n",
    "        t_z_gt = (ground_truth[:, 2] - anchors[:, 2]) / anchors[:, 5]\n",
    "        t_dx_gt = tf.math.log(ground_truth[:, 3] / anchors[:, 3])\n",
    "        t_dy_gt = tf.math.log(ground_truth[:, 4] / anchors[:, 4])\n",
    "        t_dz_gt = tf.math.log(ground_truth[:, 5] / anchors[:, 5])\n",
    "        anchor_offsets = tf.stack((t_x_gt,\n",
    "                                   t_y_gt,\n",
    "                                   t_z_gt,\n",
    "                                   t_dx_gt,\n",
    "                                   t_dy_gt,\n",
    "                                   t_dz_gt), axis=1)\n",
    "\n",
    "        return anchor_offsets\n",
    "\n",
    "\n",
    "def offset_to_anchor(anchors, offsets):\n",
    "    \"\"\"Decodes the anchor regression predictions with the\n",
    "    anchor.\n",
    "\n",
    "    Args:\n",
    "        anchors: A numpy array or a tensor of shape [N, 6]\n",
    "            representing the generated anchors.\n",
    "        offsets: A numpy array or a tensor of shape\n",
    "            [N, 6] containing the predicted offsets in the\n",
    "            anchor format  [x, y, z, dim_x, dim_y, dim_z].\n",
    "\n",
    "    Returns:\n",
    "        anchors: A numpy array of shape [N, 6]\n",
    "            representing the predicted anchor boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    fc.check_anchor_format(anchors)\n",
    "    fc.check_anchor_format(offsets)\n",
    "\n",
    "    # x = dx * dim_x + x_anch\n",
    "    x_pred = (offsets[:, 0] * anchors[:, 3]) + anchors[:, 0]\n",
    "    # y = dy * dim_y + x_anch\n",
    "    y_pred = (offsets[:, 1] * anchors[:, 4]) + anchors[:, 1]\n",
    "    # z = dz * dim_z + z_anch\n",
    "    z_pred = (offsets[:, 2] * anchors[:, 5]) + anchors[:, 2]\n",
    "\n",
    "    tensor_format = isinstance(anchors, tf.Tensor)\n",
    "    if tensor_format:\n",
    "        # dim_x = exp(log(dim_x) + dx)\n",
    "        dx_pred = tf.math.exp(tf.math.log(anchors[:, 3]) + offsets[:, 3])\n",
    "        # dim_y = exp(log(dim_y) + dy)\n",
    "        dy_pred = tf.math.exp(tf.math.log(anchors[:, 4]) + offsets[:, 4])\n",
    "        # dim_z = exp(log(dim_z) + dz)\n",
    "        dz_pred = tf.math.exp(tf.math.log(anchors[:, 5]) + offsets[:, 5])\n",
    "        anchors = tf.stack((x_pred,\n",
    "                            y_pred,\n",
    "                            z_pred,\n",
    "                            dx_pred,\n",
    "                            dy_pred,\n",
    "                            dz_pred), axis=1)\n",
    "    else:\n",
    "        dx_pred = np.exp(np.log(anchors[:, 3]) + offsets[:, 3])\n",
    "        dy_pred = np.exp(np.log(anchors[:, 4]) + offsets[:, 4])\n",
    "        dz_pred = np.exp(np.log(anchors[:, 5]) + offsets[:, 5])\n",
    "        anchors = np.stack((x_pred,\n",
    "                            y_pred,\n",
    "                            z_pred,\n",
    "                            dx_pred,\n",
    "                            dy_pred,\n",
    "                            dz_pred), axis=1)\n",
    "\n",
    "    return anchors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "class FrameCalibrationData:\n",
    "    \"\"\"Frame Calibration Holder\n",
    "        3x3    intrinsic   Intrinsic camera matrix.\n",
    "\n",
    "        3x3    rotation    Rotation matrix.\n",
    "        \n",
    "        3x1    translation Translation vector\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.intrinsic = []\n",
    "        self.rotation = []\n",
    "        self.translation = []\n",
    "\n",
    "def read_calibration(sensor_token):\n",
    "    \"\"\"Reads in Calibration file from Kitti Dataset.\n",
    "\n",
    "    Keyword Arguments:\n",
    "    ------------------\n",
    "    sensor_token: keyword argument of the sensor that captured the frame.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    frame_calibration_info : frame full calibration info\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    calib_sensor_data = level5data.get(\"calibrated_sensor\", token)\n",
    "    intrinsic_temp = calib_sensor_data.get(\"camera_intrinsic\")\n",
    "    rotation_temp = calib_sensor_data.get(\"rotation\")\n",
    "    translation_temp = calib_sensor_data.get(\"translation\")\n",
    "    \n",
    "    frame_calibration_info=FrameCalibrationData()\n",
    "    frame_calibration_info.intrinsic=intrinsic_temp\n",
    "    frame_calibration_info.rotation=rotation_temp\n",
    "    frame_calibration_info.translation=translation_temp\n",
    "    \n",
    "    return frame_calibration_info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-1e1d46b973f2>, line 735)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-1e1d46b973f2>\"\u001b[1;36m, line \u001b[1;32m735\u001b[0m\n\u001b[1;33m    ground_plane = sample.get(constants.KEY_GROUND_PLANE) #issue 2\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#from avod.builders import feature_extractor_builder\n",
    "#from avod_lyft.avod.core.feature_extractors import BevVgg\n",
    "#from avod.core.feature_extractors import ImgVgg\n",
    "#from avod.core import anchor_encoder\n",
    "from PIL import Image\n",
    "from avod.core import anchor_filter\n",
    "from avod.core import anchor_projector\n",
    "from avod.core import box_3d_encoder\n",
    "from avod.core import constants\n",
    "from avod.core import losses\n",
    "from avod.core import model\n",
    "from avod.core import summary_utils\n",
    "from avod.core.anchor_generators import grid_anchor_3d_generator\n",
    "from avod.datasets.kitti import kitti_aug\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "class RpnModel(model.DetectionModel):\n",
    "    ##############################\n",
    "    # Keys for Placeholders\n",
    "    ##############################\n",
    "    PL_BEV_INPUT = 'bev_input_pl'\n",
    "    PL_IMG_INPUT = 'img_input_pl'\n",
    "    PL_ANCHORS = 'anchors_pl'\n",
    "\n",
    "    PL_BEV_ANCHORS = 'bev_anchors_pl'\n",
    "    PL_BEV_ANCHORS_NORM = 'bev_anchors_norm_pl'\n",
    "    PL_IMG_ANCHORS = 'img_anchors_pl'\n",
    "    PL_IMG_ANCHORS_NORM = 'img_anchors_norm_pl'\n",
    "    #PL_LABEL_ANCHORS = 'label_anchors_pl'\n",
    "    PL_LABEL_BOXES_3D = 'label_boxes_3d_pl'\n",
    "    PL_LABEL_CLASSES = 'label_classes_pl'\n",
    "\n",
    "    PL_ANCHOR_IOUS = 'anchor_ious_pl'\n",
    "    PL_ANCHOR_OFFSETS = 'anchor_offsets_pl'\n",
    "    PL_ANCHOR_CLASSES = 'anchor_classes_pl'\n",
    "\n",
    "    # Sample info, including keys for projection to image space\n",
    "    # (e.g. camera matrix, image index, etc.)\n",
    "    PL_CALIB_P2 = 'frame_calib_p2'\n",
    "    PL_IMG_IDX = 'current_img_idx'\n",
    "    PL_GROUND_PLANE = 'ground_plane'\n",
    "\n",
    "    ##############################\n",
    "    # Keys for Predictions\n",
    "    ##############################\n",
    "    PRED_ANCHORS = 'rpn_anchors'\n",
    "\n",
    "    PRED_MB_OBJECTNESS_GT = 'rpn_mb_objectness_gt'\n",
    "    PRED_MB_OFFSETS_GT = 'rpn_mb_offsets_gt'\n",
    "\n",
    "    PRED_MB_MASK = 'rpn_mb_mask'\n",
    "    PRED_MB_OBJECTNESS = 'rpn_mb_objectness'\n",
    "    PRED_MB_OFFSETS = 'rpn_mb_offsets'\n",
    "\n",
    "    PRED_TOP_INDICES = 'rpn_top_indices'\n",
    "    PRED_TOP_ANCHORS = 'rpn_top_anchors'\n",
    "    PRED_TOP_OBJECTNESS_SOFTMAX = 'rpn_top_objectness_softmax'\n",
    "\n",
    "    ##############################\n",
    "    # Keys for Loss\n",
    "    ##############################\n",
    "    LOSS_RPN_OBJECTNESS = 'rpn_objectness_loss'\n",
    "    LOSS_RPN_REGRESSION = 'rpn_regression_loss'\n",
    "\n",
    "    def __init__(self, model_config, pipeline_config, train_val_test, dataset):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_config: configuration for the model\n",
    "            train_val_test: \"train\", \"val\", or \"test\"\n",
    "            dataset: the dataset that will provide samples and ground truth\n",
    "        \"\"\"\n",
    "\n",
    "        # Sets model configs (_config)\n",
    "        super(RpnModel, self).__init__(model_config)\n",
    "        self.pipeline_config = pipeline_config\n",
    "\n",
    "        if train_val_test not in [\"train\", \"val\", \"test\"]:\n",
    "            raise ValueError('Invalid train_val_test value,'\n",
    "                             'should be one of [\"train\", \"val\", \"test\"]')\n",
    "        self._train_val_test = train_val_test\n",
    "\n",
    "        self._is_training = (self._train_val_test == 'train')\n",
    "\n",
    "        # Input config\n",
    "        input_config = self._config.input_config\n",
    "        self._bev_pixel_size = np.asarray([input_config.bev_dims_h,\n",
    "                                           input_config.bev_dims_w])\n",
    "        self._bev_depth = input_config.bev_depth\n",
    "\n",
    "        self._img_pixel_size = np.asarray([input_config.img_dims_h,\n",
    "                                           input_config.img_dims_w])\n",
    "        self._img_depth = input_config.img_depth\n",
    "\n",
    "        # Rpn config\n",
    "        rpn_config = self._config.rpn_config\n",
    "        self.proposal_roi_crop_size = 3*2  #3*2\n",
    "        self._fusion_method = rpn_config.rpn_fusion_method\n",
    "\n",
    "        if self._train_val_test in [\"train\", \"val\"]:\n",
    "            self._nms_size = rpn_config.rpn_train_nms_size\n",
    "        else:\n",
    "            self._nms_size = rpn_config.rpn_test_nms_size\n",
    "\n",
    "        self._nms_iou_thresh = rpn_config.rpn_nms_iou_thresh\n",
    "\n",
    "        # Network input placeholders\n",
    "        self.placeholders = dict()\n",
    "\n",
    "        # Inputs to network placeholders\n",
    "        self._placeholder_inputs = dict()\n",
    "\n",
    "        # Information about the current sample\n",
    "        self.sample_info = dict()\n",
    "\n",
    "        # Dataset\n",
    "        self.dataset = dataset\n",
    "        self.dataset.train_val_test = self._train_val_test\n",
    "        area_extents = self.pipeline_config.kitti_utils_config.area_extents\n",
    "        self._area_extents = np.reshape(area_extents, (3, 2))\n",
    "        self._bev_extents = self._area_extents[[0, 2]]\n",
    "        #self._cluster_sizes, _ = self.dataset.get_cluster_info() \n",
    "        anchor_strides = self.pipeline_config.kitti_utils_config.anchor_strides\n",
    "        self._anchor_strides= np.reshape(anchor_strides, (-1, 2))\n",
    "        self._anchor_generator = grid_anchor_3d_generator.GridAnchor3dGenerator()\n",
    "\n",
    "        self._path_drop_probabilities = self._config.path_drop_probabilities\n",
    "        self._train_on_all_samples = self._config.train_on_all_samples\n",
    "        self._eval_all_samples = self._config.eval_all_samples\n",
    "\n",
    "        if self._train_val_test in [\"val\", \"test\"]:\n",
    "            # Disable path-drop, this should already be disabled inside the\n",
    "            # evaluator, but just in case.\n",
    "            self._path_drop_probabilities[0] = 1.0\n",
    "            self._path_drop_probabilities[1] = 1.0\n",
    "\n",
    "    def _add_placeholder(self, dtype, shape, name):\n",
    "        placeholder = tf.compat.v1.placeholder(dtype, shape, name)\n",
    "        self.placeholders[name] = placeholder\n",
    "        return placeholder\n",
    "\n",
    "    def _set_up_input_pls(self):\n",
    "        \"\"\"Sets up input placeholders by adding them to self._placeholders.\n",
    "        Keys are defined as self.PL_*.\n",
    "        \"\"\"\n",
    "        # Combine config data\n",
    "        bev_dims = np.append(self._bev_pixel_size, self._bev_depth)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('bev_input'):\n",
    "            # Placeholder for BEV image input, to be filled in with feed_dict\n",
    "            bev_input_placeholder = self._add_placeholder(tf.float32, bev_dims,\n",
    "                                                          self.PL_BEV_INPUT)\n",
    "\n",
    "            self._bev_input_batches = tf.expand_dims(\n",
    "                bev_input_placeholder, axis=0)\n",
    "\n",
    "            self._bev_preprocessed = tf.image.resize(self._bev_input_batches, self._bev_pixel_size)\n",
    "\n",
    "            # Summary Images\n",
    "            bev_summary_images = tf.split(\n",
    "                bev_input_placeholder, self._bev_depth, axis=2)\n",
    "            tf.summary.image(\"bev_maps\", bev_summary_images,\n",
    "                             max_outputs=self._bev_depth)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('img_input'):\n",
    "            # Take variable size input images\n",
    "            img_input_placeholder = self._add_placeholder(\n",
    "                tf.float32,\n",
    "                [None, None, self._img_depth],\n",
    "                self.PL_IMG_INPUT)\n",
    "\n",
    "            self._img_input_batches = tf.expand_dims(\n",
    "                img_input_placeholder, axis=0)\n",
    "\n",
    "            self._img_preprocessed = tf.image.resize(self._img_input_batches, self._img_pixel_size)\n",
    "\n",
    "            # Summary Image\n",
    "            tf.summary.image(\"rgb_image\", self._img_preprocessed,\n",
    "                             max_outputs=2)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('pl_labels'):\n",
    "            #self._add_placeholder(tf.float32, [None, 6], self.PL_LABEL_ANCHORS)\n",
    "            self._add_placeholder(tf.float32, [None, 7], self.PL_LABEL_BOXES_3D)\n",
    "            self._add_placeholder(tf.float32, [None], self.PL_LABEL_CLASSES)\n",
    "\n",
    "        # Placeholders for anchors\n",
    "        with tf.compat.v1.variable_scope('pl_anchors'):\n",
    "            self._add_placeholder(tf.float32, [None, 6], self.PL_ANCHORS)\n",
    "            self._add_placeholder(tf.float32, [None], self.PL_ANCHOR_IOUS)\n",
    "            self._add_placeholder(tf.float32, [None, 6], self.PL_ANCHOR_OFFSETS)\n",
    "            self._add_placeholder(tf.float32, [None], self.PL_ANCHOR_CLASSES)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('bev_anchor_projections'):\n",
    "                self._add_placeholder(tf.float32, [None, 4], self.PL_BEV_ANCHORS)\n",
    "                self._bev_anchors_norm_pl = self._add_placeholder( tf.float32, [None, 4], self.PL_BEV_ANCHORS_NORM)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('img_anchor_projections'):\n",
    "                self._add_placeholder(tf.float32, [None, 4], self.PL_IMG_ANCHORS)\n",
    "                self._img_anchors_norm_pl = self._add_placeholder( tf.float32, [None, 4], self.PL_IMG_ANCHORS_NORM)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('sample_info'):\n",
    "                # the calib matrix shape is (3 x 4)\n",
    "                self._add_placeholder( tf.float32, [3, 4], self.PL_CALIB_P2)\n",
    "                self._add_placeholder(tf.int32, shape=[1], name=self.PL_IMG_IDX)\n",
    "                self._add_placeholder(tf.float32, [4], self.PL_GROUND_PLANE)\n",
    "\n",
    "    def _set_up_feature_extractors(self):\n",
    "        \"\"\"Sets up feature extractors and stores feature maps and\n",
    "        bottlenecks as member variables.\n",
    "        \"\"\"\n",
    "        weight_decay=0.0005\n",
    "        #shape due to shape provided by dataset. BEV could not be adapted: too sparse.\n",
    "        \n",
    "        inputs_img = tf.keras.layers.Input(batch_shape=(None,1024,1224,3))\n",
    "        net = tf.keras.layers.Conv2D(filters = 32, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), \n",
    "                                     activation=tf.nn.relu, kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv1\")(inputs_img)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch1\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 32, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), \n",
    "                                     use_bias=False, padding='same', name=\"conv2\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch2\")(net)   \n",
    "\n",
    "        net = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool1\")(net)\n",
    "\n",
    "        net = tf.keras.layers.Conv2D(filters = 64, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv3\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch3\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 64, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv4\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch4\")(net)\n",
    "\n",
    "        net = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool2\")(net)\n",
    "\n",
    "        net = tf.keras.layers.Conv2D(filters = 128,kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv5\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch5\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 128, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), \n",
    "                                     use_bias=False, padding='same', name=\"conv6\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch6\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 128, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv7\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch7\")(net)\n",
    "\n",
    "        net = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool3\")(net)\n",
    "\n",
    "        net = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv8\") (net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch8\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv9\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch9\")(net)\n",
    "        net = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv10\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch10\")(net)\n",
    "        \n",
    "        img_vgg = tf.keras.models.Model(inputs = inputs_img, outputs = net, name=\"img_vgg\")\n",
    "        \n",
    "        self.img_bottleneck = tf.keras.layers.Conv2D(filters = 32, kernel_size = [1,1], strides =(1,1), padding='same', name=\"bottleneck\")(net)\n",
    "        self.img_bottleneck= tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros',\n",
    "                                            gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', \n",
    "                                            beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(self.img_bottleneck)\n",
    "        \n",
    "         #shape due to shape provided by dataset. BEV could not be adapted: too sparse.\n",
    "        \n",
    "        inputs_bev = tf.keras.layers.Input(batch_shape=(None,336,336,3))\n",
    "        out = tf.keras.layers.Conv2D(filters = 32, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), \n",
    "                                     activation=tf.nn.relu, kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv1\")(inputs_bev)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch1\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 32, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), \n",
    "                                     use_bias=False, padding='same', name=\"conv2\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch2\")(out)   \n",
    "\n",
    "        out = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool1\")(out)\n",
    "\n",
    "        out = tf.keras.layers.Conv2D(filters = 64, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv3\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch3\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 64, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv4\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch4\")(out)\n",
    "\n",
    "        out = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool2\")(out)\n",
    "\n",
    "        out = tf.keras.layers.Conv2D(filters = 128,kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv5\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch5\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 128, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), \n",
    "                                     use_bias=False, padding='same', name=\"conv6\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch6\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 128, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv7\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch7\")(out)\n",
    "\n",
    "        out = tf.keras.layers.MaxPooling2D(pool_size=[2,2], strides=None, padding=\"valid\", name=\"pool3\")(out)\n",
    "\n",
    "        out = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv8\") (out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch8\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv9\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch9\")(out)\n",
    "        out = tf.keras.layers.Conv2D(filters = 256, kernel_size = [3,3], strides =(1,1), bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu,\n",
    "                                     kernel_initializer='ones', kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                                     use_bias=False, padding='same', name=\"conv10\")(out)\n",
    "        out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                        moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None,\n",
    "                                        beta_constraint=None, gamma_constraint=None, name=\"batch10\")(out)\n",
    "        \n",
    "        bev_vgg = tf.keras.models.Model(inputs = inputs_bev, outputs = out, name=\"bev_vgg\")\n",
    "        \n",
    "        self.bev_bottleneck = tf.keras.layers.Conv2D(filters = 32, kernel_size = [1,1], strides =(1,1), padding='same', name=\"bottleneck\")(out)\n",
    "        self.bev_bottleneck= tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros',\n",
    "                                            gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', \n",
    "                                            beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(self.bev_bottleneck)\n",
    "\n",
    "        # # Visualize the end point feature maps being used\n",
    "        bev_vgg.summary()\n",
    "        bev_end_point=bev_vgg.get_config()\n",
    "        img_vgg.summary()\n",
    "        img_end_point=img_vgg.get_config()\n",
    "\n",
    "    def build(self):\n",
    "\n",
    "        # Setup input placeholders\n",
    "        self._set_up_input_pls()\n",
    "\n",
    "        # Setup feature extractors\n",
    "        self._set_up_feature_extractors()\n",
    "\n",
    "        bev_proposal_input = self.bev_bottleneck\n",
    "        img_proposal_input = self.img_bottleneck\n",
    "\n",
    "        fusion_mean_div_factor = 2.0\n",
    "\n",
    "        # If both img and bev probabilites are set to 1.0, don't do\n",
    "        # path drop.\n",
    "        if not (self._path_drop_probabilities[0] ==\n",
    "                self._path_drop_probabilities[1] == 1.0):\n",
    "            with tf.compat.v1.variable_scope('rpn_path_drop'):\n",
    "\n",
    "                random_values = tf.random_uniform(shape=[3],\n",
    "                                                  minval=0.0,\n",
    "                                                  maxval=1.0)\n",
    "\n",
    "                img_mask, bev_mask = self.create_path_drop_masks(\n",
    "                    self._path_drop_probabilities[0],\n",
    "                    self._path_drop_probabilities[1],\n",
    "                    random_values)\n",
    "\n",
    "                img_proposal_input = tf.multiply(img_proposal_input,\n",
    "                                                 img_mask)\n",
    "\n",
    "                bev_proposal_input = tf.multiply(bev_proposal_input,\n",
    "                                                 bev_mask)\n",
    "\n",
    "                self.img_path_drop_mask = img_mask\n",
    "                self.bev_path_drop_mask = bev_mask\n",
    "\n",
    "                # Overwrite the division factor\n",
    "                fusion_mean_div_factor = img_mask + bev_mask\n",
    "\n",
    "        with tf.compat.v1.variable_scope('proposal_roi_pooling'):\n",
    "\n",
    "            with tf.compat.v1.variable_scope('box_indices'):\n",
    "                def get_box_indices(boxes):\n",
    "                    proposals_shape = boxes.get_shape().as_list()\n",
    "                    if any(dim is None for dim in proposals_shape):\n",
    "                        proposals_shape = tf.shape(boxes)\n",
    "                    ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)\n",
    "                    multiplier = tf.expand_dims(\n",
    "                        tf.range(start=0, limit=proposals_shape[0]), 1)\n",
    "                    return tf.reshape(ones_mat * multiplier, [-1])\n",
    "\n",
    "                bev_boxes_norm_batches = tf.expand_dims(\n",
    "                    self._bev_anchors_norm_pl, axis=0)\n",
    "\n",
    "                # These should be all 0's since there is only 1 image\n",
    "                tf_box_indices = get_box_indices(bev_boxes_norm_batches)\n",
    "            \n",
    "            proposal_roi_size_tf = [3,3]\n",
    "            # Do ROI Pooling on BEV\n",
    "            bev_proposal_rois = tf.image.crop_and_resize(\n",
    "                bev_proposal_input,\n",
    "                self._bev_anchors_norm_pl,\n",
    "                tf_box_indices,\n",
    "                proposal_roi_size_tf)\n",
    "            # Do ROI Pooling on image\n",
    "            img_proposal_rois = tf.image.crop_and_resize(\n",
    "                img_proposal_input,\n",
    "                self._bev_anchors_norm_pl,\n",
    "                tf_box_indices,\n",
    "                proposal_roi_size_tf)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('proposal_roi_fusion'):\n",
    "            rpn_fusion_out = None\n",
    "            if self._fusion_method == 'mean':\n",
    "                tf_features_sum = tf.add(bev_proposal_rois, img_proposal_rois)\n",
    "                #rpn_fusion_out = tf.divide(tf_features_sum, fusion_mean_div_factor)\n",
    "                rpn_fusion_out = tf.divide(tf_features_sum, 2)\n",
    "            elif self._fusion_method == 'concat':\n",
    "                rpn_fusion_out = tf.concat(\n",
    "                    [bev_proposal_rois, img_proposal_rois], axis=3)\n",
    "            else:\n",
    "                raise ValueError('Invalid fusion method', self._fusion_method)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('anchor_predictor', 'ap', [rpn_fusion_out]):\n",
    "            #None because unknown\n",
    "            tensor_in = tf.keras.Input(shape=None, tensor=rpn_fusion_out)\n",
    "            print(\"here\", tf_features_sum)\n",
    "            # Rpn layers config\n",
    "            weight_decay = 0.005\n",
    "\n",
    "            # Use conv2d instead of fully_connected layers.\n",
    "            cls_fc6 = tf.keras.layers.Conv2D(filters=32, kernel_size = [3,3], kernel_initializer='ones', \n",
    "                                             kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='VALID', name=\"conv1\")(tensor_in)\n",
    "\n",
    "            cls_fc6_drop = tf.keras.layers.Dropout(rate = 0.5, name=\"drop1\")(cls_fc6)\n",
    "\n",
    "            cls_fc7 = tf.keras.layers.Conv2D(filters=32, kernel_size = [1,1], kernel_initializer='ones',\n",
    "                                             kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='VALID', name=\"conv2\")(cls_fc6_drop)\n",
    "\n",
    "            cls_fc7_drop = tf.keras.layers.Dropout(rate = 0.5, name=\"drop2\")(cls_fc7)\n",
    "\n",
    "            cls_fc8 = tf.keras.layers.Conv2D(filters=2, kernel_size = [1,1], kernel_initializer='ones',\n",
    "                                             kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='VALID', name=\"conv3\")(cls_fc7_drop)\n",
    "\n",
    "            objectness = tf.squeeze(cls_fc8, axis=[1,2], name='conv3/squeezed')\n",
    "\n",
    "            # Use conv2d instead of fully_connected layers.\n",
    "            reg_fc6 = tf.keras.layers.Conv2D(filters=32, kernel_size = [3,3], kernel_initializer=\"ones\", \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='VALID', name=\"conv4\")(tensor_in)\n",
    "\n",
    "            reg_fc6_drop = tf.keras.layers.Dropout(rate = 0.5, name=\"drop3\")(reg_fc6)\n",
    "\n",
    "            reg_fc7 = tf.keras.layers.Conv2D(filters = 16, kernel_size = [1, 1], kernel_initializer=\"ones\",\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding=\"same\", name=\"conv5\")(reg_fc6_drop)\n",
    "\n",
    "            reg_fc7_drop = tf.keras.layers.Dropout(rate = 0.5, name=\"drop4\")(reg_fc7)\n",
    "\n",
    "            reg_fc8 = tf.keras.layers.Conv2D(filters = 6,  kernel_size = [1, 1],  kernel_initializer=\"ones\",\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding=\"same\", name=\"conv6\")(reg_fc7_drop)\n",
    "\n",
    "            offsets = tf.squeeze(reg_fc8, axis=[1,2], name='conv6/squeezed')\n",
    "            \n",
    "            model = tf.keras.models.Model(inputs = rpn_fusion_out, outputs = offsets, name=\"rpn_fusion_prediction_anchors\")\n",
    "            model1 = tf.keras.models.Model(inputs = rpn_fusion_out, outputs = objectness, name=\"objectness predictions\")\n",
    "            model.summary()\n",
    "            model1.summary()\n",
    "        # Return the proposals\n",
    "        with tf.compat.v1.variable_scope('proposals'):\n",
    "            anchors = self.placeholders[self.PL_ANCHORS]\n",
    "\n",
    "            # Decode anchor regression offsets\n",
    "            with tf.compat.v1.variable_scope('decoding'):\n",
    "                regressed_anchors = offset_to_anchor(\n",
    "                        anchors, offsets)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('bev_projection'):\n",
    "                _, bev_proposal_boxes_norm = anchor_projector.project_to_bev(\n",
    "                    regressed_anchors, self._bev_extents)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('softmax'):\n",
    "                objectness_softmax = tf.nn.softmax(objectness)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('nms'):\n",
    "                objectness_scores = objectness_softmax[:, 1]\n",
    "\n",
    "                # Do NMS on regressed anchors\n",
    "                top_indices = tf.image.non_max_suppression(\n",
    "                    bev_proposal_boxes_norm, objectness_scores,\n",
    "                    max_output_size=self._nms_size,\n",
    "                    iou_threshold=self._nms_iou_thresh)\n",
    "\n",
    "                top_anchors = tf.gather(regressed_anchors, top_indices)\n",
    "                top_objectness_softmax = tf.gather(objectness_scores,\n",
    "                                                   top_indices)\n",
    "                # top_offsets = tf.gather(offsets, top_indices)\n",
    "                # top_objectness = tf.gather(objectness, top_indices)\n",
    "\n",
    "        # Get mini batch\n",
    "        all_ious_gt = self.placeholders[self.PL_ANCHOR_IOUS]\n",
    "        all_offsets_gt = self.placeholders[self.PL_ANCHOR_OFFSETS]\n",
    "        all_classes_gt = self.placeholders[self.PL_ANCHOR_CLASSES]\n",
    "\n",
    "        with tf.compat.v1.variable_scope('mini_batch'):\n",
    "            mini_batch_mask, _ = sample_mini_batch(all_ious_gt, 64,[0, 0.3], [0.5,1])\n",
    "\n",
    "        # ROI summary images\n",
    "        rpn_mini_batch_size =64\n",
    "        with tf.compat.v1.variable_scope('bev_rpn_rois'):\n",
    "            mb_bev_anchors_norm = tf.boolean_mask(self._bev_anchors_norm_pl,\n",
    "                                                  mini_batch_mask)\n",
    "            mb_bev_box_indices = tf.zeros_like(\n",
    "                tf.boolean_mask(all_classes_gt, mini_batch_mask),\n",
    "                dtype=tf.int32)\n",
    "\n",
    "            # Show the ROIs of the BEV input density map\n",
    "            # for the mini batch anchors\n",
    "            bev_input_rois = tf.image.crop_and_resize(self._bev_preprocessed,\n",
    "                                                      mb_bev_anchors_norm, mb_bev_box_indices, (32, 32))\n",
    "\n",
    "            bev_input_roi_summary_images = tf.split(bev_input_rois, self._bev_depth, axis=3)\n",
    "            tf.summary.image('bev_rpn_rois', bev_input_roi_summary_images[-1], max_outputs=rpn_mini_batch_size)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('img_rpn_rois'):\n",
    "            # ROIs on image input\n",
    "            mb_img_anchors_norm = tf.boolean_mask(self._img_anchors_norm_pl, mini_batch_mask)\n",
    "            mb_img_box_indices = tf.zeros_like( tf.boolean_mask(all_classes_gt, mini_batch_mask), dtype=tf.int32)\n",
    "\n",
    "            # Do test ROI pooling on mini batch\n",
    "            img_input_rois = tf.image.crop_and_resize( self._img_preprocessed,\n",
    "                                                      mb_img_anchors_norm, mb_img_box_indices, (32, 32))\n",
    "\n",
    "            tf.summary.image('img_rpn_rois', img_input_rois, max_outputs=rpn_mini_batch_size)\n",
    "\n",
    "        # Ground Truth Tensors\n",
    "        with tf.compat.v1.variable_scope('one_hot_classes'):\n",
    "\n",
    "            # Anchor classification ground truth\n",
    "            # Object / Not Object\n",
    "            min_pos_iou = 0.5\n",
    "\n",
    "            objectness_classes_gt = tf.cast(tf.greater_equal(all_ious_gt, min_pos_iou), dtype=tf.int32)\n",
    "            objectness_gt = tf.one_hot(objectness_classes_gt, depth=2, on_value=1.0 - self._config.label_smoothing_epsilon,\n",
    "                                       off_value=self._config.label_smoothing_epsilon)\n",
    "\n",
    "        # Mask predictions for mini batch\n",
    "        with tf.compat.v1.variable_scope('prediction_mini_batch'):\n",
    "            objectness_masked = tf.boolean_mask(objectness, mini_batch_mask)\n",
    "            offsets_masked = tf.boolean_mask(offsets, mini_batch_mask)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('ground_truth_mini_batch'):\n",
    "            objectness_gt_masked = tf.boolean_mask(objectness_gt, mini_batch_mask)\n",
    "            offsets_gt_masked = tf.boolean_mask(all_offsets_gt, mini_batch_mask)\n",
    "\n",
    "        # Specify the tensors to evaluate\n",
    "        predictions = dict()\n",
    "\n",
    "        # Temporary predictions for debugging\n",
    "#         predictions['anchor_ious'] = anchor_ious\n",
    "#         predictions['anchor_offsets'] = all_offsets_gt\n",
    "\n",
    "        if self._train_val_test in ['train', 'val']:\n",
    "            # All anchors\n",
    "            predictions[self.PRED_ANCHORS] = anchors\n",
    "\n",
    "            # Mini-batch masks\n",
    "            predictions[self.PRED_MB_MASK] = mini_batch_mask\n",
    "            # Mini-batch predictions\n",
    "            predictions[self.PRED_MB_OBJECTNESS] = objectness_masked\n",
    "            predictions[self.PRED_MB_OFFSETS] = offsets_masked\n",
    "\n",
    "            # Mini batch ground truth\n",
    "            predictions[self.PRED_MB_OFFSETS_GT] = offsets_gt_masked\n",
    "            predictions[self.PRED_MB_OBJECTNESS_GT] = objectness_gt_masked\n",
    "\n",
    "            # Proposals after nms\n",
    "            predictions[self.PRED_TOP_INDICES] = top_indices\n",
    "            predictions[self.PRED_TOP_ANCHORS] = top_anchors\n",
    "            predictions[\n",
    "                self.PRED_TOP_OBJECTNESS_SOFTMAX] = top_objectness_softmax\n",
    "\n",
    "        else:\n",
    "            # self._train_val_test == 'test'\n",
    "            predictions[self.PRED_TOP_ANCHORS] = top_anchors\n",
    "            predictions[\n",
    "                self.PRED_TOP_OBJECTNESS_SOFTMAX] = top_objectness_softmax\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def create_feed_dict(self, scene_index=None):\n",
    "        \"\"\" Fills in the placeholders with the actual input values.\n",
    "            Currently, only a batch size of 1 is supported\n",
    "\n",
    "        Args:\n",
    "            sample_index: optional, only used when train_val_test == 'test',\n",
    "                a particular sample index in the dataset\n",
    "                sample list to build the feed_dict for\n",
    "\n",
    "        Returns:\n",
    "            a feed_dict dictionary that can be used in a tensorflow session\n",
    "        \"\"\"\n",
    "#TODO fix to have multiple batches\n",
    "#         if self._train_val_test in [\"train\", \"val\"]:\n",
    "\n",
    "#             # sample_index should be None\n",
    "#             if sample_index is not None:\n",
    "#                 raise ValueError('sample_index should be None. Do not load '\n",
    "#                                  'particular samples during train or val')\n",
    "\n",
    "#             # During training/validation, we need a valid sample\n",
    "#             # with anchor info for loss calculation\n",
    "#             sample = None\n",
    "#             anchors_info = []\n",
    "\n",
    "#             valid_sample = False\n",
    "#             while not valid_sample:\n",
    "#                 if self._train_val_test == \"train\":\n",
    "#                     # Get the a random sample from the remaining epoch\n",
    "#                     samples = self.dataset.next_batch(batch_size=1)\n",
    "\n",
    "#                 else:  # self._train_val_test == \"val\"\n",
    "#                     # Load samples in order for validation\n",
    "#                     samples = self.dataset.next_batch(batch_size=1, shuffle=False)\n",
    "\n",
    "#                 # Only handle one sample at a time for now\n",
    "#                 sample = samples[0]\n",
    "#                 anchors_info = sample.get(constants.KEY_ANCHORS_INFO)\n",
    "\n",
    "#                 # When training, if the mini batch is empty, go to the next\n",
    "#                 # sample. Otherwise carry on with found the valid sample.\n",
    "#                 # For validation, even if 'anchors_info' is empty, keep the\n",
    "#                 # sample (this will help penalize false positives.)\n",
    "#                 # We will substitue the necessary info with zeros later on.\n",
    "#                 # Note: Training/validating all samples can be switched off.\n",
    "#                 train_cond = (self._train_val_test == \"train\" and self._train_on_all_samples)\n",
    "#                 eval_cond = (self._train_val_test == \"val\" and self._eval_all_samples)\n",
    "#                 if anchors_info or train_cond or eval_cond:\n",
    "#                     valid_sample = True\n",
    "#         else:\n",
    "        # For testing, any sample should work\n",
    "        if scene_index is not None:\n",
    "            my_scene = dataset.scene[scene_index]\n",
    "        else:\n",
    "            raise TypeError('for testing you need to put a number! will change it later on once it works fully :) ')\n",
    "            \n",
    "\n",
    "        # Only handle one sample at a time for now\n",
    "        my_sample_token = my_scene[\"first_sample_token\"]\n",
    "        sample = level5data.get('sample', my_sample_token)\n",
    "        #anchors_info = sample.get(constants.KEY_ANCHORS_INFO) #ISSUE 1\n",
    "        anchors_info = []\n",
    "        sample_name = sample.get(\"token\")\n",
    "\n",
    "        # Get ground truth data\n",
    "        # label_anchors = sample.get(constants.KEY_LABEL_ANCHORS) not really useful \n",
    "        class_type = dataset.get(\"category\", sample)\n",
    "        classes_category = class_type.get(\"name\")\n",
    "        label_classes = classes.index(classes_category, start, end)\n",
    "        # We only need orientation from box_3d\n",
    "        #label_boxes_3d = sample.get(constants.KEY_LABEL_BOXES_3D) #issue 5\n",
    "        label_boxes_3d = []\n",
    "\n",
    "        # Network input data\n",
    "        img_input = dataset.get('sample_data', sample['data'][\"CAM_FRONT\"])\n",
    "        camera_token=img_input.get(\"token\")\n",
    "        file_name=level5data.get_sample_data_path(camera_token)\n",
    "        image = Image.open(file_name)\n",
    "        # convert image to numpy array\n",
    "        img_input = np.asarray(image)\n",
    "        bev_input = dataset.get('sample_data', sample['data'][\"LIDAR_TOP\"])\n",
    "        \n",
    "        #to feed the bev [todo, move this part in a preproc file]\n",
    "        lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "        lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "        ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "        calibrated_sensor_lidar = level5data.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n",
    "        global_from_car = transform_matrix(ego_pose['translation'], Quaternion(ego_pose['rotation']), inverse=False)\n",
    "        car_from_sensor_lidar = transform_matrix(calibrated_sensor_lidar['translation'], Quaternion(calibrated_sensor_lidar['rotation']),\n",
    "                                                  inverse=False)\n",
    "        lidar_pointcloud = LidarPointCloud.from_file(lidar_filepath)\n",
    "        lidar_pointcloud.transform(car_from_sensor_lidar)\n",
    "        map_mask = level5data.map[0][\"mask\"]\n",
    "        voxel_size = (0.4,0.4,1.5)\n",
    "        z_offset = -2.0\n",
    "        #arbitrary shape, must be square though!\n",
    "        bev_shape = (336,336, 3)\n",
    "        bev = create_voxel_pointcloud(lidar_pointcloud.points, bev_shape, voxel_size=voxel_size, z_offset=z_offset)\n",
    "        ego_centric_map = get_semantic_map_around_ego(map_mask, ego_pose, voxel_size=0.4, output_shape=(336,336)) \n",
    "        bev_input = normalize_voxel_intensities(bev)\n",
    "        move_boxes_to_car_space(boxes, ego_pose)\n",
    "        scale_boxes(boxes, 0.8)\n",
    "\n",
    "        # Image shape (h, w)\n",
    "        image_shape = [image_input.get(\"height\"), image_input.get(\"width\")\n",
    "        \n",
    "        #ground plane shape (a,b,c,d) in kitti:\n",
    "        #no info on ground plane in nuscenes data, just global coordinate system\n",
    "        #which is given as x, y, z. z also always zero\n",
    "        ground_plane = sample.get(constants.KEY_GROUND_PLANE) #issue 2\n",
    "        \n",
    "        #only for cameras, of course lidars do not have instrinsic matrices\n",
    "        token=image_input.get(\"calibrated_sensor_token\") \n",
    "        stereo_calib_p2= read_calibration(token)\n",
    "\n",
    "        # Fill the placeholders for anchor information\n",
    "        self._fill_anchor_pl_inputs(anchors_info=anchors_info, ground_plane=ground_plane,\n",
    "                                    image_shape=image_shape, stereo_calib_p2=stereo_calib_p2,\n",
    "                                    sample_name=sample_name)\n",
    "\n",
    "        # this is a list to match the explicit shape for the placeholder\n",
    "        self._placeholder_inputs[self.PL_IMG_IDX] = [int(sample_name)]\n",
    "\n",
    "        # Fill in the rest\n",
    "        self._placeholder_inputs[self.PL_BEV_INPUT] = bev_input\n",
    "        self._placeholder_inputs[self.PL_IMG_INPUT] = image_input\n",
    "\n",
    "        #self._placeholder_inputs[self.PL_LABEL_ANCHORS] = label_anchors\n",
    "        self._placeholder_inputs[self.PL_LABEL_BOXES_3D] = label_boxes_3d\n",
    "        self._placeholder_inputs[self.PL_LABEL_CLASSES] = label_classes\n",
    "\n",
    "        # Sample Info\n",
    "        # img_idx is a list to match the placeholder shape\n",
    "        self._placeholder_inputs[self.PL_IMG_IDX] = [int(sample_name)]\n",
    "        self._placeholder_inputs[self.PL_CALIB_P2] = stereo_calib_p2\n",
    "        self._placeholder_inputs[self.PL_GROUND_PLANE] = ground_plane\n",
    "\n",
    "        # Temporary sample info for debugging\n",
    "        self.sample_info.clear()\n",
    "        self.sample_info['sample_name'] = sample\n",
    "        self.sample_info['rpn_mini_batch'] = anchors_info\n",
    "\n",
    "        # Create a feed_dict and fill it with input values\n",
    "        feed_dict = dict()\n",
    "        for key, value in self.placeholders.items():\n",
    "            feed_dict[value] = self._placeholder_inputs[key]\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def _fill_anchor_pl_inputs(self,\n",
    "                               classes,\n",
    "                               anchors_info,\n",
    "                               ground_plane,\n",
    "                               image_shape,\n",
    "                               stereo_calib_p2,\n",
    "                               sample_name):\n",
    "        \"\"\"\n",
    "        Fills anchor placeholder inputs with corresponding data\n",
    "\n",
    "        Args:\n",
    "            anchors_info: anchor info from mini_batch_utils\n",
    "            ground_plane: ground plane coefficients\n",
    "            image_shape: image shape (h, w), used for projecting anchors\n",
    "            sample_name: name of the sample, e.g. \"000001\"\n",
    "        \"\"\"\n",
    "\n",
    "        # Lists for merging anchors info\n",
    "        all_anchor_boxes_3d = []\n",
    "        anchors_ious = []\n",
    "        anchor_offsets = []\n",
    "        anchor_classes = []\n",
    "\n",
    "        # Create anchors for each class\n",
    "        if len(classes) > 1:\n",
    "            for class_idx in range(len(classes)):\n",
    "                # Generate anchors for all classes\n",
    "                grid_anchor_boxes_3d = self._anchor_generator.generate(\n",
    "                    area_3d=self._area_extents,\n",
    "                    #anchor_3d_sizes=self._cluster_sizes[class_idx],\n",
    "                    anchor_stride=self._anchor_strides[class_idx],\n",
    "                    ground_plane=ground_plane)\n",
    "                all_anchor_boxes_3d.append(grid_anchor_boxes_3d)\n",
    "            all_anchor_boxes_3d = np.concatenate(all_anchor_boxes_3d)\n",
    "        else:\n",
    "            # Don't loop for a single class\n",
    "            class_idx = 0\n",
    "            grid_anchor_boxes_3d = self._anchor_generator.generate(\n",
    "                area_3d=self._area_extents,\n",
    "                anchor_3d_sizes=self._cluster_sizes[class_idx],\n",
    "                anchor_stride=self._anchor_strides[class_idx],\n",
    "                ground_plane=ground_plane)\n",
    "            all_anchor_boxes_3d = grid_anchor_boxes_3d\n",
    "\n",
    "        # Filter empty anchors\n",
    "        # Skip if anchors_info is []\n",
    "        sample_has_labels = True\n",
    "        if self._train_val_test in ['train', 'val']:\n",
    "            # Read in anchor info during training / validation\n",
    "            if anchors_info:\n",
    "                anchor_indices, anchors_ious, anchor_offsets, \\\n",
    "                    anchor_classes = anchors_info\n",
    "\n",
    "                anchor_boxes_3d_to_use = all_anchor_boxes_3d[anchor_indices]\n",
    "            else:\n",
    "                train_cond = (self._train_val_test == \"train\" and\n",
    "                              self._train_on_all_samples)\n",
    "                eval_cond = (self._train_val_test == \"val\" and\n",
    "                             self._eval_all_samples)\n",
    "                if train_cond or eval_cond:\n",
    "                    sample_has_labels = False\n",
    "        else:\n",
    "            sample_has_labels = False\n",
    "\n",
    "        if not sample_has_labels:\n",
    "            # During testing, or validation with no anchor info, manually\n",
    "            # filter empty anchors\n",
    "            # TODO: share voxel_grid_2d with BEV generation if possible\n",
    "            voxel_grid_2d = \\\n",
    "                self.dataset.kitti_utils.create_sliced_voxel_grid_2d(\n",
    "                    sample_name, self.dataset.bev_source,\n",
    "                    image_shape=image_shape)\n",
    "\n",
    "            # Convert to anchors and filter\n",
    "            anchors_to_use = box_3d_encoder.box_3d_to_anchor(all_anchor_boxes_3d)\n",
    "            empty_filter = anchor_filter.get_empty_anchor_filter_2d(anchors_to_use, voxel_grid_2d, density_threshold=1)\n",
    "\n",
    "            anchor_boxes_3d_to_use = all_anchor_boxes_3d[empty_filter]\n",
    "\n",
    "        # Convert lists to ndarrays\n",
    "        anchor_boxes_3d_to_use = np.asarray(anchor_boxes_3d_to_use)\n",
    "        anchors_ious = np.asarray(anchors_ious)\n",
    "        anchor_offsets = np.asarray(anchor_offsets)\n",
    "        anchor_classes = np.asarray(anchor_classes)\n",
    "\n",
    "        # Flip anchors and centroid x offsets for augmented samples\n",
    "#             if kitti_aug.AUG_FLIPPING in sample_augs:\n",
    "#                 anchor_boxes_3d_to_use = kitti_aug.flip_boxes_3d(anchor_boxes_3d_to_use, flip_ry=False)\n",
    "#                 if anchors_info:\n",
    "#                     anchor_offsets[:, 0] = -anchor_offsets[:, 0]\n",
    "\n",
    "        # Convert to anchors\n",
    "        anchors_to_use = box_3d_encoder.box_3d_to_anchor( anchor_boxes_3d_to_use)\n",
    "        num_anchors = len(anchors_to_use)\n",
    "\n",
    "        # Project anchors into bev\n",
    "        bev_anchors, bev_anchors_norm = anchor_projector.project_to_bev( anchors_to_use, self._bev_extents)\n",
    "\n",
    "        # Project box_3d anchors into image space\n",
    "        img_anchors, img_anchors_norm = anchor_projector.project_to_image_space(anchors_to_use, stereo_calib_p2, image_shape)\n",
    "\n",
    "        # Reorder into [y1, x1, y2, x2] for tf.crop_and_resize op\n",
    "        self._bev_anchors_norm = bev_anchors_norm[:, [1, 0, 3, 2]]\n",
    "        self._img_anchors_norm = img_anchors_norm[:, [1, 0, 3, 2]]\n",
    "\n",
    "        # Fill in placeholder inputs\n",
    "        self._placeholder_inputs[self.PL_ANCHORS] = anchors_to_use\n",
    "\n",
    "        # If we are in train/validation mode, and the anchor infos\n",
    "        # are not empty, store them. Checking for just anchors_ious\n",
    "        # to be non-empty should be enough.\n",
    "        if self._train_val_test in ['train', 'val'] and \\\n",
    "                len(anchors_ious) > 0:\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_IOUS] = anchors_ious\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_OFFSETS] = anchor_offsets\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_CLASSES] = anchor_classes\n",
    "\n",
    "        # During test, or val when there is no anchor info\n",
    "        elif self._train_val_test in ['test'] or \\\n",
    "                len(anchors_ious) == 0:\n",
    "            # During testing, or validation with no gt, fill these in with 0s\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_IOUS] = \\\n",
    "                np.zeros(num_anchors)\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_OFFSETS] = \\\n",
    "                np.zeros([num_anchors, 6])\n",
    "            self._placeholder_inputs[self.PL_ANCHOR_CLASSES] = \\\n",
    "                np.zeros(num_anchors)\n",
    "        else:\n",
    "            raise ValueError('Got run mode {}, and non-empty anchor info'.\n",
    "                             format(self._train_val_test))\n",
    "\n",
    "        self._placeholder_inputs[self.PL_BEV_ANCHORS] = bev_anchors\n",
    "        self._placeholder_inputs[self.PL_BEV_ANCHORS_NORM] = self._bev_anchors_norm\n",
    "        self._placeholder_inputs[self.PL_IMG_ANCHORS] = img_anchors\n",
    "        self._placeholder_inputs[self.PL_IMG_ANCHORS_NORM] = self._img_anchors_norm\n",
    "\n",
    "    def loss(self, prediction_dict):\n",
    "\n",
    "        # these should include mini-batch values only\n",
    "        objectness_gt = prediction_dict[self.PRED_MB_OBJECTNESS_GT]\n",
    "        offsets_gt = prediction_dict[self.PRED_MB_OFFSETS_GT]\n",
    "\n",
    "        # Predictions\n",
    "        with tf.compat.v1.variable_scope('rpn_prediction_mini_batch'):\n",
    "            objectness = prediction_dict[self.PRED_MB_OBJECTNESS]\n",
    "            offsets = prediction_dict[self.PRED_MB_OFFSETS]\n",
    "\n",
    "        with tf.compat.v1.variable_scope('rpn_losses'):\n",
    "            with tf.compat.v1.variable_scope('objectness'):\n",
    "                cls_loss = losses.WeightedSoftmaxLoss()\n",
    "                cls_loss_weight = self._config.loss_config.cls_loss_weight\n",
    "                objectness_loss = cls_loss(objectness, objectness_gt, weight=cls_loss_weight)\n",
    "\n",
    "                with tf.compat.v1.variable_scope('obj_norm'):\n",
    "                    # normalize by the number of anchor mini-batches\n",
    "                    objectness_loss = objectness_loss / tf.cast( tf.shape(objectness_gt)[0], dtype=tf.float32)\n",
    "                    tf.summary.scalar('objectness', objectness_loss)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('regression'):\n",
    "                reg_loss = losses.WeightedSmoothL1Loss()\n",
    "                reg_loss_weight = self._config.loss_config.reg_loss_weight\n",
    "                anchorwise_localization_loss = reg_loss(offsets, offsets_gt, weight=reg_loss_weight)\n",
    "                masked_localization_loss = anchorwise_localization_loss * objectness_gt[:, 1]\n",
    "                localization_loss = tf.reduce_sum(masked_localization_loss)\n",
    "\n",
    "                with tf.compat.v1.variable_scope('reg_norm'):\n",
    "                    # normalize by the number of positive objects\n",
    "                    num_positives = tf.reduce_sum(objectness_gt[:, 1])\n",
    "                    # Assert the condition `num_positives > 0`\n",
    "                    with tf.control_dependencies([tf.debugging.assert_positive(num_positives)]):\n",
    "                        localization_loss = localization_loss / num_positives\n",
    "                        tf.summary.scalar('regression', localization_loss)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('total_loss'):\n",
    "                total_loss = objectness_loss + localization_loss\n",
    "\n",
    "        loss_dict = {\n",
    "            self.LOSS_RPN_OBJECTNESS: objectness_loss,\n",
    "            self.LOSS_RPN_REGRESSION: localization_loss,\n",
    "        }\n",
    "\n",
    "        return loss_dict, total_loss\n",
    "\n",
    "    def create_path_drop_masks(self,\n",
    "                               p_img,\n",
    "                               p_bev,\n",
    "                               random_values):\n",
    "        \"\"\"Determines global path drop decision based on given probabilities.\n",
    "\n",
    "        Args:\n",
    "            p_img: A tensor of float32, probability of keeping image branch\n",
    "            p_bev: A tensor of float32, probability of keeping bev branch\n",
    "            random_values: A tensor of float32 of shape [3], the results\n",
    "                of coin flips, values should range from 0.0 - 1.0.\n",
    "\n",
    "        Returns:\n",
    "            final_img_mask: A constant tensor mask containing either one or zero\n",
    "                depending on the final coin flip probability.\n",
    "            final_bev_mask: A constant tensor mask containing either one or zero\n",
    "                depending on the final coin flip probability.\n",
    "        \"\"\"\n",
    "\n",
    "        def keep_branch(): return tf.constant(1.0)\n",
    "\n",
    "        def kill_branch(): return tf.constant(0.0)\n",
    "\n",
    "        # The logic works as follows:\n",
    "        # We have flipped 3 coins, first determines the chance of keeping\n",
    "        # the image branch, second determines keeping bev branch, the third\n",
    "        # makes the final decision in the case where both branches were killed\n",
    "        # off, otherwise the initial img and bev chances are kept.\n",
    "\n",
    "        img_chances = tf.case([(tf.less(random_values[0], p_img), keep_branch)], default=kill_branch)\n",
    "\n",
    "        bev_chances = tf.case([(tf.less(random_values[1], p_bev), keep_branch)], default=kill_branch)\n",
    "\n",
    "        # Decision to determine whether both branches were killed off\n",
    "        third_flip = tf.logical_or(tf.cast(img_chances, dtype=tf.bool), tf.cast(bev_chances, dtype=tf.bool))\n",
    "        third_flip = tf.cast(third_flip, dtype=tf.float32)\n",
    "\n",
    "        # Make a second choice, for the third case\n",
    "        # Here we use a 50/50 chance to keep either image or bev\n",
    "        # If its greater than 0.5, keep the image\n",
    "        img_second_flip = tf.case([(tf.greater(random_values[2], 0.5), keep_branch)], default=kill_branch)\n",
    "        # If its less than or equal to 0.5, keep bev\n",
    "        bev_second_flip = tf.case([(tf.less_equal(random_values[2], 0.5), keep_branch)],\n",
    "                                  default=kill_branch)\n",
    "\n",
    "        # Use lambda since this returns another condition and it needs to\n",
    "        # be callable\n",
    "        final_img_mask = tf.case([(tf.equal(third_flip, 1), lambda: img_chances)], default=lambda: img_second_flip)\n",
    "\n",
    "        final_bev_mask = tf.case([(tf.equal(third_flip, 1), lambda: bev_chances)], default=lambda: bev_second_flip)\n",
    "\n",
    "        return final_img_mask, final_bev_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test this part!! (and cry)\n",
    "\n",
    "Tests and results for the RPN model part. The following changes to the model were done after errors/issues with testing:\n",
    "<li>Maybe change the use of placeholders in the future, to fit with eager execution (shorter code)</li>\n",
    "<li>Some internal keras os function rises a warning, something will be depreated, doesn't tell where and what function exactly</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Utente1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"bev_vgg\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 336, 336, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 336, 336, 32)      864       \n",
      "_________________________________________________________________\n",
      "batch1 (BatchNormalization)  (None, 336, 336, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 336, 336, 32)      9216      \n",
      "_________________________________________________________________\n",
      "batch2 (BatchNormalization)  (None, 336, 336, 32)      128       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 168, 168, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 168, 168, 64)      18432     \n",
      "_________________________________________________________________\n",
      "batch3 (BatchNormalization)  (None, 168, 168, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 168, 168, 64)      36864     \n",
      "_________________________________________________________________\n",
      "batch4 (BatchNormalization)  (None, 168, 168, 64)      256       \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 84, 84, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv5 (Conv2D)               (None, 84, 84, 128)       73728     \n",
      "_________________________________________________________________\n",
      "batch5 (BatchNormalization)  (None, 84, 84, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv6 (Conv2D)               (None, 84, 84, 128)       147456    \n",
      "_________________________________________________________________\n",
      "batch6 (BatchNormalization)  (None, 84, 84, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv7 (Conv2D)               (None, 84, 84, 128)       147456    \n",
      "_________________________________________________________________\n",
      "batch7 (BatchNormalization)  (None, 84, 84, 128)       512       \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 42, 42, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv8 (Conv2D)               (None, 42, 42, 256)       294912    \n",
      "_________________________________________________________________\n",
      "batch8 (BatchNormalization)  (None, 42, 42, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv9 (Conv2D)               (None, 42, 42, 256)       589824    \n",
      "_________________________________________________________________\n",
      "batch9 (BatchNormalization)  (None, 42, 42, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv10 (Conv2D)              (None, 42, 42, 256)       589824    \n",
      "_________________________________________________________________\n",
      "batch10 (BatchNormalization) (None, 42, 42, 256)       1024      \n",
      "=================================================================\n",
      "Total params: 1,913,952\n",
      "Trainable params: 1,911,264\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "Model: \"img_vgg\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1024, 1224, 3)]   0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 1024, 1224, 32)    864       \n",
      "_________________________________________________________________\n",
      "batch1 (BatchNormalization)  (None, 1024, 1224, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 1024, 1224, 32)    9216      \n",
      "_________________________________________________________________\n",
      "batch2 (BatchNormalization)  (None, 1024, 1224, 32)    128       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 512, 612, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 512, 612, 64)      18432     \n",
      "_________________________________________________________________\n",
      "batch3 (BatchNormalization)  (None, 512, 612, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 512, 612, 64)      36864     \n",
      "_________________________________________________________________\n",
      "batch4 (BatchNormalization)  (None, 512, 612, 64)      256       \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 256, 306, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv5 (Conv2D)               (None, 256, 306, 128)     73728     \n",
      "_________________________________________________________________\n",
      "batch5 (BatchNormalization)  (None, 256, 306, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv6 (Conv2D)               (None, 256, 306, 128)     147456    \n",
      "_________________________________________________________________\n",
      "batch6 (BatchNormalization)  (None, 256, 306, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv7 (Conv2D)               (None, 256, 306, 128)     147456    \n",
      "_________________________________________________________________\n",
      "batch7 (BatchNormalization)  (None, 256, 306, 128)     512       \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 128, 153, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv8 (Conv2D)               (None, 128, 153, 256)     294912    \n",
      "_________________________________________________________________\n",
      "batch8 (BatchNormalization)  (None, 128, 153, 256)     1024      \n",
      "_________________________________________________________________\n",
      "conv9 (Conv2D)               (None, 128, 153, 256)     589824    \n",
      "_________________________________________________________________\n",
      "batch9 (BatchNormalization)  (None, 128, 153, 256)     1024      \n",
      "_________________________________________________________________\n",
      "conv10 (Conv2D)              (None, 128, 153, 256)     589824    \n",
      "_________________________________________________________________\n",
      "batch10 (BatchNormalization) (None, 128, 153, 256)     1024      \n",
      "=================================================================\n",
      "Total params: 1,913,952\n",
      "Trainable params: 1,911,264\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "here Tensor(\"proposal_roi_fusion/Add:0\", shape=(None, 3, 3, 32), dtype=float32)\n",
      "Model: \"rpn_fusion_prediction_anchors\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 3, 3, 32)]        0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 1, 1, 32)          9248      \n",
      "_________________________________________________________________\n",
      "drop3 (Dropout)              (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv5 (Conv2D)               (None, 1, 1, 16)          528       \n",
      "_________________________________________________________________\n",
      "drop4 (Dropout)              (None, 1, 1, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv6 (Conv2D)               (None, 1, 1, 6)           102       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_anchor_predictor [(None, 6)]               0         \n",
      "=================================================================\n",
      "Total params: 9,878\n",
      "Trainable params: 9,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"objectness predictions\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 3, 3, 32)]        0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 1, 1, 32)          9248      \n",
      "_________________________________________________________________\n",
      "drop1 (Dropout)              (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 1, 1, 32)          1056      \n",
      "_________________________________________________________________\n",
      "drop2 (Dropout)              (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 1, 1, 2)           66        \n",
      "_________________________________________________________________\n",
      "tf_op_layer_anchor_predictor [(None, 2)]               0         \n",
      "=================================================================\n",
      "Total params: 10,370\n",
      "Trainable params: 10,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "{'rpn_objectness_loss': <tf.Tensor 'rpn_losses/objectness/obj_norm/truediv:0' shape=() dtype=float32>, 'rpn_regression_loss': <tf.Tensor 'rpn_losses/regression/reg_norm/truediv:0' shape=() dtype=float32>} Tensor(\"rpn_losses/total_loss/add:0\", shape=(), dtype=float32) {'rpn_anchors': <tf.Tensor 'pl_anchors/anchors_pl:0' shape=(None, 6) dtype=float32>, 'rpn_mb_mask': <tf.Tensor 'mini_batch/LogicalOr_1:0' shape=(None,) dtype=bool>, 'rpn_mb_objectness': <tf.Tensor 'prediction_mini_batch/boolean_mask/GatherV2:0' shape=(None, 2) dtype=float32>, 'rpn_mb_offsets': <tf.Tensor 'prediction_mini_batch/boolean_mask_1/GatherV2:0' shape=(None, 6) dtype=float32>, 'rpn_mb_offsets_gt': <tf.Tensor 'ground_truth_mini_batch/boolean_mask_1/GatherV2:0' shape=(None, 6) dtype=float32>, 'rpn_mb_objectness_gt': <tf.Tensor 'ground_truth_mini_batch/boolean_mask/GatherV2:0' shape=(None, 2) dtype=float32>, 'rpn_top_indices': <tf.Tensor 'proposals/nms/non_max_suppression/NonMaxSuppressionV3:0' shape=(None,) dtype=int32>, 'rpn_top_anchors': <tf.Tensor 'proposals/nms/GatherV2:0' shape=(None, 6) dtype=float32>, 'rpn_top_objectness_softmax': <tf.Tensor 'proposals/nms/GatherV2_1:0' shape=(None,) dtype=float32>}\n"
     ]
    }
   ],
   "source": [
    "import avod.builders.config_builder_util as config_build\n",
    "config_path = 'avod/configs/unittest_model.config'\n",
    "pipe_path = 'avod/configs/unittest_pipeline.config'\n",
    "model_config = config_build.get_model_config_from_file(config_path)\n",
    "pipeline_config=config_build.get_configs_from_pipeline_file(pipe_path, \"val\")\n",
    "\n",
    "rpn_model = RpnModel(model_config, pipeline_config[3],\n",
    "                         train_val_test=\"val\",\n",
    "                         dataset=level5data)\n",
    "\n",
    "predictions = rpn_model.build()\n",
    "\n",
    "loss, total_loss = rpn_model.loss(predictions)\n",
    "print(loss, total_loss, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Tests for avod.core.models.bev_rpn\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import avod\n",
    "import avod.builders.config_builder_util as config_build\n",
    "from avod.builders.dataset_builder import DatasetBuilder\n",
    "from avod.protos import pipeline_pb2\n",
    "\n",
    "\n",
    "\n",
    "def test_rpn_loss(self):\n",
    "    # Use \"val\" so that the first sample is loaded each time\n",
    "    rpn_model = RpnModel(self.model_config,\n",
    "                         train_val_test=\"val\",\n",
    "                         dataset=self.dataset)\n",
    "\n",
    "    predictions = rpn_model.build()\n",
    "\n",
    "    loss, total_loss = rpn_model.loss(predictions)\n",
    "\n",
    "    feed_dict = rpn_model.create_feed_dict()\n",
    "\n",
    "    with self.test_session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        loss_dict_out = sess.run(loss, feed_dict=feed_dict)\n",
    "        print('Losses ', loss_dict_out)\n",
    "\n",
    "def test_create_path_drop_masks(self):\n",
    "    # Tests creating path drop choices\n",
    "    # based on the given probabilities\n",
    "\n",
    "    rpn_model = RpnModel(self.model_config,\n",
    "                         train_val_test=\"val\",\n",
    "                         dataset=self.dataset)\n",
    "    rpn_model.build()\n",
    "    ##################################\n",
    "    # Test-Case 1 : Keep img, Keep bev\n",
    "    ##################################\n",
    "    p_img = tf.constant(0.6)\n",
    "    p_bev = tf.constant(0.85)\n",
    "\n",
    "    # Set the random numbers for testing purposes\n",
    "    rand_choice = [0.53, 0.83, 0.05]\n",
    "    rand_choice_tensor = tf.convert_to_tensor(rand_choice)\n",
    "\n",
    "    img_mask, bev_mask = rpn_model.create_path_drop_masks(\n",
    "        p_img, p_bev, rand_choice_tensor)\n",
    "\n",
    "    with self.test_session():\n",
    "        img_mask_out = img_mask.eval()\n",
    "        bev_mask_out = bev_mask.eval()\n",
    "        np.testing.assert_array_equal(img_mask_out, 1.0)\n",
    "        np.testing.assert_array_equal(bev_mask_out, 1.0)\n",
    "\n",
    "    ##################################\n",
    "    # Test-Case 2 : Kill img, Keep bev\n",
    "    ##################################\n",
    "    p_img = tf.constant(0.2)\n",
    "    p_bev = tf.constant(0.85)\n",
    "\n",
    "    img_mask, bev_mask = rpn_model.create_path_drop_masks(\n",
    "        p_img, p_bev, rand_choice_tensor)\n",
    "\n",
    "    with self.test_session():\n",
    "        img_mask_out = img_mask.eval()\n",
    "        bev_mask_out = bev_mask.eval()\n",
    "        np.testing.assert_array_equal(img_mask_out, 0.0)\n",
    "        np.testing.assert_array_equal(bev_mask_out, 1.0)\n",
    "\n",
    "    ##################################\n",
    "    # Test-Case 3 : Keep img, Kill bev\n",
    "    ##################################\n",
    "    p_img = tf.constant(0.9)\n",
    "    p_bev = tf.constant(0.1)\n",
    "\n",
    "    img_mask, bev_mask = rpn_model.create_path_drop_masks(\n",
    "        p_img, p_bev, rand_choice_tensor)\n",
    "\n",
    "    with self.test_session():\n",
    "        img_mask_out = img_mask.eval()\n",
    "        bev_mask_out = bev_mask.eval()\n",
    "        np.testing.assert_array_equal(img_mask_out, 1.0)\n",
    "        np.testing.assert_array_equal(bev_mask_out, 0.0)\n",
    "\n",
    "    ##############################################\n",
    "    # Test-Case 4 : Kill img, Kill bev, third flip\n",
    "    ##############################################\n",
    "    p_img = tf.constant(0.0)\n",
    "    p_bev = tf.constant(0.1)\n",
    "\n",
    "    img_mask, bev_mask = rpn_model.create_path_drop_masks(\n",
    "        p_img, p_bev, rand_choice_tensor)\n",
    "\n",
    "    with self.test_session():\n",
    "        img_mask_out = img_mask.eval()\n",
    "        bev_mask_out = bev_mask.eval()\n",
    "        np.testing.assert_array_equal(img_mask_out, 0.0)\n",
    "        # Because of the third condition, we expect to be keeping bev\n",
    "        np.testing.assert_array_equal(bev_mask_out, 1.0)\n",
    "\n",
    "    ##############################################\n",
    "    # Test-Case 5 : Kill img, Kill bev, third flip\n",
    "    ##############################################\n",
    "    # Let's flip the third chance and keep img instead\n",
    "    rand_choice = [0.53, 0.83, 0.61]\n",
    "    rand_choice_tensor = tf.convert_to_tensor(rand_choice)\n",
    "    p_img = tf.constant(0.0)\n",
    "    p_bev = tf.constant(0.1)\n",
    "\n",
    "    img_mask, bev_mask = rpn_model.create_path_drop_masks(\n",
    "        p_img, p_bev, rand_choice_tensor)\n",
    "\n",
    "    with self.test_session():\n",
    "        img_mask_out = img_mask.eval()\n",
    "        bev_mask_out = bev_mask.eval()\n",
    "        # Because of the third condition, we expect to be keeping img\n",
    "        np.testing.assert_array_equal(img_mask_out, 1.0)\n",
    "        np.testing.assert_array_equal(bev_mask_out, 0.0)\n",
    "\n",
    "def test_path_drop_input_multiplication(self):\n",
    "    # Tests the result of final image/bev inputs\n",
    "    # based on the path drop decisions\n",
    "\n",
    "    rpn_model = RpnModel(self.model_config,\n",
    "                         train_val_test=\"val\",\n",
    "                         dataset=self.dataset)\n",
    "    rpn_model.build()\n",
    "    # Shape of input feature map\n",
    "    dummy_img_feature_shape = [1, 30, 50, 2]\n",
    "    random_values = np.random.randint(low=1.0,\n",
    "                                      high=256.0,\n",
    "                                      size=2).astype(np.float32)\n",
    "\n",
    "    dummy_img_feature_map = tf.fill(dummy_img_feature_shape,\n",
    "                                    random_values[0])\n",
    "    # Assume both features map are the same size, this is not\n",
    "    # the case inside the network\n",
    "    dummy_bev_feature_map = tf.fill(dummy_img_feature_shape,\n",
    "                                    random_values[1])\n",
    "\n",
    "    ##################################\n",
    "    # Test-Case 1 : Keep img, Kill bev\n",
    "    ##################################\n",
    "    exp_img_input = np.full(dummy_img_feature_shape, random_values[0])\n",
    "    exp_bev_input = np.full(dummy_img_feature_shape, 0.0)\n",
    "\n",
    "    p_img = tf.constant(0.6)\n",
    "    p_bev = tf.constant(0.4)\n",
    "\n",
    "    # Set the random numbers for testing purposes\n",
    "    rand_choice = [0.53, 0.83, 0.05]\n",
    "    rand_choice_tensor = tf.convert_to_tensor(rand_choice)\n",
    "\n",
    "    img_mask, bev_mask = rpn_model.create_path_drop_masks(\n",
    "        p_img, p_bev, rand_choice_tensor)\n",
    "\n",
    "    final_img_input = tf.multiply(dummy_img_feature_map,\n",
    "                                  img_mask)\n",
    "\n",
    "    final_bev_input = tf.multiply(dummy_bev_feature_map,\n",
    "                                  bev_mask)\n",
    "\n",
    "    with self.test_session():\n",
    "        final_img_input_out = final_img_input.eval()\n",
    "        final_bev_input_out = final_bev_input.eval()\n",
    "        np.testing.assert_array_equal(final_img_input_out,\n",
    "                                      exp_img_input)\n",
    "        np.testing.assert_array_equal(final_bev_input_out,\n",
    "                                      exp_bev_input)\n",
    "\n",
    "    ##################################\n",
    "    # Test-Case 2 : Kill img, Keep bev\n",
    "    ##################################\n",
    "    exp_img_input = np.full(dummy_img_feature_shape, 0)\n",
    "    exp_bev_input = np.full(dummy_img_feature_shape, random_values[1])\n",
    "\n",
    "    p_img = tf.constant(0.4)\n",
    "    p_bev = tf.constant(0.9)\n",
    "\n",
    "    img_mask, bev_mask = rpn_model.create_path_drop_masks(\n",
    "        p_img, p_bev, rand_choice_tensor)\n",
    "\n",
    "    final_img_input = tf.multiply(dummy_img_feature_map,\n",
    "                                  img_mask)\n",
    "\n",
    "    final_bev_input = tf.multiply(dummy_bev_feature_map,\n",
    "                                  bev_mask)\n",
    "\n",
    "    with self.test_session():\n",
    "        final_img_input_out = final_img_input.eval()\n",
    "        final_bev_input_out = final_bev_input.eval()\n",
    "        np.testing.assert_array_equal(final_img_input_out,\n",
    "                                      exp_img_input)\n",
    "        np.testing.assert_array_equal(final_bev_input_out,\n",
    "                                      exp_bev_input)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> AVOD MODEL: </b> second stage detector for the AVOD algorithm. It uses FPN as feature extractors.\n",
    "<b> FPN: </b> Feature Pyramid Network (FPN) is a feature extractor designed for such pyramid concept with accuracy and speed in mind. It replaces the feature extractor of detectors like Faster R-CNN and generates multiple feature map layers (multi-scale feature maps) with better quality information than the regular feature pyramid for object detection. [Understanding Feature Pyramid Networks for object detection (FPN)](https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from avod.builders import avod_fc_layers_builder\n",
    "from avod.builders import avod_loss_builder\n",
    "\n",
    "from avod.core import anchor_projector\n",
    "from avod.core import anchor_encoder\n",
    "from avod.core import box_3d_encoder\n",
    "from avod.core import box_8c_encoder\n",
    "from avod.core import box_4c_encoder\n",
    "\n",
    "from avod.core import box_list\n",
    "from avod.core import box_list_ops\n",
    "\n",
    "from avod.core import model\n",
    "from avod.core import orientation_encoder\n",
    "\n",
    "from avod.core.mini_batch_utils import MiniBatchUtils\n",
    "\n",
    "class AvodModel(model.DetectionModel):\n",
    "    ##############################\n",
    "    # Keys for Predictions\n",
    "    ##############################\n",
    "    # Mini batch (mb) ground truth\n",
    "    PRED_MB_CLASSIFICATIONS_GT = 'avod_mb_classifications_gt'\n",
    "    PRED_MB_OFFSETS_GT = 'avod_mb_offsets_gt'\n",
    "    PRED_MB_ORIENTATIONS_GT = 'avod_mb_orientations_gt'\n",
    "\n",
    "    # Mini batch (mb) predictions\n",
    "    PRED_MB_CLASSIFICATION_LOGITS = 'avod_mb_classification_logits'\n",
    "    PRED_MB_CLASSIFICATION_SOFTMAX = 'avod_mb_classification_softmax'\n",
    "    PRED_MB_OFFSETS = 'avod_mb_offsets'\n",
    "    PRED_MB_ANGLE_VECTORS = 'avod_mb_angle_vectors'\n",
    "\n",
    "    # Top predictions after BEV NMS\n",
    "    PRED_TOP_CLASSIFICATION_LOGITS = 'avod_top_classification_logits'\n",
    "    PRED_TOP_CLASSIFICATION_SOFTMAX = 'avod_top_classification_softmax'\n",
    "\n",
    "    PRED_TOP_PREDICTION_ANCHORS = 'avod_top_prediction_anchors'\n",
    "    PRED_TOP_PREDICTION_BOXES_3D = 'avod_top_prediction_boxes_3d'\n",
    "    PRED_TOP_ORIENTATIONS = 'avod_top_orientations'\n",
    "\n",
    "    # Other box representations\n",
    "    PRED_TOP_BOXES_8C = 'avod_top_regressed_boxes_8c'\n",
    "    PRED_TOP_BOXES_4C = 'avod_top_prediction_boxes_4c'\n",
    "\n",
    "    # Mini batch (mb) predictions (for debugging)\n",
    "    PRED_MB_MASK = 'avod_mb_mask'\n",
    "    PRED_MB_POS_MASK = 'avod_mb_pos_mask'\n",
    "    PRED_MB_ANCHORS_GT = 'avod_mb_anchors_gt'\n",
    "    PRED_MB_CLASS_INDICES_GT = 'avod_mb_gt_classes'\n",
    "\n",
    "    # All predictions (for debugging)\n",
    "    PRED_ALL_CLASSIFICATIONS = 'avod_classifications'\n",
    "    PRED_ALL_OFFSETS = 'avod_offsets'\n",
    "    PRED_ALL_ANGLE_VECTORS = 'avod_angle_vectors'\n",
    "\n",
    "    PRED_MAX_IOUS = 'avod_max_ious'\n",
    "    PRED_ALL_IOUS = 'avod_anchor_ious'\n",
    "\n",
    "    ##############################\n",
    "    # Keys for Loss\n",
    "    ##############################\n",
    "    LOSS_FINAL_CLASSIFICATION = 'avod_classification_loss'\n",
    "    LOSS_FINAL_REGRESSION = 'avod_regression_loss'\n",
    "\n",
    "    # (for debugging)\n",
    "    LOSS_FINAL_ORIENTATION = 'avod_orientation_loss'\n",
    "    LOSS_FINAL_LOCALIZATION = 'avod_localization_loss'\n",
    "\n",
    "    def __init__(self, model_config, train_val_test, dataset, classes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_config: configuration for the model\n",
    "            train_val_test: \"train\", \"val\", or \"test\"\n",
    "            dataset: the dataset that will provide samples and ground truth\n",
    "        \"\"\"\n",
    "\n",
    "        # Sets model configs (_config)\n",
    "        super(AvodModel, self).__init__(model_config)\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "        # Dataset config\n",
    "        self._num_final_classes = length(classes)+1\n",
    "\n",
    "        # Input config\n",
    "        input_config = self._config.input_config\n",
    "        self._bev_pixel_size = np.asarray([input_config.bev_dims_h, input_config.bev_dims_w])\n",
    "        self._bev_depth = input_config.bev_depth\n",
    "\n",
    "        self._img_pixel_size = np.asarray([input_config.img_dims_h, input_config.img_dims_w])\n",
    "        self._img_depth = [input_config.img_depth]\n",
    "\n",
    "        # AVOD config\n",
    "        avod_config = self._config.avod_config\n",
    "        self._proposal_roi_crop_size = [3,3]\n",
    "        self._positive_selection = avod_config.avod_positive_selection\n",
    "        self._nms_size = avod_config.avod_nms_size\n",
    "        self._nms_iou_threshold = avod_config.avod_nms_iou_thresh\n",
    "        self._path_drop_probabilities = self._config.path_drop_probabilities\n",
    "        self._box_rep = avod_config.avod_box_representation\n",
    "\n",
    "        if self._box_rep not in ['box_3d', 'box_8c', 'box_8co', 'box_4c', 'box_4ca']:\n",
    "            raise ValueError('Invalid box representation', self._box_rep)\n",
    "\n",
    "        # Create the RpnModel\n",
    "        self._rpn_model = RpnModel(model_config, train_val_test, dataset)\n",
    "\n",
    "        if train_val_test not in [\"train\", \"val\", \"test\"]:\n",
    "            raise ValueError('Invalid train_val_test value,'\n",
    "                             'should be one of [\"train\", \"val\", \"test\"]')\n",
    "        self._train_val_test = train_val_test\n",
    "        self._is_training = (self._train_val_test == 'train')\n",
    "\n",
    "        self.sample_info = {}\n",
    "\n",
    "    def build(self):\n",
    "        rpn_model = self._rpn_model\n",
    "\n",
    "        # Share the same prediction dict as RPN\n",
    "        prediction_dict = rpn_model.build()\n",
    "\n",
    "        top_anchors = prediction_dict[RpnModel.PRED_TOP_ANCHORS]\n",
    "        ground_plane = rpn_model.placeholders[RpnModel.PL_GROUND_PLANE]\n",
    "\n",
    "        class_labels = rpn_model.placeholders[RpnModel.PL_LABEL_CLASSES]\n",
    "\n",
    "        with tf.compat.v1.variable_scope('avod_projection'):\n",
    "\n",
    "            if self._config.expand_proposals_xz > 0.0:\n",
    "\n",
    "                expand_length = self._config.expand_proposals_xz\n",
    "\n",
    "                # Expand anchors along x and z\n",
    "                with tf.compat.v1.variable_scope('expand_xz'):\n",
    "                    expanded_dim_x = top_anchors[:, 3] + expand_length\n",
    "                    expanded_dim_z = top_anchors[:, 5] + expand_length\n",
    "\n",
    "                    expanded_anchors = tf.stack([top_anchors[:, 0], top_anchors[:, 1], top_anchors[:, 2], expanded_dim_x,\n",
    "                                                 top_anchors[:, 4], expanded_dim_z] , axis=1)\n",
    "\n",
    "                avod_projection_in = expanded_anchors\n",
    "\n",
    "            else:\n",
    "                avod_projection_in = top_anchors\n",
    "            \n",
    "            area_extents = self._config.dataset_config.kitti_utils_config.area_extents\n",
    "            self.area_extents = np.reshape(area_extents, (3, 2))\n",
    "            self.bev_extents = self.area_extents[[0, 2]]\n",
    "            with tf.compat.v1.variable_scope('bev'):\n",
    "                # Project top anchors into bev and image spaces\n",
    "                bev_proposal_boxes, bev_proposal_boxes_norm = anchor_projector.project_to_bev(avod_projection_in, self.bev_extents)\n",
    "\n",
    "                # Reorder projected boxes into [y1, x1, y2, x2]\n",
    "                bev_proposal_boxes_tf_order = anchor_projector.reorder_projected_boxes(bev_proposal_boxes)\n",
    "                bev_proposal_boxes_norm_tf_order = anchor_projector.reorder_projected_boxes(bev_proposal_boxes_norm)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('img'):\n",
    "                image_shape = tf.cast(tf.shape(rpn_model.placeholders[RpnModel.PL_IMG_INPUT])[0:2], tf.float32)\n",
    "                img_proposal_boxes, img_proposal_boxes_norm = anchor_projector.tf_project_to_image_space(avod_projection_in, \n",
    "                                                                                                         rpn_model.placeholders[RpnModel.PL_CALIB_P2], \n",
    "                                                                                                         image_shape)\n",
    "                # Only reorder the normalized img\n",
    "                img_proposal_boxes_norm_tf_order = anchor_projector.reorder_projected_boxes(img_proposal_boxes_norm)\n",
    "\n",
    "        bev_feature_maps = rpn_model.bev_feature_maps\n",
    "        img_feature_maps = rpn_model.img_feature_maps\n",
    "\n",
    "        if not (self._path_drop_probabilities[0] == self._path_drop_probabilities[1] == 1.0):\n",
    "\n",
    "            with tf.compat.v1.variable_scope('avod_path_drop'):\n",
    "\n",
    "                img_mask = rpn_model.img_path_drop_mask\n",
    "                bev_mask = rpn_model.bev_path_drop_mask\n",
    "\n",
    "                img_feature_maps = tf.math.multiply(img_feature_maps, img_mask)\n",
    "\n",
    "                bev_feature_maps = tf.math.multiply(bev_feature_maps, bev_mask)\n",
    "        else:\n",
    "            bev_mask = tf.constant(1.0)\n",
    "            img_mask = tf.constant(1.0)\n",
    "\n",
    "        # ROI Pooling\n",
    "        with tf.compat.v1.variable_scope('avod_roi_pooling'):\n",
    "            def get_box_indices(boxes):\n",
    "                proposals_shape = boxes.get_shape().as_list()\n",
    "                if any(dim is None for dim in proposals_shape):\n",
    "                    proposals_shape = tf.shape(boxes)\n",
    "                ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)\n",
    "                multiplier = tf.expand_dims( tf.range(start=0, limit=proposals_shape[0]), 1)\n",
    "                return tf.reshape(ones_mat * multiplier, [-1])\n",
    "\n",
    "            bev_boxes_norm_batches = tf.expand_dims(bev_proposal_boxes_norm, axis=0)\n",
    "\n",
    "            # These should be all 0's since there is only 1 image\n",
    "            tf_box_indices = get_box_indices(bev_boxes_norm_batches)\n",
    "\n",
    "            # Do ROI Pooling on BEV\n",
    "            bev_rois = tf.image.crop_and_resize(bev_feature_maps, bev_proposal_boxes_norm_tf_order,\n",
    "                                                tf_box_indices, self._proposal_roi_crop_size, name='bev_rois')\n",
    "            # Do ROI Pooling on image\n",
    "            img_rois = tf.image.crop_and_resize(img_feature_maps, img_proposal_boxes_norm_tf_order,\n",
    "                                                tf_box_indices, self._proposal_roi_crop_size, name='img_rois')\n",
    "\n",
    "        # Fully connected layers (Box Predictor)\n",
    "        avod_layers_config = self.model_config.layers_config.avod_config\n",
    "\n",
    "        fc_output_layers = avod_fc_layers_builder.build(\n",
    "                layers_config=avod_layers_config,\n",
    "                input_rois=[bev_rois, img_rois],\n",
    "                input_weights=[bev_mask, img_mask],\n",
    "                num_final_classes=self._num_final_classes,\n",
    "                box_rep=self._box_rep,\n",
    "                top_anchors=top_anchors,\n",
    "                ground_plane=ground_plane,\n",
    "                is_training=self._is_training)\n",
    "\n",
    "        all_cls_logits = fc_output_layers[avod_fc_layers_builder.KEY_CLS_LOGITS]\n",
    "        all_offsets = fc_output_layers[avod_fc_layers_builder.KEY_OFFSETS]\n",
    "\n",
    "        # This may be None\n",
    "        all_angle_vectors = fc_output_layers.get(avod_fc_layers_builder.KEY_ANGLE_VECTORS)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('softmax'):\n",
    "            all_cls_softmax = tf.nn.softmax(all_cls_logits)\n",
    "\n",
    "        ######################################################\n",
    "        # Subsample mini_batch for the loss function\n",
    "        ######################################################\n",
    "        # Get the ground truth tensors\n",
    "        anchors_gt = rpn_model.placeholders[RpnModel.PL_LABEL_ANCHORS]\n",
    "        if self._box_rep in ['box_3d', 'box_4ca']:\n",
    "            boxes_3d_gt = rpn_model.placeholders[RpnModel.PL_LABEL_BOXES_3D]\n",
    "            orientations_gt = boxes_3d_gt[:, 6]\n",
    "        elif self._box_rep in ['box_8c', 'box_8co', 'box_4c']:\n",
    "            boxes_3d_gt = rpn_model.placeholders[RpnModel.PL_LABEL_BOXES_3D]\n",
    "        else:\n",
    "            raise NotImplementedError('Ground truth tensors not implemented')\n",
    "\n",
    "        # Project anchor_gts to 2D bev\n",
    "        with tf.compat.v1.variable_scope('avod_gt_projection'):\n",
    "            bev_anchor_boxes_gt, _ = anchor_projector.project_to_bev( anchors_gt, self.bev_extents)\n",
    "\n",
    "            bev_anchor_boxes_gt_tf_order = anchor_projector.reorder_projected_boxes(bev_anchor_boxes_gt)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('avod_box_list'):\n",
    "            # Convert to box_list format\n",
    "            anchor_box_list_gt = box_list.BoxList(bev_anchor_boxes_gt_tf_order)\n",
    "            anchor_box_list = box_list.BoxList(bev_proposal_boxes_tf_order)\n",
    "\n",
    "        mb_mask, mb_class_label_indices, mb_gt_indices = sample_mini_batch( anchor_box_list_gt=anchor_box_list_gt,\n",
    "                                                                                anchor_box_list=anchor_box_list,\n",
    "                                                                                class_labels=class_labels)\n",
    "\n",
    "        # Create classification one_hot vector\n",
    "        with tf.compat.v1.variable_scope('avod_one_hot_classes'):\n",
    "            mb_classification_gt = tf.one_hot( mb_class_label_indices, depth=self._num_final_classes, \n",
    "                                              on_value=1.0 - self._config.label_smoothing_epsilon,\n",
    "                                              off_value=(self._config.label_smoothing_epsilon /\n",
    "                                                         self.dataset.num_classes))\n",
    "\n",
    "        # TODO: Don't create a mini batch in test mode\n",
    "        # Mask predictions\n",
    "        with tf.compat.v1.variable_scope('avod_apply_mb_mask'):\n",
    "            # Classification\n",
    "            mb_classifications_logits = tf.boolean_mask( all_cls_logits, mb_mask)\n",
    "            mb_classifications_softmax = tf.boolean_mask( all_cls_softmax, mb_mask)\n",
    "\n",
    "            # Offsets\n",
    "            mb_offsets = tf.boolean_mask(all_offsets, mb_mask)\n",
    "\n",
    "            # Angle Vectors\n",
    "            if all_angle_vectors is not None:\n",
    "                mb_angle_vectors = tf.boolean_mask(all_angle_vectors, mb_mask)\n",
    "            else:\n",
    "                mb_angle_vectors = None\n",
    "\n",
    "        # Encode anchor offsets\n",
    "        with tf.compat.v1.variable_scope('avod_encode_mb_anchors'):\n",
    "            mb_anchors = tf.boolean_mask(top_anchors, mb_mask)\n",
    "\n",
    "            if self._box_rep == 'box_3d':\n",
    "                # Gather corresponding ground truth anchors for each mb sample\n",
    "                mb_anchors_gt = tf.gather(anchors_gt, mb_gt_indices)\n",
    "                mb_offsets_gt = anchor_encoder.tf_anchor_to_offset(mb_anchors, mb_anchors_gt)\n",
    "\n",
    "                # Gather corresponding ground truth orientation for each\n",
    "                # mb sample\n",
    "                mb_orientations_gt = tf.gather(orientations_gt, mb_gt_indices)\n",
    "            elif self._box_rep in ['box_8c', 'box_8co']:\n",
    "\n",
    "                # Get boxes_3d ground truth mini-batch and convert to box_8c\n",
    "                mb_boxes_3d_gt = tf.gather(boxes_3d_gt, mb_gt_indices)\n",
    "                if self._box_rep == 'box_8c':\n",
    "                    mb_boxes_8c_gt = box_8c_encoder.tf_box_3d_to_box_8c(mb_boxes_3d_gt)\n",
    "                elif self._box_rep == 'box_8co':\n",
    "                    mb_boxes_8c_gt = box_8c_encoder.tf_box_3d_to_box_8co(mb_boxes_3d_gt)\n",
    "\n",
    "                # Convert proposals: anchors -> box_3d -> box8c\n",
    "                proposal_boxes_3d = box_3d_encoder.anchors_to_box_3d(top_anchors, fix_lw=True)\n",
    "                proposal_boxes_8c = box_8c_encoder.tf_box_3d_to_box_8c(proposal_boxes_3d)\n",
    "\n",
    "                # Get mini batch offsets\n",
    "                mb_boxes_8c = tf.boolean_mask(proposal_boxes_8c, mb_mask)\n",
    "                mb_offsets_gt = box_8c_encoder.tf_box_8c_to_offsets(mb_boxes_8c, mb_boxes_8c_gt)\n",
    "\n",
    "                # Flatten the offsets to a (N x 24) vector\n",
    "                mb_offsets_gt = tf.reshape(mb_offsets_gt, [-1, 24])\n",
    "\n",
    "            elif self._box_rep in ['box_4c', 'box_4ca']:\n",
    "\n",
    "                # Get ground plane for box_4c conversion\n",
    "                ground_plane = self._rpn_model.placeholders[self._rpn_model.PL_GROUND_PLANE]\n",
    "\n",
    "                # Convert gt boxes_3d -> box_4c\n",
    "                mb_boxes_3d_gt = tf.gather(boxes_3d_gt, mb_gt_indices)\n",
    "                mb_boxes_4c_gt = box_4c_encoder.tf_box_3d_to_box_4c(mb_boxes_3d_gt, ground_plane)\n",
    "\n",
    "                # Convert proposals: anchors -> box_3d -> box_4c\n",
    "                proposal_boxes_3d = box_3d_encoder.anchors_to_box_3d(top_anchors, fix_lw=True)\n",
    "                proposal_boxes_4c = box_4c_encoder.tf_box_3d_to_box_4c(proposal_boxes_3d, ground_plane)\n",
    "\n",
    "                # Get mini batch\n",
    "                mb_boxes_4c = tf.boolean_mask(proposal_boxes_4c, mb_mask)\n",
    "                mb_offsets_gt = box_4c_encoder.tf_box_4c_to_offsets(mb_boxes_4c, mb_boxes_4c_gt)\n",
    "\n",
    "                if self._box_rep == 'box_4ca':\n",
    "                    # Gather corresponding ground truth orientation for each\n",
    "                    # mb sample\n",
    "                    mb_orientations_gt = tf.gather(orientations_gt, mb_gt_indices)\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError('Anchor encoding not implemented for', self._box_rep)\n",
    "\n",
    "        ######################################################\n",
    "        # ROI summary images\n",
    "        ######################################################\n",
    "        avod_mini_batch_size = self._config.dataset_config.kitti_utils_config.avod_config.mini_batch_size \n",
    "        with tf.compat.v1.variable_scope('bev_avod_rois'):\n",
    "            mb_bev_anchors_norm = tf.boolean_mask( bev_proposal_boxes_norm_tf_order, mb_mask)\n",
    "            mb_bev_box_indices = tf.zeros_like(mb_gt_indices, dtype=tf.int32)\n",
    "\n",
    "            # Show the ROIs of the BEV input density map\n",
    "            # for the mini batch anchors\n",
    "            bev_input_rois = tf.image.crop_and_resize( self._rpn_model._bev_preprocessed, mb_bev_anchors_norm, \n",
    "                                                      mb_bev_box_indices, (32, 32))\n",
    "\n",
    "            bev_input_roi_summary_images = tf.split(bev_input_rois, self._bev_depth, axis=3)\n",
    "            tf.summary.image('bev_avod_rois', bev_input_roi_summary_images[-1], max_outputs=avod_mini_batch_size)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('img_avod_rois'):\n",
    "            # ROIs on image input\n",
    "            mb_img_anchors_norm = tf.boolean_mask(img_proposal_boxes_norm_tf_order, mb_mask)\n",
    "            mb_img_box_indices = tf.zeros_like(mb_gt_indices, dtype=tf.int32)\n",
    "\n",
    "            # Do test ROI pooling on mini batch\n",
    "            img_input_rois = tf.image.crop_and_resize( self._rpn_model._img_preprocessed, mb_img_anchors_norm, \n",
    "                                                      mb_img_box_indices, (32, 32))\n",
    "\n",
    "            tf.summary.image('img_avod_rois', img_input_rois, max_outputs=avod_mini_batch_size)\n",
    "\n",
    "        ######################################################\n",
    "        # Final Predictions\n",
    "        ######################################################\n",
    "        # Get orientations from angle vectors\n",
    "        if all_angle_vectors is not None:\n",
    "            with tf.compat.v1.variable_scope('avod_orientation'):\n",
    "                all_orientations = orientation_encoder.tf_angle_vector_to_orientation(all_angle_vectors)\n",
    "\n",
    "        # Apply offsets to regress proposals\n",
    "        with tf.compat.v1.variable_scope('avod_regression'):\n",
    "            if self._box_rep == 'box_3d':\n",
    "                prediction_anchors = anchor_encoder.offset_to_anchor(top_anchors, all_offsets)\n",
    "\n",
    "            elif self._box_rep in ['box_8c', 'box_8co']:\n",
    "                # Reshape the 24-dim regressed offsets to (N x 3 x 8)\n",
    "                reshaped_offsets = tf.reshape(all_offsets, [-1, 3, 8])\n",
    "                # Given the offsets, get the boxes_8c\n",
    "                prediction_boxes_8c = box_8c_encoder.tf_offsets_to_box_8c(proposal_boxes_8c, reshaped_offsets)\n",
    "                # Convert corners back to box3D\n",
    "                prediction_boxes_3d = box_8c_encoder.box_8c_to_box_3d(prediction_boxes_8c)\n",
    "\n",
    "                # Convert the box_3d to anchor format for nms\n",
    "                prediction_anchors = box_3d_encoder.tf_box_3d_to_anchor(prediction_boxes_3d)\n",
    "\n",
    "            elif self._box_rep in ['box_4c', 'box_4ca']:\n",
    "                # Convert predictions box_4c -> box_3d\n",
    "                prediction_boxes_4c = box_4c_encoder.tf_offsets_to_box_4c(proposal_boxes_4c, all_offsets)\n",
    "\n",
    "                prediction_boxes_3d = box_4c_encoder.tf_box_4c_to_box_3d(prediction_boxes_4c, ground_plane)\n",
    "\n",
    "                # Convert to anchor format for nms\n",
    "                prediction_anchors = box_3d_encoder.tf_box_3d_to_anchor(prediction_boxes_3d)\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError('Regression not implemented for',\n",
    "                                          self._box_rep)\n",
    "\n",
    "        # Apply Non-oriented NMS in BEV\n",
    "        with tf.compat.v1.variable_scope('avod_nms'):\n",
    "            bev_extents = self.bev_extents\n",
    "\n",
    "            with tf.compat.v1.variable_scope('bev_projection'):\n",
    "                # Project predictions into BEV\n",
    "                avod_bev_boxes, _ = anchor_projector.project_to_bev(prediction_anchors, bev_extents)\n",
    "                avod_bev_boxes_tf_order = anchor_projector.reorder_projected_boxes(avod_bev_boxes)\n",
    "\n",
    "            # Get top score from second column onward\n",
    "            all_top_scores = tf.math.reduce_max(all_cls_logits[:, 1:], axis=1)\n",
    "\n",
    "            # Apply NMS in BEV\n",
    "            nms_indices = tf.image.non_max_suppression( avod_bev_boxes_tf_order, all_top_scores, \n",
    "                                                       max_output_size=self._nms_size, iou_threshold=self._nms_iou_threshold)\n",
    "\n",
    "            # Gather predictions from NMS indices\n",
    "            top_classification_logits = tf.gather(all_cls_logits, nms_indices)\n",
    "            top_classification_softmax = tf.gather(all_cls_softmax, nms_indices)\n",
    "            top_prediction_anchors = tf.gather(prediction_anchors, nms_indices)\n",
    "\n",
    "            if self._box_rep == 'box_3d':\n",
    "                top_orientations = tf.gather(\n",
    "                    all_orientations, nms_indices)\n",
    "\n",
    "            elif self._box_rep in ['box_8c', 'box_8co']:\n",
    "                top_prediction_boxes_3d = tf.gather( prediction_boxes_3d, nms_indices)\n",
    "                top_prediction_boxes_8c = tf.gather( prediction_boxes_8c, nms_indices)\n",
    "\n",
    "            elif self._box_rep == 'box_4c':\n",
    "                top_prediction_boxes_3d = tf.gather( prediction_boxes_3d, nms_indices)\n",
    "                top_prediction_boxes_4c = tf.gather( prediction_boxes_4c, nms_indices)\n",
    "\n",
    "            elif self._box_rep == 'box_4ca':\n",
    "                top_prediction_boxes_3d = tf.gather( prediction_boxes_3d, nms_indices)\n",
    "                top_prediction_boxes_4c = tf.gather( prediction_boxes_4c, nms_indices)\n",
    "                top_orientations = tf.gather( all_orientations, nms_indices)\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError('NMS gather not implemented for', self._box_rep)\n",
    "\n",
    "        if self._train_val_test in ['train', 'val']:\n",
    "            # Additional entries are added to the shared prediction_dict\n",
    "            # Mini batch predictions\n",
    "            prediction_dict[self.PRED_MB_CLASSIFICATION_LOGITS] = mb_classifications_logits\n",
    "            prediction_dict[self.PRED_MB_CLASSIFICATION_SOFTMAX] = mb_classifications_softmax\n",
    "            prediction_dict[self.PRED_MB_OFFSETS] = mb_offsets\n",
    "\n",
    "            # Mini batch ground truth\n",
    "            prediction_dict[self.PRED_MB_CLASSIFICATIONS_GT] = mb_classification_gt\n",
    "            prediction_dict[self.PRED_MB_OFFSETS_GT] = mb_offsets_gt\n",
    "\n",
    "            # Top NMS predictions\n",
    "            prediction_dict[self.PRED_TOP_CLASSIFICATION_LOGITS] = top_classification_logits\n",
    "            prediction_dict[self.PRED_TOP_CLASSIFICATION_SOFTMAX] = top_classification_softmax\n",
    "\n",
    "            prediction_dict[self.PRED_TOP_PREDICTION_ANCHORS] = top_prediction_anchors\n",
    "\n",
    "            # Mini batch predictions (for debugging)\n",
    "            prediction_dict[self.PRED_MB_MASK] = mb_mask\n",
    "            # prediction_dict[self.PRED_MB_POS_MASK] = mb_pos_mask\n",
    "            prediction_dict[self.PRED_MB_CLASS_INDICES_GT] = mb_class_label_indices\n",
    "\n",
    "            # All predictions (for debugging)\n",
    "            prediction_dict[self.PRED_ALL_CLASSIFICATIONS] = all_cls_logits\n",
    "            prediction_dict[self.PRED_ALL_OFFSETS] = all_offsets\n",
    "\n",
    "            # Path drop masks (for debugging)\n",
    "            prediction_dict['bev_mask'] = bev_mask\n",
    "            prediction_dict['img_mask'] = img_mask\n",
    "\n",
    "        else:\n",
    "            # self._train_val_test == 'test'\n",
    "            prediction_dict[self.PRED_TOP_CLASSIFICATION_SOFTMAX] = top_classification_softmax\n",
    "            prediction_dict[self.PRED_TOP_PREDICTION_ANCHORS] = top_prediction_anchors\n",
    "\n",
    "        if self._box_rep == 'box_3d':\n",
    "            prediction_dict[self.PRED_MB_ANCHORS_GT] = mb_anchors_gt\n",
    "            prediction_dict[self.PRED_MB_ORIENTATIONS_GT] = mb_orientations_gt\n",
    "            prediction_dict[self.PRED_MB_ANGLE_VECTORS] = mb_angle_vectors\n",
    "\n",
    "            prediction_dict[self.PRED_TOP_ORIENTATIONS] = top_orientations\n",
    "\n",
    "            # For debugging\n",
    "            prediction_dict[self.PRED_ALL_ANGLE_VECTORS] = all_angle_vectors\n",
    "\n",
    "        elif self._box_rep in ['box_8c', 'box_8co']:\n",
    "            prediction_dict[self.PRED_TOP_PREDICTION_BOXES_3D] = top_prediction_boxes_3d\n",
    "\n",
    "            # Store the corners before converting for visualization purposes\n",
    "            prediction_dict[self.PRED_TOP_BOXES_8C] = top_prediction_boxes_8c\n",
    "\n",
    "        elif self._box_rep == 'box_4c':\n",
    "            prediction_dict[self.PRED_TOP_PREDICTION_BOXES_3D] = top_prediction_boxes_3d\n",
    "            prediction_dict[self.PRED_TOP_BOXES_4C] = top_prediction_boxes_4c\n",
    "\n",
    "        elif self._box_rep == 'box_4ca':\n",
    "            if self._train_val_test in ['train', 'val']:\n",
    "                prediction_dict[self.PRED_MB_ORIENTATIONS_GT] = mb_orientations_gt\n",
    "                prediction_dict[self.PRED_MB_ANGLE_VECTORS] = mb_angle_vectors\n",
    "\n",
    "            prediction_dict[self.PRED_TOP_PREDICTION_BOXES_3D] = top_prediction_boxes_3d\n",
    "            prediction_dict[self.PRED_TOP_BOXES_4C] = top_prediction_boxes_4c\n",
    "            prediction_dict[self.PRED_TOP_ORIENTATIONS] = top_orientations\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Prediction dict not implemented for',\n",
    "                                      self._box_rep)\n",
    "\n",
    "        # prediction_dict[self.PRED_MAX_IOUS] = max_ious\n",
    "        # prediction_dict[self.PRED_ALL_IOUS] = all_ious\n",
    "\n",
    "        return prediction_dict\n",
    "\n",
    "    def sample_mini_batch(self, anchor_box_list_gt, anchor_box_list,\n",
    "                          class_labels):\n",
    "\n",
    "        with tf.compat.v1.variable_scope('avod_create_mb_mask'):\n",
    "            # Get IoU for every anchor\n",
    "            all_ious = box_list_ops.iou(anchor_box_list_gt, anchor_box_list)\n",
    "            max_ious = tf.math.reduce_max(all_ious, axis=0)\n",
    "            max_iou_indices = tf.math.argmax(all_ious, axis=0)\n",
    "\n",
    "            # Sample a pos/neg mini-batch from anchors with highest IoU match\n",
    "            mini_batch_utils = MiniBatchUtils(self.dataset)\n",
    "            mb_mask, mb_pos_mask = mini_batch_utils.sample_avod_mini_batch(\n",
    "                max_ious)\n",
    "            mb_class_label_indices = mini_batch_utils.mask_class_label_indices(\n",
    "                mb_pos_mask, mb_mask, max_iou_indices, class_labels)\n",
    "\n",
    "            mb_gt_indices = tf.boolean_mask(max_iou_indices, mb_mask)\n",
    "\n",
    "        return mb_mask, mb_class_label_indices, mb_gt_indices\n",
    "\n",
    "    def create_feed_dict(self):\n",
    "        feed_dict = self._rpn_model.create_feed_dict()\n",
    "        self.sample_info = self._rpn_model.sample_info\n",
    "        return feed_dict\n",
    "\n",
    "    def loss(self, prediction_dict):\n",
    "        # Note: The loss should be using mini-batch values only\n",
    "        loss_dict, rpn_loss = self._rpn_model.loss(prediction_dict)\n",
    "        losses_output = avod_loss_builder.build(self, prediction_dict)\n",
    "\n",
    "        classification_loss = losses_output[avod_loss_builder.KEY_CLASSIFICATION_LOSS]\n",
    "\n",
    "        final_reg_loss = losses_output[avod_loss_builder.KEY_REGRESSION_LOSS]\n",
    "\n",
    "        avod_loss = losses_output[avod_loss_builder.KEY_AVOD_LOSS]\n",
    "\n",
    "        offset_loss_norm = losses_output[avod_loss_builder.KEY_OFFSET_LOSS_NORM]\n",
    "\n",
    "        loss_dict.update({self.LOSS_FINAL_CLASSIFICATION: classification_loss})\n",
    "        loss_dict.update({self.LOSS_FINAL_REGRESSION: final_reg_loss})\n",
    "\n",
    "        # Add localization and orientation losses to loss dict for plotting\n",
    "        loss_dict.update({self.LOSS_FINAL_LOCALIZATION: offset_loss_norm})\n",
    "\n",
    "        ang_loss_loss_norm = losses_output.get(avod_loss_builder.KEY_ANG_LOSS_NORM)\n",
    "        if ang_loss_loss_norm is not None:\n",
    "            loss_dict.update({self.LOSS_FINAL_ORIENTATION: ang_loss_loss_norm})\n",
    "\n",
    "        with tf.compat.v1.variable_scope('model_total_loss'):\n",
    "            total_loss = rpn_loss + avod_loss\n",
    "\n",
    "        return loss_dict, total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
